{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.629129, acc.: 40.62%] [G loss: 0.622638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.376094, acc.: 75.00%] [G loss: 0.724654]\n",
      "2 [D loss: 0.338856, acc.: 81.25%] [G loss: 0.865592]\n",
      "3 [D loss: 0.324815, acc.: 78.12%] [G loss: 0.947941]\n",
      "4 [D loss: 0.242271, acc.: 93.75%] [G loss: 1.189906]\n",
      "5 [D loss: 0.239594, acc.: 96.88%] [G loss: 1.244903]\n",
      "6 [D loss: 0.213583, acc.: 90.62%] [G loss: 1.446320]\n",
      "7 [D loss: 0.139553, acc.: 100.00%] [G loss: 1.530020]\n",
      "8 [D loss: 0.147485, acc.: 100.00%] [G loss: 1.693536]\n",
      "9 [D loss: 0.124042, acc.: 100.00%] [G loss: 1.816324]\n",
      "10 [D loss: 0.086729, acc.: 100.00%] [G loss: 1.815099]\n",
      "11 [D loss: 0.101408, acc.: 100.00%] [G loss: 1.914283]\n",
      "12 [D loss: 0.109316, acc.: 100.00%] [G loss: 2.033732]\n",
      "13 [D loss: 0.070656, acc.: 100.00%] [G loss: 2.156601]\n",
      "14 [D loss: 0.093805, acc.: 100.00%] [G loss: 2.088259]\n",
      "15 [D loss: 0.084493, acc.: 100.00%] [G loss: 2.261597]\n",
      "16 [D loss: 0.070523, acc.: 100.00%] [G loss: 2.325712]\n",
      "17 [D loss: 0.072979, acc.: 100.00%] [G loss: 2.390721]\n",
      "18 [D loss: 0.060172, acc.: 100.00%] [G loss: 2.423348]\n",
      "19 [D loss: 0.063191, acc.: 100.00%] [G loss: 2.490518]\n",
      "20 [D loss: 0.051988, acc.: 100.00%] [G loss: 2.544976]\n",
      "21 [D loss: 0.046580, acc.: 100.00%] [G loss: 2.531646]\n",
      "22 [D loss: 0.056324, acc.: 100.00%] [G loss: 2.645743]\n",
      "23 [D loss: 0.044950, acc.: 100.00%] [G loss: 2.675138]\n",
      "24 [D loss: 0.040257, acc.: 100.00%] [G loss: 2.693383]\n",
      "25 [D loss: 0.049548, acc.: 100.00%] [G loss: 2.740489]\n",
      "26 [D loss: 0.043627, acc.: 100.00%] [G loss: 2.882534]\n",
      "27 [D loss: 0.029269, acc.: 100.00%] [G loss: 2.841419]\n",
      "28 [D loss: 0.050769, acc.: 100.00%] [G loss: 2.953339]\n",
      "29 [D loss: 0.035146, acc.: 100.00%] [G loss: 2.949086]\n",
      "30 [D loss: 0.033944, acc.: 100.00%] [G loss: 2.943500]\n",
      "31 [D loss: 0.033756, acc.: 100.00%] [G loss: 3.004267]\n",
      "32 [D loss: 0.036930, acc.: 100.00%] [G loss: 3.134121]\n",
      "33 [D loss: 0.027389, acc.: 100.00%] [G loss: 3.107294]\n",
      "34 [D loss: 0.036145, acc.: 100.00%] [G loss: 3.074479]\n",
      "35 [D loss: 0.041065, acc.: 100.00%] [G loss: 3.293949]\n",
      "36 [D loss: 0.022562, acc.: 100.00%] [G loss: 3.288590]\n",
      "37 [D loss: 0.024090, acc.: 100.00%] [G loss: 3.340599]\n",
      "38 [D loss: 0.033329, acc.: 100.00%] [G loss: 3.388996]\n",
      "39 [D loss: 0.018357, acc.: 100.00%] [G loss: 3.426499]\n",
      "40 [D loss: 0.023319, acc.: 100.00%] [G loss: 3.527111]\n",
      "41 [D loss: 0.023720, acc.: 100.00%] [G loss: 3.511467]\n",
      "42 [D loss: 0.029421, acc.: 100.00%] [G loss: 3.399268]\n",
      "43 [D loss: 0.024598, acc.: 100.00%] [G loss: 3.422721]\n",
      "44 [D loss: 0.021547, acc.: 100.00%] [G loss: 3.456769]\n",
      "45 [D loss: 0.020240, acc.: 100.00%] [G loss: 3.549833]\n",
      "46 [D loss: 0.021272, acc.: 100.00%] [G loss: 3.681001]\n",
      "47 [D loss: 0.022889, acc.: 100.00%] [G loss: 3.495944]\n",
      "48 [D loss: 0.019040, acc.: 100.00%] [G loss: 3.572705]\n",
      "49 [D loss: 0.024289, acc.: 100.00%] [G loss: 3.675746]\n",
      "50 [D loss: 0.025519, acc.: 100.00%] [G loss: 3.710011]\n",
      "51 [D loss: 0.027113, acc.: 100.00%] [G loss: 3.689120]\n",
      "52 [D loss: 0.021048, acc.: 100.00%] [G loss: 3.724686]\n",
      "53 [D loss: 0.019073, acc.: 100.00%] [G loss: 3.962518]\n",
      "54 [D loss: 0.018690, acc.: 100.00%] [G loss: 3.846392]\n",
      "55 [D loss: 0.026235, acc.: 100.00%] [G loss: 3.959182]\n",
      "56 [D loss: 0.013325, acc.: 100.00%] [G loss: 3.839045]\n",
      "57 [D loss: 0.024071, acc.: 100.00%] [G loss: 3.916688]\n",
      "58 [D loss: 0.026391, acc.: 100.00%] [G loss: 3.962220]\n",
      "59 [D loss: 0.017657, acc.: 100.00%] [G loss: 4.110026]\n",
      "60 [D loss: 0.016527, acc.: 100.00%] [G loss: 4.126308]\n",
      "61 [D loss: 0.013964, acc.: 100.00%] [G loss: 3.857058]\n",
      "62 [D loss: 0.017599, acc.: 100.00%] [G loss: 4.060855]\n",
      "63 [D loss: 0.020874, acc.: 100.00%] [G loss: 4.013964]\n",
      "64 [D loss: 0.011978, acc.: 100.00%] [G loss: 4.135318]\n",
      "65 [D loss: 0.015633, acc.: 100.00%] [G loss: 4.047230]\n",
      "66 [D loss: 0.020368, acc.: 100.00%] [G loss: 4.044742]\n",
      "67 [D loss: 0.013429, acc.: 100.00%] [G loss: 4.156349]\n",
      "68 [D loss: 0.018095, acc.: 100.00%] [G loss: 4.131680]\n",
      "69 [D loss: 0.013292, acc.: 100.00%] [G loss: 4.318061]\n",
      "70 [D loss: 0.019211, acc.: 100.00%] [G loss: 4.140129]\n",
      "71 [D loss: 0.011059, acc.: 100.00%] [G loss: 4.080346]\n",
      "72 [D loss: 0.025109, acc.: 100.00%] [G loss: 4.066512]\n",
      "73 [D loss: 0.016994, acc.: 100.00%] [G loss: 4.011352]\n",
      "74 [D loss: 0.023458, acc.: 100.00%] [G loss: 4.184296]\n",
      "75 [D loss: 0.017762, acc.: 100.00%] [G loss: 4.101934]\n",
      "76 [D loss: 0.024481, acc.: 100.00%] [G loss: 4.312380]\n",
      "77 [D loss: 0.015475, acc.: 100.00%] [G loss: 4.258453]\n",
      "78 [D loss: 0.019069, acc.: 100.00%] [G loss: 4.318460]\n",
      "79 [D loss: 0.016502, acc.: 100.00%] [G loss: 4.282870]\n",
      "80 [D loss: 0.018494, acc.: 100.00%] [G loss: 4.391923]\n",
      "81 [D loss: 0.015672, acc.: 100.00%] [G loss: 4.349297]\n",
      "82 [D loss: 0.015658, acc.: 100.00%] [G loss: 4.188155]\n",
      "83 [D loss: 0.013381, acc.: 100.00%] [G loss: 4.478442]\n",
      "84 [D loss: 0.010782, acc.: 100.00%] [G loss: 4.488807]\n",
      "85 [D loss: 0.010776, acc.: 100.00%] [G loss: 4.485978]\n",
      "86 [D loss: 0.013517, acc.: 100.00%] [G loss: 4.262675]\n",
      "87 [D loss: 0.016620, acc.: 100.00%] [G loss: 4.304198]\n",
      "88 [D loss: 0.027459, acc.: 100.00%] [G loss: 4.398241]\n",
      "89 [D loss: 0.030142, acc.: 100.00%] [G loss: 4.421781]\n",
      "90 [D loss: 0.016613, acc.: 100.00%] [G loss: 4.386185]\n",
      "91 [D loss: 0.012425, acc.: 100.00%] [G loss: 4.312088]\n",
      "92 [D loss: 0.022359, acc.: 100.00%] [G loss: 4.492487]\n",
      "93 [D loss: 0.026745, acc.: 100.00%] [G loss: 4.669976]\n",
      "94 [D loss: 0.009349, acc.: 100.00%] [G loss: 4.759317]\n",
      "95 [D loss: 0.016988, acc.: 100.00%] [G loss: 4.724353]\n",
      "96 [D loss: 0.011541, acc.: 100.00%] [G loss: 4.438161]\n",
      "97 [D loss: 0.008043, acc.: 100.00%] [G loss: 4.586563]\n",
      "98 [D loss: 0.011835, acc.: 100.00%] [G loss: 4.572023]\n",
      "99 [D loss: 0.011461, acc.: 100.00%] [G loss: 4.478390]\n",
      "100 [D loss: 0.011849, acc.: 100.00%] [G loss: 4.639130]\n",
      "101 [D loss: 0.022721, acc.: 100.00%] [G loss: 4.797066]\n",
      "102 [D loss: 0.015297, acc.: 100.00%] [G loss: 4.731039]\n",
      "103 [D loss: 0.020668, acc.: 100.00%] [G loss: 4.634885]\n",
      "104 [D loss: 0.016290, acc.: 100.00%] [G loss: 4.597537]\n",
      "105 [D loss: 0.021405, acc.: 100.00%] [G loss: 4.719994]\n",
      "106 [D loss: 0.015528, acc.: 100.00%] [G loss: 4.739352]\n",
      "107 [D loss: 0.014985, acc.: 100.00%] [G loss: 4.759926]\n",
      "108 [D loss: 0.018291, acc.: 100.00%] [G loss: 4.565520]\n",
      "109 [D loss: 0.026148, acc.: 100.00%] [G loss: 4.476449]\n",
      "110 [D loss: 0.025588, acc.: 100.00%] [G loss: 4.725563]\n",
      "111 [D loss: 0.026770, acc.: 100.00%] [G loss: 4.964568]\n",
      "112 [D loss: 0.053675, acc.: 100.00%] [G loss: 4.751586]\n",
      "113 [D loss: 0.016132, acc.: 100.00%] [G loss: 5.095782]\n",
      "114 [D loss: 0.031475, acc.: 100.00%] [G loss: 4.739113]\n",
      "115 [D loss: 0.017105, acc.: 100.00%] [G loss: 4.994655]\n",
      "116 [D loss: 0.012818, acc.: 100.00%] [G loss: 4.974770]\n",
      "117 [D loss: 0.056803, acc.: 96.88%] [G loss: 5.290926]\n",
      "118 [D loss: 0.088469, acc.: 96.88%] [G loss: 4.090018]\n",
      "119 [D loss: 0.078271, acc.: 96.88%] [G loss: 5.133401]\n",
      "120 [D loss: 0.011870, acc.: 100.00%] [G loss: 5.636772]\n",
      "121 [D loss: 0.144503, acc.: 93.75%] [G loss: 4.951776]\n",
      "122 [D loss: 0.022757, acc.: 100.00%] [G loss: 5.837853]\n",
      "123 [D loss: 0.455922, acc.: 84.38%] [G loss: 4.525193]\n",
      "124 [D loss: 0.024987, acc.: 100.00%] [G loss: 5.193230]\n",
      "125 [D loss: 0.068176, acc.: 96.88%] [G loss: 4.470788]\n",
      "126 [D loss: 0.027053, acc.: 100.00%] [G loss: 4.433661]\n",
      "127 [D loss: 0.026673, acc.: 100.00%] [G loss: 4.618317]\n",
      "128 [D loss: 0.205023, acc.: 84.38%] [G loss: 4.481522]\n",
      "129 [D loss: 0.030843, acc.: 100.00%] [G loss: 5.321379]\n",
      "130 [D loss: 0.401337, acc.: 84.38%] [G loss: 4.553813]\n",
      "131 [D loss: 0.058027, acc.: 100.00%] [G loss: 5.855608]\n",
      "132 [D loss: 1.332366, acc.: 50.00%] [G loss: 2.946905]\n",
      "133 [D loss: 0.341544, acc.: 78.12%] [G loss: 2.923613]\n",
      "134 [D loss: 0.191343, acc.: 87.50%] [G loss: 4.224185]\n",
      "135 [D loss: 0.053149, acc.: 100.00%] [G loss: 4.827802]\n",
      "136 [D loss: 0.066196, acc.: 96.88%] [G loss: 5.202730]\n",
      "137 [D loss: 0.090926, acc.: 96.88%] [G loss: 4.089072]\n",
      "138 [D loss: 0.025596, acc.: 100.00%] [G loss: 3.818057]\n",
      "139 [D loss: 0.120745, acc.: 93.75%] [G loss: 3.966377]\n",
      "140 [D loss: 0.090082, acc.: 100.00%] [G loss: 4.887370]\n",
      "141 [D loss: 0.436752, acc.: 75.00%] [G loss: 3.560684]\n",
      "142 [D loss: 0.073859, acc.: 96.88%] [G loss: 4.106622]\n",
      "143 [D loss: 0.083130, acc.: 100.00%] [G loss: 4.419768]\n",
      "144 [D loss: 0.157110, acc.: 96.88%] [G loss: 3.284298]\n",
      "145 [D loss: 0.102301, acc.: 96.88%] [G loss: 3.825634]\n",
      "146 [D loss: 0.120792, acc.: 96.88%] [G loss: 3.369965]\n",
      "147 [D loss: 0.153722, acc.: 93.75%] [G loss: 2.747446]\n",
      "148 [D loss: 0.095952, acc.: 96.88%] [G loss: 3.395224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.369417, acc.: 84.38%] [G loss: 3.878480]\n",
      "150 [D loss: 1.354254, acc.: 50.00%] [G loss: 3.614594]\n",
      "151 [D loss: 0.111090, acc.: 100.00%] [G loss: 3.629623]\n",
      "152 [D loss: 0.551473, acc.: 71.88%] [G loss: 2.358260]\n",
      "153 [D loss: 0.204672, acc.: 87.50%] [G loss: 2.813787]\n",
      "154 [D loss: 0.175048, acc.: 93.75%] [G loss: 3.734856]\n",
      "155 [D loss: 0.476932, acc.: 75.00%] [G loss: 3.013251]\n",
      "156 [D loss: 0.115857, acc.: 96.88%] [G loss: 3.677480]\n",
      "157 [D loss: 0.249564, acc.: 93.75%] [G loss: 2.640384]\n",
      "158 [D loss: 0.285134, acc.: 90.62%] [G loss: 3.629907]\n",
      "159 [D loss: 0.166087, acc.: 96.88%] [G loss: 3.961783]\n",
      "160 [D loss: 0.249619, acc.: 87.50%] [G loss: 3.017440]\n",
      "161 [D loss: 0.326613, acc.: 84.38%] [G loss: 3.377290]\n",
      "162 [D loss: 0.410821, acc.: 81.25%] [G loss: 3.552097]\n",
      "163 [D loss: 0.176907, acc.: 100.00%] [G loss: 2.808477]\n",
      "164 [D loss: 0.230296, acc.: 90.62%] [G loss: 3.191915]\n",
      "165 [D loss: 0.366565, acc.: 84.38%] [G loss: 2.191305]\n",
      "166 [D loss: 0.232115, acc.: 87.50%] [G loss: 2.920983]\n",
      "167 [D loss: 0.332440, acc.: 84.38%] [G loss: 2.829803]\n",
      "168 [D loss: 0.476322, acc.: 68.75%] [G loss: 3.433935]\n",
      "169 [D loss: 0.153397, acc.: 100.00%] [G loss: 3.065400]\n",
      "170 [D loss: 0.186850, acc.: 93.75%] [G loss: 2.980435]\n",
      "171 [D loss: 0.177509, acc.: 96.88%] [G loss: 2.773821]\n",
      "172 [D loss: 0.141579, acc.: 93.75%] [G loss: 3.107610]\n",
      "173 [D loss: 0.601211, acc.: 71.88%] [G loss: 3.002694]\n",
      "174 [D loss: 0.121179, acc.: 100.00%] [G loss: 2.945154]\n",
      "175 [D loss: 0.307373, acc.: 90.62%] [G loss: 2.618778]\n",
      "176 [D loss: 0.122987, acc.: 96.88%] [G loss: 3.423505]\n",
      "177 [D loss: 0.338664, acc.: 87.50%] [G loss: 2.397406]\n",
      "178 [D loss: 0.204957, acc.: 87.50%] [G loss: 3.167886]\n",
      "179 [D loss: 0.185928, acc.: 96.88%] [G loss: 3.472052]\n",
      "180 [D loss: 0.144346, acc.: 100.00%] [G loss: 3.889638]\n",
      "181 [D loss: 0.434433, acc.: 75.00%] [G loss: 3.535076]\n",
      "182 [D loss: 0.396673, acc.: 87.50%] [G loss: 2.943248]\n",
      "183 [D loss: 0.414382, acc.: 71.88%] [G loss: 4.391051]\n",
      "184 [D loss: 1.755513, acc.: 28.12%] [G loss: 1.864138]\n",
      "185 [D loss: 0.283031, acc.: 81.25%] [G loss: 2.995915]\n",
      "186 [D loss: 0.140811, acc.: 100.00%] [G loss: 2.960101]\n",
      "187 [D loss: 0.192837, acc.: 90.62%] [G loss: 2.751375]\n",
      "188 [D loss: 0.207196, acc.: 87.50%] [G loss: 3.227327]\n",
      "189 [D loss: 0.167849, acc.: 96.88%] [G loss: 2.778818]\n",
      "190 [D loss: 0.160846, acc.: 100.00%] [G loss: 2.760451]\n",
      "191 [D loss: 0.191494, acc.: 100.00%] [G loss: 2.573575]\n",
      "192 [D loss: 0.179999, acc.: 100.00%] [G loss: 3.065495]\n",
      "193 [D loss: 0.959268, acc.: 50.00%] [G loss: 1.648628]\n",
      "194 [D loss: 0.365906, acc.: 71.88%] [G loss: 2.507983]\n",
      "195 [D loss: 0.172889, acc.: 96.88%] [G loss: 2.990286]\n",
      "196 [D loss: 0.269113, acc.: 84.38%] [G loss: 2.996746]\n",
      "197 [D loss: 0.253707, acc.: 96.88%] [G loss: 3.263958]\n",
      "198 [D loss: 0.205902, acc.: 93.75%] [G loss: 2.249669]\n",
      "199 [D loss: 0.227357, acc.: 87.50%] [G loss: 2.370475]\n",
      "200 [D loss: 0.255311, acc.: 90.62%] [G loss: 3.546533]\n",
      "201 [D loss: 0.470021, acc.: 78.12%] [G loss: 2.041712]\n",
      "202 [D loss: 0.169781, acc.: 96.88%] [G loss: 2.987044]\n",
      "203 [D loss: 0.294966, acc.: 84.38%] [G loss: 2.220909]\n",
      "204 [D loss: 0.278809, acc.: 93.75%] [G loss: 3.716720]\n",
      "205 [D loss: 0.457904, acc.: 71.88%] [G loss: 3.040058]\n",
      "206 [D loss: 0.186790, acc.: 100.00%] [G loss: 2.859980]\n",
      "207 [D loss: 0.572087, acc.: 62.50%] [G loss: 3.126799]\n",
      "208 [D loss: 0.265982, acc.: 96.88%] [G loss: 3.585898]\n",
      "209 [D loss: 0.716993, acc.: 59.38%] [G loss: 2.092054]\n",
      "210 [D loss: 0.151320, acc.: 100.00%] [G loss: 2.789460]\n",
      "211 [D loss: 0.497798, acc.: 75.00%] [G loss: 1.999282]\n",
      "212 [D loss: 0.267450, acc.: 87.50%] [G loss: 3.439177]\n",
      "213 [D loss: 0.485406, acc.: 75.00%] [G loss: 1.969656]\n",
      "214 [D loss: 0.203826, acc.: 90.62%] [G loss: 3.035699]\n",
      "215 [D loss: 0.259462, acc.: 93.75%] [G loss: 2.810645]\n",
      "216 [D loss: 0.339765, acc.: 87.50%] [G loss: 2.312150]\n",
      "217 [D loss: 0.172876, acc.: 93.75%] [G loss: 3.393297]\n",
      "218 [D loss: 0.264829, acc.: 90.62%] [G loss: 3.135261]\n",
      "219 [D loss: 0.298878, acc.: 87.50%] [G loss: 3.052014]\n",
      "220 [D loss: 0.369424, acc.: 84.38%] [G loss: 2.215312]\n",
      "221 [D loss: 0.296482, acc.: 84.38%] [G loss: 3.148043]\n",
      "222 [D loss: 0.737610, acc.: 62.50%] [G loss: 1.283522]\n",
      "223 [D loss: 0.334832, acc.: 75.00%] [G loss: 2.698673]\n",
      "224 [D loss: 0.220436, acc.: 90.62%] [G loss: 3.045754]\n",
      "225 [D loss: 0.260913, acc.: 93.75%] [G loss: 2.739605]\n",
      "226 [D loss: 0.231325, acc.: 96.88%] [G loss: 3.009829]\n",
      "227 [D loss: 0.825227, acc.: 56.25%] [G loss: 1.544838]\n",
      "228 [D loss: 0.264290, acc.: 81.25%] [G loss: 3.433864]\n",
      "229 [D loss: 0.626020, acc.: 59.38%] [G loss: 1.765491]\n",
      "230 [D loss: 0.318392, acc.: 81.25%] [G loss: 3.081653]\n",
      "231 [D loss: 0.523083, acc.: 65.62%] [G loss: 2.501172]\n",
      "232 [D loss: 0.211200, acc.: 96.88%] [G loss: 3.133039]\n",
      "233 [D loss: 0.405655, acc.: 84.38%] [G loss: 2.487142]\n",
      "234 [D loss: 0.406075, acc.: 78.12%] [G loss: 2.805546]\n",
      "235 [D loss: 0.413081, acc.: 75.00%] [G loss: 2.508955]\n",
      "236 [D loss: 0.389914, acc.: 78.12%] [G loss: 3.385682]\n",
      "237 [D loss: 0.572036, acc.: 65.62%] [G loss: 2.580847]\n",
      "238 [D loss: 0.584736, acc.: 68.75%] [G loss: 2.368419]\n",
      "239 [D loss: 0.323781, acc.: 90.62%] [G loss: 2.906065]\n",
      "240 [D loss: 0.757446, acc.: 56.25%] [G loss: 2.466371]\n",
      "241 [D loss: 0.257013, acc.: 93.75%] [G loss: 3.316455]\n",
      "242 [D loss: 0.794582, acc.: 56.25%] [G loss: 1.199240]\n",
      "243 [D loss: 0.331436, acc.: 81.25%] [G loss: 2.469741]\n",
      "244 [D loss: 0.219975, acc.: 90.62%] [G loss: 2.854648]\n",
      "245 [D loss: 0.740507, acc.: 56.25%] [G loss: 1.322124]\n",
      "246 [D loss: 0.423210, acc.: 71.88%] [G loss: 3.355909]\n",
      "247 [D loss: 1.264807, acc.: 28.12%] [G loss: 0.858663]\n",
      "248 [D loss: 0.536304, acc.: 62.50%] [G loss: 2.283055]\n",
      "249 [D loss: 0.443504, acc.: 78.12%] [G loss: 2.292357]\n",
      "250 [D loss: 0.671948, acc.: 53.12%] [G loss: 1.522321]\n",
      "251 [D loss: 0.310905, acc.: 90.62%] [G loss: 2.954705]\n",
      "252 [D loss: 1.017543, acc.: 34.38%] [G loss: 1.236741]\n",
      "253 [D loss: 0.346564, acc.: 84.38%] [G loss: 2.405360]\n",
      "254 [D loss: 0.901562, acc.: 37.50%] [G loss: 1.105103]\n",
      "255 [D loss: 0.518569, acc.: 68.75%] [G loss: 2.105379]\n",
      "256 [D loss: 0.677117, acc.: 56.25%] [G loss: 1.592852]\n",
      "257 [D loss: 0.536296, acc.: 68.75%] [G loss: 1.865993]\n",
      "258 [D loss: 0.615817, acc.: 71.88%] [G loss: 1.612484]\n",
      "259 [D loss: 0.689680, acc.: 56.25%] [G loss: 1.347328]\n",
      "260 [D loss: 0.497993, acc.: 75.00%] [G loss: 1.775296]\n",
      "261 [D loss: 0.668023, acc.: 62.50%] [G loss: 1.589481]\n",
      "262 [D loss: 0.792660, acc.: 50.00%] [G loss: 1.334537]\n",
      "263 [D loss: 0.751845, acc.: 46.88%] [G loss: 1.788334]\n",
      "264 [D loss: 0.796975, acc.: 46.88%] [G loss: 1.227386]\n",
      "265 [D loss: 0.658024, acc.: 59.38%] [G loss: 1.635537]\n",
      "266 [D loss: 0.761435, acc.: 50.00%] [G loss: 1.381279]\n",
      "267 [D loss: 0.825640, acc.: 46.88%] [G loss: 1.061408]\n",
      "268 [D loss: 0.631644, acc.: 56.25%] [G loss: 1.261780]\n",
      "269 [D loss: 0.790740, acc.: 43.75%] [G loss: 1.096080]\n",
      "270 [D loss: 0.644617, acc.: 62.50%] [G loss: 1.513229]\n",
      "271 [D loss: 0.629853, acc.: 56.25%] [G loss: 1.575803]\n",
      "272 [D loss: 0.945290, acc.: 28.12%] [G loss: 0.703034]\n",
      "273 [D loss: 0.654770, acc.: 56.25%] [G loss: 1.244594]\n",
      "274 [D loss: 0.668262, acc.: 56.25%] [G loss: 1.209503]\n",
      "275 [D loss: 0.657741, acc.: 62.50%] [G loss: 1.257980]\n",
      "276 [D loss: 0.755508, acc.: 40.62%] [G loss: 0.985523]\n",
      "277 [D loss: 0.684011, acc.: 56.25%] [G loss: 1.115867]\n",
      "278 [D loss: 0.636202, acc.: 56.25%] [G loss: 1.387397]\n",
      "279 [D loss: 0.666521, acc.: 56.25%] [G loss: 1.222361]\n",
      "280 [D loss: 0.742777, acc.: 53.12%] [G loss: 1.030031]\n",
      "281 [D loss: 0.669601, acc.: 59.38%] [G loss: 1.432051]\n",
      "282 [D loss: 1.043963, acc.: 25.00%] [G loss: 0.654804]\n",
      "283 [D loss: 0.729666, acc.: 43.75%] [G loss: 0.953870]\n",
      "284 [D loss: 0.724310, acc.: 40.62%] [G loss: 1.062276]\n",
      "285 [D loss: 1.049548, acc.: 25.00%] [G loss: 0.690226]\n",
      "286 [D loss: 0.632697, acc.: 62.50%] [G loss: 0.923335]\n",
      "287 [D loss: 0.828003, acc.: 43.75%] [G loss: 0.809145]\n",
      "288 [D loss: 0.714836, acc.: 50.00%] [G loss: 0.883567]\n",
      "289 [D loss: 0.726767, acc.: 62.50%] [G loss: 1.038517]\n",
      "290 [D loss: 0.814639, acc.: 37.50%] [G loss: 0.869515]\n",
      "291 [D loss: 0.828996, acc.: 40.62%] [G loss: 0.732082]\n",
      "292 [D loss: 0.632900, acc.: 53.12%] [G loss: 1.016446]\n",
      "293 [D loss: 0.850101, acc.: 28.12%] [G loss: 0.723762]\n",
      "294 [D loss: 0.741589, acc.: 46.88%] [G loss: 0.827544]\n",
      "295 [D loss: 0.795876, acc.: 46.88%] [G loss: 0.759915]\n",
      "296 [D loss: 0.783205, acc.: 37.50%] [G loss: 0.805964]\n",
      "297 [D loss: 0.837473, acc.: 34.38%] [G loss: 0.705229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 [D loss: 0.744286, acc.: 40.62%] [G loss: 0.725923]\n",
      "299 [D loss: 0.767985, acc.: 43.75%] [G loss: 0.729696]\n",
      "300 [D loss: 0.614529, acc.: 53.12%] [G loss: 0.950136]\n",
      "301 [D loss: 0.773065, acc.: 37.50%] [G loss: 0.844122]\n",
      "302 [D loss: 0.761313, acc.: 43.75%] [G loss: 0.724131]\n",
      "303 [D loss: 0.873984, acc.: 25.00%] [G loss: 0.602031]\n",
      "304 [D loss: 0.769646, acc.: 37.50%] [G loss: 0.653271]\n",
      "305 [D loss: 0.754203, acc.: 37.50%] [G loss: 0.680697]\n",
      "306 [D loss: 0.703333, acc.: 43.75%] [G loss: 0.798347]\n",
      "307 [D loss: 0.856223, acc.: 21.88%] [G loss: 0.671659]\n",
      "308 [D loss: 0.722055, acc.: 46.88%] [G loss: 0.708060]\n",
      "309 [D loss: 0.832800, acc.: 31.25%] [G loss: 0.623030]\n",
      "310 [D loss: 0.773714, acc.: 40.62%] [G loss: 0.622650]\n",
      "311 [D loss: 0.715605, acc.: 50.00%] [G loss: 0.667351]\n",
      "312 [D loss: 0.729464, acc.: 40.62%] [G loss: 0.720230]\n",
      "313 [D loss: 0.767680, acc.: 43.75%] [G loss: 0.653711]\n",
      "314 [D loss: 0.721303, acc.: 37.50%] [G loss: 0.655999]\n",
      "315 [D loss: 0.730025, acc.: 37.50%] [G loss: 0.677624]\n",
      "316 [D loss: 0.723127, acc.: 40.62%] [G loss: 0.690481]\n",
      "317 [D loss: 0.743689, acc.: 40.62%] [G loss: 0.676775]\n",
      "318 [D loss: 0.755130, acc.: 37.50%] [G loss: 0.648832]\n",
      "319 [D loss: 0.689728, acc.: 50.00%] [G loss: 0.658497]\n",
      "320 [D loss: 0.748886, acc.: 43.75%] [G loss: 0.669280]\n",
      "321 [D loss: 0.721574, acc.: 46.88%] [G loss: 0.687794]\n",
      "322 [D loss: 0.731929, acc.: 46.88%] [G loss: 0.678668]\n",
      "323 [D loss: 0.692530, acc.: 50.00%] [G loss: 0.691232]\n",
      "324 [D loss: 0.651074, acc.: 59.38%] [G loss: 0.689291]\n",
      "325 [D loss: 0.699379, acc.: 50.00%] [G loss: 0.732785]\n",
      "326 [D loss: 0.721841, acc.: 46.88%] [G loss: 0.690132]\n",
      "327 [D loss: 0.667461, acc.: 62.50%] [G loss: 0.694860]\n",
      "328 [D loss: 0.690153, acc.: 43.75%] [G loss: 0.708375]\n",
      "329 [D loss: 0.723383, acc.: 43.75%] [G loss: 0.698681]\n",
      "330 [D loss: 0.722899, acc.: 37.50%] [G loss: 0.665460]\n",
      "331 [D loss: 0.726660, acc.: 40.62%] [G loss: 0.666026]\n",
      "332 [D loss: 0.706949, acc.: 43.75%] [G loss: 0.693538]\n",
      "333 [D loss: 0.695418, acc.: 53.12%] [G loss: 0.689761]\n",
      "334 [D loss: 0.705626, acc.: 46.88%] [G loss: 0.685762]\n",
      "335 [D loss: 0.676776, acc.: 46.88%] [G loss: 0.688106]\n",
      "336 [D loss: 0.680059, acc.: 43.75%] [G loss: 0.716689]\n",
      "337 [D loss: 0.726888, acc.: 43.75%] [G loss: 0.657469]\n",
      "338 [D loss: 0.727640, acc.: 40.62%] [G loss: 0.638038]\n",
      "339 [D loss: 0.739946, acc.: 37.50%] [G loss: 0.610108]\n",
      "340 [D loss: 0.753391, acc.: 43.75%] [G loss: 0.618105]\n",
      "341 [D loss: 0.678921, acc.: 46.88%] [G loss: 0.647791]\n",
      "342 [D loss: 0.697271, acc.: 40.62%] [G loss: 0.636323]\n",
      "343 [D loss: 0.705781, acc.: 43.75%] [G loss: 0.663379]\n",
      "344 [D loss: 0.714981, acc.: 43.75%] [G loss: 0.689468]\n",
      "345 [D loss: 0.691523, acc.: 43.75%] [G loss: 0.702872]\n",
      "346 [D loss: 0.738798, acc.: 37.50%] [G loss: 0.644121]\n",
      "347 [D loss: 0.727549, acc.: 40.62%] [G loss: 0.652464]\n",
      "348 [D loss: 0.682338, acc.: 46.88%] [G loss: 0.682935]\n",
      "349 [D loss: 0.686845, acc.: 50.00%] [G loss: 0.685856]\n",
      "350 [D loss: 0.713279, acc.: 46.88%] [G loss: 0.663037]\n",
      "351 [D loss: 0.708409, acc.: 43.75%] [G loss: 0.648700]\n",
      "352 [D loss: 0.723546, acc.: 37.50%] [G loss: 0.653966]\n",
      "353 [D loss: 0.695246, acc.: 46.88%] [G loss: 0.652198]\n",
      "354 [D loss: 0.659420, acc.: 50.00%] [G loss: 0.672080]\n",
      "355 [D loss: 0.705203, acc.: 53.12%] [G loss: 0.660890]\n",
      "356 [D loss: 0.690441, acc.: 50.00%] [G loss: 0.643702]\n",
      "357 [D loss: 0.706788, acc.: 43.75%] [G loss: 0.641060]\n",
      "358 [D loss: 0.687841, acc.: 50.00%] [G loss: 0.639812]\n",
      "359 [D loss: 0.706155, acc.: 53.12%] [G loss: 0.651560]\n",
      "360 [D loss: 0.669123, acc.: 56.25%] [G loss: 0.658575]\n",
      "361 [D loss: 0.716986, acc.: 43.75%] [G loss: 0.638069]\n",
      "362 [D loss: 0.652058, acc.: 50.00%] [G loss: 0.668119]\n",
      "363 [D loss: 0.673708, acc.: 50.00%] [G loss: 0.701054]\n",
      "364 [D loss: 0.723496, acc.: 43.75%] [G loss: 0.656092]\n",
      "365 [D loss: 0.750461, acc.: 37.50%] [G loss: 0.614231]\n",
      "366 [D loss: 0.716330, acc.: 43.75%] [G loss: 0.602406]\n",
      "367 [D loss: 0.677419, acc.: 50.00%] [G loss: 0.654331]\n",
      "368 [D loss: 0.695300, acc.: 46.88%] [G loss: 0.643008]\n",
      "369 [D loss: 0.703058, acc.: 46.88%] [G loss: 0.639699]\n",
      "370 [D loss: 0.698882, acc.: 46.88%] [G loss: 0.642829]\n",
      "371 [D loss: 0.710040, acc.: 43.75%] [G loss: 0.629578]\n",
      "372 [D loss: 0.712420, acc.: 43.75%] [G loss: 0.610975]\n",
      "373 [D loss: 0.683689, acc.: 50.00%] [G loss: 0.601916]\n",
      "374 [D loss: 0.677825, acc.: 46.88%] [G loss: 0.624186]\n",
      "375 [D loss: 0.709883, acc.: 46.88%] [G loss: 0.628745]\n",
      "376 [D loss: 0.688085, acc.: 46.88%] [G loss: 0.646804]\n",
      "377 [D loss: 0.659509, acc.: 50.00%] [G loss: 0.647666]\n",
      "378 [D loss: 0.715623, acc.: 43.75%] [G loss: 0.650072]\n",
      "379 [D loss: 0.703640, acc.: 46.88%] [G loss: 0.648163]\n",
      "380 [D loss: 0.698973, acc.: 46.88%] [G loss: 0.638443]\n",
      "381 [D loss: 0.714183, acc.: 43.75%] [G loss: 0.617381]\n",
      "382 [D loss: 0.689885, acc.: 46.88%] [G loss: 0.619558]\n",
      "383 [D loss: 0.704242, acc.: 50.00%] [G loss: 0.620669]\n",
      "384 [D loss: 0.687103, acc.: 50.00%] [G loss: 0.639120]\n",
      "385 [D loss: 0.701412, acc.: 50.00%] [G loss: 0.629156]\n",
      "386 [D loss: 0.675540, acc.: 50.00%] [G loss: 0.638724]\n",
      "387 [D loss: 0.686279, acc.: 50.00%] [G loss: 0.635299]\n",
      "388 [D loss: 0.689902, acc.: 43.75%] [G loss: 0.639589]\n",
      "389 [D loss: 0.706904, acc.: 50.00%] [G loss: 0.635218]\n",
      "390 [D loss: 0.709176, acc.: 43.75%] [G loss: 0.636360]\n",
      "391 [D loss: 0.698627, acc.: 46.88%] [G loss: 0.632605]\n",
      "392 [D loss: 0.686743, acc.: 46.88%] [G loss: 0.647381]\n",
      "393 [D loss: 0.645866, acc.: 50.00%] [G loss: 0.672873]\n",
      "394 [D loss: 0.662727, acc.: 46.88%] [G loss: 0.686490]\n",
      "395 [D loss: 0.665508, acc.: 53.12%] [G loss: 0.686725]\n",
      "396 [D loss: 0.679508, acc.: 46.88%] [G loss: 0.666319]\n",
      "397 [D loss: 0.672345, acc.: 50.00%] [G loss: 0.664942]\n",
      "398 [D loss: 0.682476, acc.: 53.12%] [G loss: 0.654237]\n",
      "399 [D loss: 0.662331, acc.: 53.12%] [G loss: 0.655346]\n",
      "400 [D loss: 0.686620, acc.: 50.00%] [G loss: 0.659219]\n",
      "401 [D loss: 0.663725, acc.: 50.00%] [G loss: 0.655888]\n",
      "402 [D loss: 0.675283, acc.: 50.00%] [G loss: 0.654867]\n",
      "403 [D loss: 0.680441, acc.: 46.88%] [G loss: 0.645675]\n",
      "404 [D loss: 0.661013, acc.: 46.88%] [G loss: 0.652824]\n",
      "405 [D loss: 0.694834, acc.: 40.62%] [G loss: 0.639850]\n",
      "406 [D loss: 0.675794, acc.: 46.88%] [G loss: 0.647002]\n",
      "407 [D loss: 0.681695, acc.: 50.00%] [G loss: 0.655942]\n",
      "408 [D loss: 0.661462, acc.: 46.88%] [G loss: 0.680135]\n",
      "409 [D loss: 0.659432, acc.: 50.00%] [G loss: 0.670114]\n",
      "410 [D loss: 0.700224, acc.: 46.88%] [G loss: 0.662325]\n",
      "411 [D loss: 0.678566, acc.: 50.00%] [G loss: 0.645693]\n",
      "412 [D loss: 0.671568, acc.: 53.12%] [G loss: 0.642058]\n",
      "413 [D loss: 0.654964, acc.: 50.00%] [G loss: 0.652515]\n",
      "414 [D loss: 0.667774, acc.: 53.12%] [G loss: 0.678859]\n",
      "415 [D loss: 0.673258, acc.: 50.00%] [G loss: 0.665956]\n",
      "416 [D loss: 0.668360, acc.: 50.00%] [G loss: 0.686568]\n",
      "417 [D loss: 0.658640, acc.: 53.12%] [G loss: 0.692762]\n",
      "418 [D loss: 0.674560, acc.: 53.12%] [G loss: 0.668090]\n",
      "419 [D loss: 0.659420, acc.: 46.88%] [G loss: 0.667960]\n",
      "420 [D loss: 0.665123, acc.: 50.00%] [G loss: 0.649911]\n",
      "421 [D loss: 0.659722, acc.: 50.00%] [G loss: 0.660348]\n",
      "422 [D loss: 0.674410, acc.: 46.88%] [G loss: 0.658038]\n",
      "423 [D loss: 0.677834, acc.: 50.00%] [G loss: 0.650215]\n",
      "424 [D loss: 0.676771, acc.: 46.88%] [G loss: 0.643526]\n",
      "425 [D loss: 0.667626, acc.: 50.00%] [G loss: 0.662713]\n",
      "426 [D loss: 0.672539, acc.: 50.00%] [G loss: 0.673796]\n",
      "427 [D loss: 0.657818, acc.: 46.88%] [G loss: 0.657603]\n",
      "428 [D loss: 0.648847, acc.: 46.88%] [G loss: 0.681236]\n",
      "429 [D loss: 0.677951, acc.: 43.75%] [G loss: 0.670016]\n",
      "430 [D loss: 0.675922, acc.: 46.88%] [G loss: 0.670235]\n",
      "431 [D loss: 0.672204, acc.: 46.88%] [G loss: 0.663332]\n",
      "432 [D loss: 0.656463, acc.: 46.88%] [G loss: 0.652183]\n",
      "433 [D loss: 0.662825, acc.: 50.00%] [G loss: 0.639324]\n",
      "434 [D loss: 0.691914, acc.: 50.00%] [G loss: 0.649682]\n",
      "435 [D loss: 0.670992, acc.: 50.00%] [G loss: 0.671304]\n",
      "436 [D loss: 0.676383, acc.: 46.88%] [G loss: 0.658494]\n",
      "437 [D loss: 0.661208, acc.: 50.00%] [G loss: 0.656757]\n",
      "438 [D loss: 0.663992, acc.: 46.88%] [G loss: 0.668424]\n",
      "439 [D loss: 0.659641, acc.: 50.00%] [G loss: 0.690642]\n",
      "440 [D loss: 0.669819, acc.: 43.75%] [G loss: 0.679711]\n",
      "441 [D loss: 0.667083, acc.: 53.12%] [G loss: 0.685025]\n",
      "442 [D loss: 0.649036, acc.: 53.12%] [G loss: 0.682430]\n",
      "443 [D loss: 0.684263, acc.: 56.25%] [G loss: 0.669905]\n",
      "444 [D loss: 0.662096, acc.: 43.75%] [G loss: 0.668361]\n",
      "445 [D loss: 0.649560, acc.: 53.12%] [G loss: 0.667215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 [D loss: 0.664754, acc.: 53.12%] [G loss: 0.672377]\n",
      "447 [D loss: 0.648059, acc.: 56.25%] [G loss: 0.661273]\n",
      "448 [D loss: 0.667011, acc.: 46.88%] [G loss: 0.672075]\n",
      "449 [D loss: 0.678079, acc.: 56.25%] [G loss: 0.679965]\n",
      "450 [D loss: 0.643066, acc.: 53.12%] [G loss: 0.704520]\n",
      "451 [D loss: 0.647444, acc.: 65.62%] [G loss: 0.716516]\n",
      "452 [D loss: 0.668329, acc.: 53.12%] [G loss: 0.716248]\n",
      "453 [D loss: 0.655367, acc.: 65.62%] [G loss: 0.718661]\n",
      "454 [D loss: 0.642480, acc.: 71.88%] [G loss: 0.698003]\n",
      "455 [D loss: 0.672721, acc.: 56.25%] [G loss: 0.687448]\n",
      "456 [D loss: 0.678614, acc.: 56.25%] [G loss: 0.681117]\n",
      "457 [D loss: 0.645864, acc.: 62.50%] [G loss: 0.688395]\n",
      "458 [D loss: 0.698021, acc.: 50.00%] [G loss: 0.680559]\n",
      "459 [D loss: 0.664455, acc.: 50.00%] [G loss: 0.689848]\n",
      "460 [D loss: 0.664225, acc.: 46.88%] [G loss: 0.680749]\n",
      "461 [D loss: 0.658230, acc.: 56.25%] [G loss: 0.681917]\n",
      "462 [D loss: 0.645291, acc.: 59.38%] [G loss: 0.678644]\n",
      "463 [D loss: 0.646046, acc.: 46.88%] [G loss: 0.673954]\n",
      "464 [D loss: 0.639777, acc.: 65.62%] [G loss: 0.671234]\n",
      "465 [D loss: 0.652457, acc.: 59.38%] [G loss: 0.704227]\n",
      "466 [D loss: 0.628944, acc.: 65.62%] [G loss: 0.698371]\n",
      "467 [D loss: 0.673009, acc.: 62.50%] [G loss: 0.683507]\n",
      "468 [D loss: 0.710135, acc.: 56.25%] [G loss: 0.686535]\n",
      "469 [D loss: 0.668259, acc.: 62.50%] [G loss: 0.685052]\n",
      "470 [D loss: 0.662772, acc.: 59.38%] [G loss: 0.697712]\n",
      "471 [D loss: 0.670160, acc.: 56.25%] [G loss: 0.696831]\n",
      "472 [D loss: 0.665891, acc.: 53.12%] [G loss: 0.686820]\n",
      "473 [D loss: 0.651322, acc.: 62.50%] [G loss: 0.700917]\n",
      "474 [D loss: 0.664307, acc.: 53.12%] [G loss: 0.666267]\n",
      "475 [D loss: 0.692611, acc.: 40.62%] [G loss: 0.649626]\n",
      "476 [D loss: 0.622194, acc.: 62.50%] [G loss: 0.679857]\n",
      "477 [D loss: 0.688189, acc.: 53.12%] [G loss: 0.677031]\n",
      "478 [D loss: 0.632106, acc.: 65.62%] [G loss: 0.661547]\n",
      "479 [D loss: 0.663887, acc.: 59.38%] [G loss: 0.677591]\n",
      "480 [D loss: 0.636579, acc.: 56.25%] [G loss: 0.697508]\n",
      "481 [D loss: 0.677968, acc.: 56.25%] [G loss: 0.687152]\n",
      "482 [D loss: 0.682344, acc.: 59.38%] [G loss: 0.686854]\n",
      "483 [D loss: 0.714906, acc.: 56.25%] [G loss: 0.733398]\n",
      "484 [D loss: 0.672558, acc.: 53.12%] [G loss: 0.741281]\n",
      "485 [D loss: 0.716864, acc.: 43.75%] [G loss: 0.722389]\n",
      "486 [D loss: 0.665657, acc.: 50.00%] [G loss: 0.716722]\n",
      "487 [D loss: 0.672467, acc.: 46.88%] [G loss: 0.712766]\n",
      "488 [D loss: 0.665618, acc.: 50.00%] [G loss: 0.703229]\n",
      "489 [D loss: 0.670505, acc.: 56.25%] [G loss: 0.723561]\n",
      "490 [D loss: 0.678943, acc.: 46.88%] [G loss: 0.689164]\n",
      "491 [D loss: 0.637821, acc.: 56.25%] [G loss: 0.698729]\n",
      "492 [D loss: 0.671970, acc.: 53.12%] [G loss: 0.680820]\n",
      "493 [D loss: 0.648573, acc.: 53.12%] [G loss: 0.701305]\n",
      "494 [D loss: 0.673581, acc.: 50.00%] [G loss: 0.727039]\n",
      "495 [D loss: 0.637468, acc.: 46.88%] [G loss: 0.741871]\n",
      "496 [D loss: 0.691356, acc.: 43.75%] [G loss: 0.711413]\n",
      "497 [D loss: 0.642070, acc.: 53.12%] [G loss: 0.703959]\n",
      "498 [D loss: 0.635890, acc.: 56.25%] [G loss: 0.711511]\n",
      "499 [D loss: 0.660020, acc.: 46.88%] [G loss: 0.716229]\n",
      "500 [D loss: 0.634181, acc.: 59.38%] [G loss: 0.706613]\n",
      "501 [D loss: 0.631854, acc.: 53.12%] [G loss: 0.739053]\n",
      "502 [D loss: 0.624141, acc.: 62.50%] [G loss: 0.762286]\n",
      "503 [D loss: 0.650661, acc.: 59.38%] [G loss: 0.778686]\n",
      "504 [D loss: 0.649286, acc.: 62.50%] [G loss: 0.727726]\n",
      "505 [D loss: 0.666005, acc.: 46.88%] [G loss: 0.722686]\n",
      "506 [D loss: 0.644185, acc.: 59.38%] [G loss: 0.716812]\n",
      "507 [D loss: 0.648428, acc.: 50.00%] [G loss: 0.705936]\n",
      "508 [D loss: 0.652668, acc.: 50.00%] [G loss: 0.696314]\n",
      "509 [D loss: 0.649222, acc.: 56.25%] [G loss: 0.710636]\n",
      "510 [D loss: 0.636320, acc.: 53.12%] [G loss: 0.702941]\n",
      "511 [D loss: 0.630542, acc.: 56.25%] [G loss: 0.719330]\n",
      "512 [D loss: 0.691942, acc.: 46.88%] [G loss: 0.700624]\n",
      "513 [D loss: 0.656892, acc.: 46.88%] [G loss: 0.682126]\n",
      "514 [D loss: 0.684348, acc.: 46.88%] [G loss: 0.686235]\n",
      "515 [D loss: 0.645227, acc.: 56.25%] [G loss: 0.712433]\n",
      "516 [D loss: 0.650275, acc.: 56.25%] [G loss: 0.701378]\n",
      "517 [D loss: 0.646445, acc.: 59.38%] [G loss: 0.703958]\n",
      "518 [D loss: 0.623628, acc.: 62.50%] [G loss: 0.697269]\n",
      "519 [D loss: 0.643641, acc.: 50.00%] [G loss: 0.687351]\n",
      "520 [D loss: 0.697400, acc.: 46.88%] [G loss: 0.686930]\n",
      "521 [D loss: 0.663868, acc.: 46.88%] [G loss: 0.734147]\n",
      "522 [D loss: 0.702319, acc.: 46.88%] [G loss: 0.739261]\n",
      "523 [D loss: 0.705743, acc.: 46.88%] [G loss: 0.715210]\n",
      "524 [D loss: 0.655718, acc.: 56.25%] [G loss: 0.732904]\n",
      "525 [D loss: 0.727669, acc.: 40.62%] [G loss: 0.792679]\n",
      "526 [D loss: 0.671448, acc.: 53.12%] [G loss: 0.813578]\n",
      "527 [D loss: 0.647384, acc.: 71.88%] [G loss: 0.774062]\n",
      "528 [D loss: 0.749826, acc.: 28.12%] [G loss: 0.651770]\n",
      "529 [D loss: 0.684156, acc.: 43.75%] [G loss: 0.645782]\n",
      "530 [D loss: 0.693274, acc.: 40.62%] [G loss: 0.647192]\n",
      "531 [D loss: 0.660727, acc.: 56.25%] [G loss: 0.664806]\n",
      "532 [D loss: 0.670679, acc.: 53.12%] [G loss: 0.655057]\n",
      "533 [D loss: 0.679945, acc.: 46.88%] [G loss: 0.693922]\n",
      "534 [D loss: 0.692427, acc.: 53.12%] [G loss: 0.679522]\n",
      "535 [D loss: 0.655076, acc.: 53.12%] [G loss: 0.706035]\n",
      "536 [D loss: 0.714217, acc.: 37.50%] [G loss: 0.704729]\n",
      "537 [D loss: 0.676128, acc.: 50.00%] [G loss: 0.690321]\n",
      "538 [D loss: 0.701309, acc.: 50.00%] [G loss: 0.709747]\n",
      "539 [D loss: 0.730371, acc.: 31.25%] [G loss: 0.728292]\n",
      "540 [D loss: 0.676750, acc.: 56.25%] [G loss: 0.763995]\n",
      "541 [D loss: 0.717471, acc.: 50.00%] [G loss: 0.715923]\n",
      "542 [D loss: 0.678778, acc.: 46.88%] [G loss: 0.685566]\n",
      "543 [D loss: 0.710046, acc.: 40.62%] [G loss: 0.658441]\n",
      "544 [D loss: 0.701126, acc.: 43.75%] [G loss: 0.706677]\n",
      "545 [D loss: 0.699490, acc.: 43.75%] [G loss: 0.735748]\n",
      "546 [D loss: 0.651575, acc.: 59.38%] [G loss: 0.781139]\n",
      "547 [D loss: 0.701885, acc.: 43.75%] [G loss: 0.727696]\n",
      "548 [D loss: 0.608494, acc.: 62.50%] [G loss: 0.731142]\n",
      "549 [D loss: 0.642001, acc.: 56.25%] [G loss: 0.759114]\n",
      "550 [D loss: 0.702967, acc.: 43.75%] [G loss: 0.692608]\n",
      "551 [D loss: 0.670040, acc.: 50.00%] [G loss: 0.730747]\n",
      "552 [D loss: 0.637641, acc.: 59.38%] [G loss: 0.739351]\n",
      "553 [D loss: 0.671858, acc.: 40.62%] [G loss: 0.710178]\n",
      "554 [D loss: 0.665257, acc.: 50.00%] [G loss: 0.672967]\n",
      "555 [D loss: 0.660830, acc.: 56.25%] [G loss: 0.659309]\n",
      "556 [D loss: 0.621798, acc.: 71.88%] [G loss: 0.653291]\n",
      "557 [D loss: 0.664823, acc.: 62.50%] [G loss: 0.650748]\n",
      "558 [D loss: 0.664469, acc.: 65.62%] [G loss: 0.646143]\n",
      "559 [D loss: 0.663460, acc.: 59.38%] [G loss: 0.678378]\n",
      "560 [D loss: 0.641083, acc.: 59.38%] [G loss: 0.686697]\n",
      "561 [D loss: 0.644236, acc.: 62.50%] [G loss: 0.699761]\n",
      "562 [D loss: 0.686408, acc.: 56.25%] [G loss: 0.702763]\n",
      "563 [D loss: 0.692277, acc.: 46.88%] [G loss: 0.751239]\n",
      "564 [D loss: 0.637009, acc.: 56.25%] [G loss: 0.803911]\n",
      "565 [D loss: 0.684607, acc.: 50.00%] [G loss: 0.778807]\n",
      "566 [D loss: 0.707519, acc.: 50.00%] [G loss: 0.748325]\n",
      "567 [D loss: 0.703708, acc.: 43.75%] [G loss: 0.719295]\n",
      "568 [D loss: 0.671925, acc.: 43.75%] [G loss: 0.715453]\n",
      "569 [D loss: 0.685439, acc.: 50.00%] [G loss: 0.664865]\n",
      "570 [D loss: 0.653651, acc.: 53.12%] [G loss: 0.681372]\n",
      "571 [D loss: 0.646957, acc.: 56.25%] [G loss: 0.696192]\n",
      "572 [D loss: 0.649433, acc.: 53.12%] [G loss: 0.716360]\n",
      "573 [D loss: 0.623693, acc.: 65.62%] [G loss: 0.750091]\n",
      "574 [D loss: 0.620251, acc.: 75.00%] [G loss: 0.758118]\n",
      "575 [D loss: 0.695896, acc.: 56.25%] [G loss: 0.737788]\n",
      "576 [D loss: 0.661627, acc.: 59.38%] [G loss: 0.710910]\n",
      "577 [D loss: 0.666203, acc.: 50.00%] [G loss: 0.707755]\n",
      "578 [D loss: 0.660128, acc.: 53.12%] [G loss: 0.742987]\n",
      "579 [D loss: 0.672861, acc.: 46.88%] [G loss: 0.711224]\n",
      "580 [D loss: 0.639331, acc.: 65.62%] [G loss: 0.699179]\n",
      "581 [D loss: 0.669599, acc.: 43.75%] [G loss: 0.685532]\n",
      "582 [D loss: 0.691809, acc.: 56.25%] [G loss: 0.727075]\n",
      "583 [D loss: 0.677783, acc.: 50.00%] [G loss: 0.724872]\n",
      "584 [D loss: 0.665303, acc.: 68.75%] [G loss: 0.744300]\n",
      "585 [D loss: 0.641719, acc.: 78.12%] [G loss: 0.748178]\n",
      "586 [D loss: 0.658394, acc.: 62.50%] [G loss: 0.707813]\n",
      "587 [D loss: 0.710048, acc.: 62.50%] [G loss: 0.690970]\n",
      "588 [D loss: 0.628925, acc.: 68.75%] [G loss: 0.709265]\n",
      "589 [D loss: 0.674497, acc.: 62.50%] [G loss: 0.698447]\n",
      "590 [D loss: 0.671811, acc.: 65.62%] [G loss: 0.674301]\n",
      "591 [D loss: 0.717597, acc.: 56.25%] [G loss: 0.705920]\n",
      "592 [D loss: 0.657565, acc.: 50.00%] [G loss: 0.681964]\n",
      "593 [D loss: 0.662134, acc.: 53.12%] [G loss: 0.699846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594 [D loss: 0.678114, acc.: 50.00%] [G loss: 0.713923]\n",
      "595 [D loss: 0.625403, acc.: 75.00%] [G loss: 0.721043]\n",
      "596 [D loss: 0.632130, acc.: 65.62%] [G loss: 0.690931]\n",
      "597 [D loss: 0.651274, acc.: 56.25%] [G loss: 0.670592]\n",
      "598 [D loss: 0.664446, acc.: 50.00%] [G loss: 0.683542]\n",
      "599 [D loss: 0.649318, acc.: 56.25%] [G loss: 0.709596]\n",
      "600 [D loss: 0.663244, acc.: 53.12%] [G loss: 0.688307]\n",
      "601 [D loss: 0.677596, acc.: 50.00%] [G loss: 0.707338]\n",
      "602 [D loss: 0.660056, acc.: 62.50%] [G loss: 0.691089]\n",
      "603 [D loss: 0.691200, acc.: 62.50%] [G loss: 0.729094]\n",
      "604 [D loss: 0.690358, acc.: 46.88%] [G loss: 0.803655]\n",
      "605 [D loss: 0.638366, acc.: 75.00%] [G loss: 0.802583]\n",
      "606 [D loss: 0.666099, acc.: 65.62%] [G loss: 0.729439]\n",
      "607 [D loss: 0.671577, acc.: 56.25%] [G loss: 0.753449]\n",
      "608 [D loss: 0.649054, acc.: 65.62%] [G loss: 0.748643]\n",
      "609 [D loss: 0.740645, acc.: 34.38%] [G loss: 0.736203]\n",
      "610 [D loss: 0.637874, acc.: 56.25%] [G loss: 0.727694]\n",
      "611 [D loss: 0.673338, acc.: 53.12%] [G loss: 0.694001]\n",
      "612 [D loss: 0.680206, acc.: 53.12%] [G loss: 0.701337]\n",
      "613 [D loss: 0.629485, acc.: 62.50%] [G loss: 0.723866]\n",
      "614 [D loss: 0.693599, acc.: 53.12%] [G loss: 0.731224]\n",
      "615 [D loss: 0.625759, acc.: 59.38%] [G loss: 0.725634]\n",
      "616 [D loss: 0.720904, acc.: 43.75%] [G loss: 0.723377]\n",
      "617 [D loss: 0.666603, acc.: 65.62%] [G loss: 0.696563]\n",
      "618 [D loss: 0.687435, acc.: 53.12%] [G loss: 0.710236]\n",
      "619 [D loss: 0.655178, acc.: 65.62%] [G loss: 0.696925]\n",
      "620 [D loss: 0.664612, acc.: 56.25%] [G loss: 0.713618]\n",
      "621 [D loss: 0.662455, acc.: 56.25%] [G loss: 0.714305]\n",
      "622 [D loss: 0.643567, acc.: 59.38%] [G loss: 0.745685]\n",
      "623 [D loss: 0.668008, acc.: 53.12%] [G loss: 0.741022]\n",
      "624 [D loss: 0.680408, acc.: 46.88%] [G loss: 0.771223]\n",
      "625 [D loss: 0.621436, acc.: 68.75%] [G loss: 0.777963]\n",
      "626 [D loss: 0.671523, acc.: 53.12%] [G loss: 0.793192]\n",
      "627 [D loss: 0.636567, acc.: 65.62%] [G loss: 0.751179]\n",
      "628 [D loss: 0.636408, acc.: 65.62%] [G loss: 0.716784]\n",
      "629 [D loss: 0.675339, acc.: 56.25%] [G loss: 0.709066]\n",
      "630 [D loss: 0.631020, acc.: 62.50%] [G loss: 0.749361]\n",
      "631 [D loss: 0.666450, acc.: 53.12%] [G loss: 0.735154]\n",
      "632 [D loss: 0.667703, acc.: 56.25%] [G loss: 0.745655]\n",
      "633 [D loss: 0.655433, acc.: 68.75%] [G loss: 0.783393]\n",
      "634 [D loss: 0.677912, acc.: 53.12%] [G loss: 0.742206]\n",
      "635 [D loss: 0.650731, acc.: 62.50%] [G loss: 0.726454]\n",
      "636 [D loss: 0.668902, acc.: 65.62%] [G loss: 0.738629]\n",
      "637 [D loss: 0.682041, acc.: 46.88%] [G loss: 0.709708]\n",
      "638 [D loss: 0.667828, acc.: 56.25%] [G loss: 0.726130]\n",
      "639 [D loss: 0.641966, acc.: 75.00%] [G loss: 0.705962]\n",
      "640 [D loss: 0.645908, acc.: 78.12%] [G loss: 0.724790]\n",
      "641 [D loss: 0.637652, acc.: 65.62%] [G loss: 0.680735]\n",
      "642 [D loss: 0.684002, acc.: 68.75%] [G loss: 0.730072]\n",
      "643 [D loss: 0.617270, acc.: 75.00%] [G loss: 0.709961]\n",
      "644 [D loss: 0.737524, acc.: 65.62%] [G loss: 0.753570]\n",
      "645 [D loss: 0.617213, acc.: 78.12%] [G loss: 0.744900]\n",
      "646 [D loss: 0.668848, acc.: 65.62%] [G loss: 0.764704]\n",
      "647 [D loss: 0.616196, acc.: 78.12%] [G loss: 0.712941]\n",
      "648 [D loss: 0.631477, acc.: 81.25%] [G loss: 0.703387]\n",
      "649 [D loss: 0.676808, acc.: 68.75%] [G loss: 0.701221]\n",
      "650 [D loss: 0.587923, acc.: 65.62%] [G loss: 0.728926]\n",
      "651 [D loss: 0.680641, acc.: 46.88%] [G loss: 0.757193]\n",
      "652 [D loss: 0.630921, acc.: 65.62%] [G loss: 0.738886]\n",
      "653 [D loss: 0.677210, acc.: 56.25%] [G loss: 0.762066]\n",
      "654 [D loss: 0.662676, acc.: 59.38%] [G loss: 0.728183]\n",
      "655 [D loss: 0.642197, acc.: 65.62%] [G loss: 0.723172]\n",
      "656 [D loss: 0.654478, acc.: 56.25%] [G loss: 0.719964]\n",
      "657 [D loss: 0.671707, acc.: 62.50%] [G loss: 0.709634]\n",
      "658 [D loss: 0.684894, acc.: 56.25%] [G loss: 0.725285]\n",
      "659 [D loss: 0.640402, acc.: 59.38%] [G loss: 0.739859]\n",
      "660 [D loss: 0.658881, acc.: 53.12%] [G loss: 0.732127]\n",
      "661 [D loss: 0.644720, acc.: 56.25%] [G loss: 0.712280]\n",
      "662 [D loss: 0.643618, acc.: 53.12%] [G loss: 0.709572]\n",
      "663 [D loss: 0.623567, acc.: 65.62%] [G loss: 0.678684]\n",
      "664 [D loss: 0.672663, acc.: 56.25%] [G loss: 0.692997]\n",
      "665 [D loss: 0.634707, acc.: 71.88%] [G loss: 0.711664]\n",
      "666 [D loss: 0.641300, acc.: 59.38%] [G loss: 0.746249]\n",
      "667 [D loss: 0.665093, acc.: 62.50%] [G loss: 0.750683]\n",
      "668 [D loss: 0.669323, acc.: 53.12%] [G loss: 0.750538]\n",
      "669 [D loss: 0.683925, acc.: 50.00%] [G loss: 0.758109]\n",
      "670 [D loss: 0.677001, acc.: 62.50%] [G loss: 0.722026]\n",
      "671 [D loss: 0.700338, acc.: 43.75%] [G loss: 0.727372]\n",
      "672 [D loss: 0.676620, acc.: 46.88%] [G loss: 0.754111]\n",
      "673 [D loss: 0.711538, acc.: 37.50%] [G loss: 0.717061]\n",
      "674 [D loss: 0.669840, acc.: 43.75%] [G loss: 0.719154]\n",
      "675 [D loss: 0.699437, acc.: 43.75%] [G loss: 0.671212]\n",
      "676 [D loss: 0.632649, acc.: 59.38%] [G loss: 0.682716]\n",
      "677 [D loss: 0.662752, acc.: 56.25%] [G loss: 0.682142]\n",
      "678 [D loss: 0.694164, acc.: 56.25%] [G loss: 0.746527]\n",
      "679 [D loss: 0.680098, acc.: 43.75%] [G loss: 0.761430]\n",
      "680 [D loss: 0.721568, acc.: 37.50%] [G loss: 0.747992]\n",
      "681 [D loss: 0.689484, acc.: 50.00%] [G loss: 0.742420]\n",
      "682 [D loss: 0.657499, acc.: 43.75%] [G loss: 0.716614]\n",
      "683 [D loss: 0.681130, acc.: 56.25%] [G loss: 0.703792]\n",
      "684 [D loss: 0.642407, acc.: 65.62%] [G loss: 0.721838]\n",
      "685 [D loss: 0.669064, acc.: 59.38%] [G loss: 0.700195]\n",
      "686 [D loss: 0.672908, acc.: 40.62%] [G loss: 0.669980]\n",
      "687 [D loss: 0.658467, acc.: 37.50%] [G loss: 0.695783]\n",
      "688 [D loss: 0.658510, acc.: 50.00%] [G loss: 0.716856]\n",
      "689 [D loss: 0.680342, acc.: 53.12%] [G loss: 0.732080]\n",
      "690 [D loss: 0.656503, acc.: 59.38%] [G loss: 0.734017]\n",
      "691 [D loss: 0.616800, acc.: 65.62%] [G loss: 0.728562]\n",
      "692 [D loss: 0.688978, acc.: 50.00%] [G loss: 0.699656]\n",
      "693 [D loss: 0.633144, acc.: 71.88%] [G loss: 0.706175]\n",
      "694 [D loss: 0.684978, acc.: 59.38%] [G loss: 0.692380]\n",
      "695 [D loss: 0.651710, acc.: 46.88%] [G loss: 0.718668]\n",
      "696 [D loss: 0.690216, acc.: 53.12%] [G loss: 0.708271]\n",
      "697 [D loss: 0.639602, acc.: 62.50%] [G loss: 0.703490]\n",
      "698 [D loss: 0.659844, acc.: 56.25%] [G loss: 0.719757]\n",
      "699 [D loss: 0.659163, acc.: 50.00%] [G loss: 0.745336]\n",
      "700 [D loss: 0.634809, acc.: 56.25%] [G loss: 0.756572]\n",
      "701 [D loss: 0.682952, acc.: 46.88%] [G loss: 0.739163]\n",
      "702 [D loss: 0.653579, acc.: 62.50%] [G loss: 0.729824]\n",
      "703 [D loss: 0.633095, acc.: 59.38%] [G loss: 0.766464]\n",
      "704 [D loss: 0.681941, acc.: 59.38%] [G loss: 0.718034]\n",
      "705 [D loss: 0.621884, acc.: 56.25%] [G loss: 0.706988]\n",
      "706 [D loss: 0.651624, acc.: 59.38%] [G loss: 0.698541]\n",
      "707 [D loss: 0.665544, acc.: 56.25%] [G loss: 0.692628]\n",
      "708 [D loss: 0.636492, acc.: 68.75%] [G loss: 0.712459]\n",
      "709 [D loss: 0.677575, acc.: 46.88%] [G loss: 0.722901]\n",
      "710 [D loss: 0.657158, acc.: 65.62%] [G loss: 0.740152]\n",
      "711 [D loss: 0.676013, acc.: 59.38%] [G loss: 0.736961]\n",
      "712 [D loss: 0.634683, acc.: 75.00%] [G loss: 0.736492]\n",
      "713 [D loss: 0.699482, acc.: 62.50%] [G loss: 0.723449]\n",
      "714 [D loss: 0.612055, acc.: 62.50%] [G loss: 0.751788]\n",
      "715 [D loss: 0.683605, acc.: 50.00%] [G loss: 0.747983]\n",
      "716 [D loss: 0.662857, acc.: 53.12%] [G loss: 0.771142]\n",
      "717 [D loss: 0.640363, acc.: 59.38%] [G loss: 0.769173]\n",
      "718 [D loss: 0.663447, acc.: 56.25%] [G loss: 0.750175]\n",
      "719 [D loss: 0.658707, acc.: 53.12%] [G loss: 0.749383]\n",
      "720 [D loss: 0.663046, acc.: 65.62%] [G loss: 0.733128]\n",
      "721 [D loss: 0.694785, acc.: 50.00%] [G loss: 0.734602]\n",
      "722 [D loss: 0.681738, acc.: 50.00%] [G loss: 0.708750]\n",
      "723 [D loss: 0.647942, acc.: 62.50%] [G loss: 0.727321]\n",
      "724 [D loss: 0.664245, acc.: 59.38%] [G loss: 0.739542]\n",
      "725 [D loss: 0.672024, acc.: 50.00%] [G loss: 0.751782]\n",
      "726 [D loss: 0.685174, acc.: 59.38%] [G loss: 0.740459]\n",
      "727 [D loss: 0.670797, acc.: 50.00%] [G loss: 0.736833]\n",
      "728 [D loss: 0.657135, acc.: 68.75%] [G loss: 0.726943]\n",
      "729 [D loss: 0.669926, acc.: 56.25%] [G loss: 0.727400]\n",
      "730 [D loss: 0.665316, acc.: 62.50%] [G loss: 0.694316]\n",
      "731 [D loss: 0.655640, acc.: 56.25%] [G loss: 0.698243]\n",
      "732 [D loss: 0.692361, acc.: 50.00%] [G loss: 0.737027]\n",
      "733 [D loss: 0.638649, acc.: 56.25%] [G loss: 0.753093]\n",
      "734 [D loss: 0.667662, acc.: 56.25%] [G loss: 0.795755]\n",
      "735 [D loss: 0.648863, acc.: 62.50%] [G loss: 0.783254]\n",
      "736 [D loss: 0.705585, acc.: 40.62%] [G loss: 0.728272]\n",
      "737 [D loss: 0.673330, acc.: 56.25%] [G loss: 0.723234]\n",
      "738 [D loss: 0.665104, acc.: 68.75%] [G loss: 0.712640]\n",
      "739 [D loss: 0.678061, acc.: 53.12%] [G loss: 0.748833]\n",
      "740 [D loss: 0.634920, acc.: 62.50%] [G loss: 0.769446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 [D loss: 0.675362, acc.: 50.00%] [G loss: 0.689016]\n",
      "742 [D loss: 0.675856, acc.: 53.12%] [G loss: 0.682107]\n",
      "743 [D loss: 0.700160, acc.: 46.88%] [G loss: 0.727668]\n",
      "744 [D loss: 0.624783, acc.: 65.62%] [G loss: 0.765933]\n",
      "745 [D loss: 0.669664, acc.: 50.00%] [G loss: 0.759584]\n",
      "746 [D loss: 0.695063, acc.: 43.75%] [G loss: 0.723350]\n",
      "747 [D loss: 0.677984, acc.: 46.88%] [G loss: 0.712702]\n",
      "748 [D loss: 0.666948, acc.: 59.38%] [G loss: 0.736302]\n",
      "749 [D loss: 0.665341, acc.: 53.12%] [G loss: 0.713291]\n",
      "750 [D loss: 0.649577, acc.: 59.38%] [G loss: 0.721857]\n",
      "751 [D loss: 0.666139, acc.: 53.12%] [G loss: 0.743199]\n",
      "752 [D loss: 0.628134, acc.: 62.50%] [G loss: 0.770421]\n",
      "753 [D loss: 0.700577, acc.: 53.12%] [G loss: 0.722671]\n",
      "754 [D loss: 0.692345, acc.: 40.62%] [G loss: 0.700320]\n",
      "755 [D loss: 0.667044, acc.: 59.38%] [G loss: 0.730281]\n",
      "756 [D loss: 0.671875, acc.: 53.12%] [G loss: 0.735020]\n",
      "757 [D loss: 0.668140, acc.: 59.38%] [G loss: 0.711637]\n",
      "758 [D loss: 0.663997, acc.: 62.50%] [G loss: 0.729016]\n",
      "759 [D loss: 0.674442, acc.: 65.62%] [G loss: 0.719642]\n",
      "760 [D loss: 0.674521, acc.: 43.75%] [G loss: 0.701526]\n",
      "761 [D loss: 0.623631, acc.: 78.12%] [G loss: 0.719662]\n",
      "762 [D loss: 0.652480, acc.: 62.50%] [G loss: 0.723401]\n",
      "763 [D loss: 0.662701, acc.: 62.50%] [G loss: 0.772305]\n",
      "764 [D loss: 0.686264, acc.: 46.88%] [G loss: 0.751126]\n",
      "765 [D loss: 0.639856, acc.: 65.62%] [G loss: 0.761049]\n",
      "766 [D loss: 0.650022, acc.: 65.62%] [G loss: 0.754437]\n",
      "767 [D loss: 0.645628, acc.: 62.50%] [G loss: 0.761927]\n",
      "768 [D loss: 0.668476, acc.: 53.12%] [G loss: 0.747929]\n",
      "769 [D loss: 0.636475, acc.: 65.62%] [G loss: 0.758862]\n",
      "770 [D loss: 0.613563, acc.: 62.50%] [G loss: 0.747806]\n",
      "771 [D loss: 0.662253, acc.: 59.38%] [G loss: 0.761640]\n",
      "772 [D loss: 0.612360, acc.: 68.75%] [G loss: 0.762139]\n",
      "773 [D loss: 0.614975, acc.: 81.25%] [G loss: 0.762212]\n",
      "774 [D loss: 0.628536, acc.: 56.25%] [G loss: 0.741020]\n",
      "775 [D loss: 0.627449, acc.: 53.12%] [G loss: 0.732333]\n",
      "776 [D loss: 0.663957, acc.: 53.12%] [G loss: 0.761497]\n",
      "777 [D loss: 0.653135, acc.: 65.62%] [G loss: 0.777361]\n",
      "778 [D loss: 0.620749, acc.: 68.75%] [G loss: 0.757121]\n",
      "779 [D loss: 0.633676, acc.: 68.75%] [G loss: 0.730515]\n",
      "780 [D loss: 0.613919, acc.: 71.88%] [G loss: 0.716971]\n",
      "781 [D loss: 0.639077, acc.: 62.50%] [G loss: 0.751705]\n",
      "782 [D loss: 0.665219, acc.: 65.62%] [G loss: 0.746751]\n",
      "783 [D loss: 0.660397, acc.: 56.25%] [G loss: 0.774500]\n",
      "784 [D loss: 0.632509, acc.: 68.75%] [G loss: 0.755803]\n",
      "785 [D loss: 0.632630, acc.: 71.88%] [G loss: 0.742569]\n",
      "786 [D loss: 0.655910, acc.: 62.50%] [G loss: 0.730509]\n",
      "787 [D loss: 0.627751, acc.: 71.88%] [G loss: 0.756525]\n",
      "788 [D loss: 0.676680, acc.: 43.75%] [G loss: 0.781257]\n",
      "789 [D loss: 0.689056, acc.: 53.12%] [G loss: 0.796522]\n",
      "790 [D loss: 0.607342, acc.: 75.00%] [G loss: 0.814841]\n",
      "791 [D loss: 0.657027, acc.: 62.50%] [G loss: 0.810610]\n",
      "792 [D loss: 0.648008, acc.: 65.62%] [G loss: 0.783419]\n",
      "793 [D loss: 0.623046, acc.: 78.12%] [G loss: 0.778523]\n",
      "794 [D loss: 0.657902, acc.: 59.38%] [G loss: 0.750666]\n",
      "795 [D loss: 0.630778, acc.: 62.50%] [G loss: 0.754898]\n",
      "796 [D loss: 0.631768, acc.: 75.00%] [G loss: 0.742709]\n",
      "797 [D loss: 0.655890, acc.: 65.62%] [G loss: 0.725759]\n",
      "798 [D loss: 0.638304, acc.: 62.50%] [G loss: 0.727980]\n",
      "799 [D loss: 0.597775, acc.: 75.00%] [G loss: 0.733327]\n",
      "800 [D loss: 0.695221, acc.: 53.12%] [G loss: 0.735922]\n",
      "801 [D loss: 0.671832, acc.: 46.88%] [G loss: 0.740293]\n",
      "802 [D loss: 0.616926, acc.: 59.38%] [G loss: 0.748415]\n",
      "803 [D loss: 0.631789, acc.: 62.50%] [G loss: 0.772581]\n",
      "804 [D loss: 0.613529, acc.: 78.12%] [G loss: 0.757153]\n",
      "805 [D loss: 0.648591, acc.: 53.12%] [G loss: 0.738682]\n",
      "806 [D loss: 0.598845, acc.: 59.38%] [G loss: 0.724853]\n",
      "807 [D loss: 0.638716, acc.: 62.50%] [G loss: 0.717663]\n",
      "808 [D loss: 0.639611, acc.: 46.88%] [G loss: 0.750187]\n",
      "809 [D loss: 0.623339, acc.: 62.50%] [G loss: 0.770927]\n",
      "810 [D loss: 0.656815, acc.: 62.50%] [G loss: 0.768727]\n",
      "811 [D loss: 0.592996, acc.: 84.38%] [G loss: 0.766628]\n",
      "812 [D loss: 0.611622, acc.: 62.50%] [G loss: 0.730274]\n",
      "813 [D loss: 0.620075, acc.: 65.62%] [G loss: 0.726798]\n",
      "814 [D loss: 0.602155, acc.: 71.88%] [G loss: 0.729463]\n",
      "815 [D loss: 0.613364, acc.: 59.38%] [G loss: 0.791191]\n",
      "816 [D loss: 0.623499, acc.: 68.75%] [G loss: 0.800623]\n",
      "817 [D loss: 0.631674, acc.: 65.62%] [G loss: 0.799497]\n",
      "818 [D loss: 0.666348, acc.: 53.12%] [G loss: 0.794163]\n",
      "819 [D loss: 0.629447, acc.: 65.62%] [G loss: 0.766032]\n",
      "820 [D loss: 0.658060, acc.: 65.62%] [G loss: 0.743468]\n",
      "821 [D loss: 0.704803, acc.: 50.00%] [G loss: 0.759450]\n",
      "822 [D loss: 0.610849, acc.: 75.00%] [G loss: 0.774540]\n",
      "823 [D loss: 0.647270, acc.: 56.25%] [G loss: 0.801486]\n",
      "824 [D loss: 0.657558, acc.: 59.38%] [G loss: 0.800186]\n",
      "825 [D loss: 0.609517, acc.: 78.12%] [G loss: 0.761717]\n",
      "826 [D loss: 0.650309, acc.: 46.88%] [G loss: 0.727217]\n",
      "827 [D loss: 0.623376, acc.: 62.50%] [G loss: 0.747934]\n",
      "828 [D loss: 0.683013, acc.: 46.88%] [G loss: 0.738213]\n",
      "829 [D loss: 0.599155, acc.: 62.50%] [G loss: 0.773085]\n",
      "830 [D loss: 0.603674, acc.: 59.38%] [G loss: 0.769134]\n",
      "831 [D loss: 0.652641, acc.: 59.38%] [G loss: 0.759841]\n",
      "832 [D loss: 0.624667, acc.: 68.75%] [G loss: 0.752746]\n",
      "833 [D loss: 0.688223, acc.: 53.12%] [G loss: 0.768549]\n",
      "834 [D loss: 0.621437, acc.: 56.25%] [G loss: 0.790320]\n",
      "835 [D loss: 0.641344, acc.: 62.50%] [G loss: 0.816441]\n",
      "836 [D loss: 0.648724, acc.: 56.25%] [G loss: 0.803791]\n",
      "837 [D loss: 0.614757, acc.: 90.62%] [G loss: 0.770959]\n",
      "838 [D loss: 0.650331, acc.: 56.25%] [G loss: 0.725869]\n",
      "839 [D loss: 0.657423, acc.: 62.50%] [G loss: 0.765799]\n",
      "840 [D loss: 0.622020, acc.: 68.75%] [G loss: 0.771699]\n",
      "841 [D loss: 0.653397, acc.: 65.62%] [G loss: 0.763443]\n",
      "842 [D loss: 0.627921, acc.: 59.38%] [G loss: 0.801259]\n",
      "843 [D loss: 0.637140, acc.: 68.75%] [G loss: 0.761898]\n",
      "844 [D loss: 0.638960, acc.: 56.25%] [G loss: 0.786839]\n",
      "845 [D loss: 0.640478, acc.: 53.12%] [G loss: 0.842047]\n",
      "846 [D loss: 0.599033, acc.: 71.88%] [G loss: 0.870132]\n",
      "847 [D loss: 0.675629, acc.: 53.12%] [G loss: 0.822865]\n",
      "848 [D loss: 0.695946, acc.: 40.62%] [G loss: 0.836320]\n",
      "849 [D loss: 0.651442, acc.: 62.50%] [G loss: 0.794955]\n",
      "850 [D loss: 0.667986, acc.: 56.25%] [G loss: 0.787357]\n",
      "851 [D loss: 0.634189, acc.: 62.50%] [G loss: 0.779751]\n",
      "852 [D loss: 0.643708, acc.: 68.75%] [G loss: 0.771175]\n",
      "853 [D loss: 0.640643, acc.: 62.50%] [G loss: 0.767027]\n",
      "854 [D loss: 0.633853, acc.: 71.88%] [G loss: 0.779692]\n",
      "855 [D loss: 0.610267, acc.: 62.50%] [G loss: 0.798404]\n",
      "856 [D loss: 0.617991, acc.: 65.62%] [G loss: 0.837649]\n",
      "857 [D loss: 0.655961, acc.: 53.12%] [G loss: 0.820811]\n",
      "858 [D loss: 0.639288, acc.: 65.62%] [G loss: 0.783422]\n",
      "859 [D loss: 0.667885, acc.: 53.12%] [G loss: 0.750772]\n",
      "860 [D loss: 0.707315, acc.: 43.75%] [G loss: 0.775896]\n",
      "861 [D loss: 0.634221, acc.: 68.75%] [G loss: 0.752407]\n",
      "862 [D loss: 0.615999, acc.: 65.62%] [G loss: 0.766298]\n",
      "863 [D loss: 0.616737, acc.: 68.75%] [G loss: 0.764409]\n",
      "864 [D loss: 0.640298, acc.: 62.50%] [G loss: 0.752257]\n",
      "865 [D loss: 0.619891, acc.: 65.62%] [G loss: 0.754534]\n",
      "866 [D loss: 0.568260, acc.: 71.88%] [G loss: 0.772191]\n",
      "867 [D loss: 0.661276, acc.: 53.12%] [G loss: 0.773033]\n",
      "868 [D loss: 0.673858, acc.: 53.12%] [G loss: 0.746515]\n",
      "869 [D loss: 0.671819, acc.: 53.12%] [G loss: 0.737102]\n",
      "870 [D loss: 0.608594, acc.: 68.75%] [G loss: 0.761229]\n",
      "871 [D loss: 0.599220, acc.: 75.00%] [G loss: 0.786882]\n",
      "872 [D loss: 0.619160, acc.: 65.62%] [G loss: 0.774620]\n",
      "873 [D loss: 0.613660, acc.: 65.62%] [G loss: 0.787318]\n",
      "874 [D loss: 0.602612, acc.: 78.12%] [G loss: 0.801020]\n",
      "875 [D loss: 0.654583, acc.: 62.50%] [G loss: 0.758560]\n",
      "876 [D loss: 0.638606, acc.: 62.50%] [G loss: 0.818336]\n",
      "877 [D loss: 0.598430, acc.: 68.75%] [G loss: 0.797359]\n",
      "878 [D loss: 0.718536, acc.: 40.62%] [G loss: 0.767918]\n",
      "879 [D loss: 0.616362, acc.: 62.50%] [G loss: 0.773576]\n",
      "880 [D loss: 0.645693, acc.: 50.00%] [G loss: 0.793873]\n",
      "881 [D loss: 0.640331, acc.: 62.50%] [G loss: 0.801946]\n",
      "882 [D loss: 0.612258, acc.: 78.12%] [G loss: 0.873307]\n",
      "883 [D loss: 0.663619, acc.: 56.25%] [G loss: 0.852915]\n",
      "884 [D loss: 0.657419, acc.: 62.50%] [G loss: 0.770500]\n",
      "885 [D loss: 0.662898, acc.: 59.38%] [G loss: 0.741285]\n",
      "886 [D loss: 0.610084, acc.: 71.88%] [G loss: 0.745377]\n",
      "887 [D loss: 0.670308, acc.: 56.25%] [G loss: 0.728155]\n",
      "888 [D loss: 0.660386, acc.: 50.00%] [G loss: 0.737832]\n",
      "889 [D loss: 0.625240, acc.: 71.88%] [G loss: 0.757376]\n",
      "890 [D loss: 0.605996, acc.: 78.12%] [G loss: 0.759038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891 [D loss: 0.664319, acc.: 59.38%] [G loss: 0.779497]\n",
      "892 [D loss: 0.625675, acc.: 62.50%] [G loss: 0.756995]\n",
      "893 [D loss: 0.589943, acc.: 75.00%] [G loss: 0.738217]\n",
      "894 [D loss: 0.652587, acc.: 59.38%] [G loss: 0.752003]\n",
      "895 [D loss: 0.634320, acc.: 71.88%] [G loss: 0.770870]\n",
      "896 [D loss: 0.602519, acc.: 68.75%] [G loss: 0.791279]\n",
      "897 [D loss: 0.642360, acc.: 56.25%] [G loss: 0.756295]\n",
      "898 [D loss: 0.632413, acc.: 62.50%] [G loss: 0.763950]\n",
      "899 [D loss: 0.608939, acc.: 71.88%] [G loss: 0.721839]\n",
      "900 [D loss: 0.657096, acc.: 56.25%] [G loss: 0.734435]\n",
      "901 [D loss: 0.649951, acc.: 68.75%] [G loss: 0.762222]\n",
      "902 [D loss: 0.598403, acc.: 81.25%] [G loss: 0.765035]\n",
      "903 [D loss: 0.675492, acc.: 50.00%] [G loss: 0.788597]\n",
      "904 [D loss: 0.602254, acc.: 68.75%] [G loss: 0.849429]\n",
      "905 [D loss: 0.644332, acc.: 56.25%] [G loss: 0.773429]\n",
      "906 [D loss: 0.607097, acc.: 71.88%] [G loss: 0.789918]\n",
      "907 [D loss: 0.631685, acc.: 56.25%] [G loss: 0.815407]\n",
      "908 [D loss: 0.645749, acc.: 62.50%] [G loss: 0.829738]\n",
      "909 [D loss: 0.606181, acc.: 78.12%] [G loss: 0.809335]\n",
      "910 [D loss: 0.585638, acc.: 78.12%] [G loss: 0.773676]\n",
      "911 [D loss: 0.650722, acc.: 68.75%] [G loss: 0.806070]\n",
      "912 [D loss: 0.664200, acc.: 68.75%] [G loss: 0.806780]\n",
      "913 [D loss: 0.587205, acc.: 78.12%] [G loss: 0.780609]\n",
      "914 [D loss: 0.655989, acc.: 59.38%] [G loss: 0.756953]\n",
      "915 [D loss: 0.617311, acc.: 65.62%] [G loss: 0.781580]\n",
      "916 [D loss: 0.678994, acc.: 56.25%] [G loss: 0.789351]\n",
      "917 [D loss: 0.647576, acc.: 59.38%] [G loss: 0.887482]\n",
      "918 [D loss: 0.600286, acc.: 71.88%] [G loss: 0.936902]\n",
      "919 [D loss: 0.582788, acc.: 71.88%] [G loss: 0.837514]\n",
      "920 [D loss: 0.611125, acc.: 71.88%] [G loss: 0.780305]\n",
      "921 [D loss: 0.606435, acc.: 71.88%] [G loss: 0.813200]\n",
      "922 [D loss: 0.599668, acc.: 71.88%] [G loss: 0.746764]\n",
      "923 [D loss: 0.664969, acc.: 53.12%] [G loss: 0.774101]\n",
      "924 [D loss: 0.583950, acc.: 71.88%] [G loss: 0.767567]\n",
      "925 [D loss: 0.629870, acc.: 65.62%] [G loss: 0.758966]\n",
      "926 [D loss: 0.639464, acc.: 62.50%] [G loss: 0.774702]\n",
      "927 [D loss: 0.603790, acc.: 62.50%] [G loss: 0.782390]\n",
      "928 [D loss: 0.660735, acc.: 59.38%] [G loss: 0.798668]\n",
      "929 [D loss: 0.610779, acc.: 71.88%] [G loss: 0.800950]\n",
      "930 [D loss: 0.595723, acc.: 75.00%] [G loss: 0.781677]\n",
      "931 [D loss: 0.611083, acc.: 59.38%] [G loss: 0.819359]\n",
      "932 [D loss: 0.624360, acc.: 65.62%] [G loss: 0.791295]\n",
      "933 [D loss: 0.624266, acc.: 68.75%] [G loss: 0.820828]\n",
      "934 [D loss: 0.644570, acc.: 56.25%] [G loss: 0.830591]\n",
      "935 [D loss: 0.602887, acc.: 71.88%] [G loss: 0.808221]\n",
      "936 [D loss: 0.621477, acc.: 71.88%] [G loss: 0.786017]\n",
      "937 [D loss: 0.624222, acc.: 68.75%] [G loss: 0.803549]\n",
      "938 [D loss: 0.647440, acc.: 59.38%] [G loss: 0.799044]\n",
      "939 [D loss: 0.609506, acc.: 62.50%] [G loss: 0.798430]\n",
      "940 [D loss: 0.641941, acc.: 62.50%] [G loss: 0.809995]\n",
      "941 [D loss: 0.614390, acc.: 71.88%] [G loss: 0.786134]\n",
      "942 [D loss: 0.596738, acc.: 84.38%] [G loss: 0.806262]\n",
      "943 [D loss: 0.647835, acc.: 62.50%] [G loss: 0.806984]\n",
      "944 [D loss: 0.577701, acc.: 81.25%] [G loss: 0.831402]\n",
      "945 [D loss: 0.643732, acc.: 78.12%] [G loss: 0.796910]\n",
      "946 [D loss: 0.652558, acc.: 62.50%] [G loss: 0.749646]\n",
      "947 [D loss: 0.616628, acc.: 59.38%] [G loss: 0.757033]\n",
      "948 [D loss: 0.673618, acc.: 46.88%] [G loss: 0.751827]\n",
      "949 [D loss: 0.657819, acc.: 50.00%] [G loss: 0.738196]\n",
      "950 [D loss: 0.652998, acc.: 56.25%] [G loss: 0.739379]\n",
      "951 [D loss: 0.618681, acc.: 53.12%] [G loss: 0.769783]\n",
      "952 [D loss: 0.607024, acc.: 65.62%] [G loss: 0.826095]\n",
      "953 [D loss: 0.707046, acc.: 56.25%] [G loss: 0.818036]\n",
      "954 [D loss: 0.584367, acc.: 84.38%] [G loss: 0.845330]\n",
      "955 [D loss: 0.637058, acc.: 59.38%] [G loss: 0.804187]\n",
      "956 [D loss: 0.587106, acc.: 71.88%] [G loss: 0.816294]\n",
      "957 [D loss: 0.625057, acc.: 65.62%] [G loss: 0.856283]\n",
      "958 [D loss: 0.609543, acc.: 62.50%] [G loss: 0.867349]\n",
      "959 [D loss: 0.672702, acc.: 62.50%] [G loss: 0.817625]\n",
      "960 [D loss: 0.655526, acc.: 59.38%] [G loss: 0.845436]\n",
      "961 [D loss: 0.663742, acc.: 50.00%] [G loss: 0.777596]\n",
      "962 [D loss: 0.590832, acc.: 71.88%] [G loss: 0.798068]\n",
      "963 [D loss: 0.652023, acc.: 59.38%] [G loss: 0.770542]\n",
      "964 [D loss: 0.663244, acc.: 59.38%] [G loss: 0.780836]\n",
      "965 [D loss: 0.621446, acc.: 65.62%] [G loss: 0.794991]\n",
      "966 [D loss: 0.649200, acc.: 62.50%] [G loss: 0.818339]\n",
      "967 [D loss: 0.651431, acc.: 65.62%] [G loss: 0.812625]\n",
      "968 [D loss: 0.627541, acc.: 59.38%] [G loss: 0.842431]\n",
      "969 [D loss: 0.618477, acc.: 71.88%] [G loss: 0.813878]\n",
      "970 [D loss: 0.635495, acc.: 62.50%] [G loss: 0.785519]\n",
      "971 [D loss: 0.626516, acc.: 68.75%] [G loss: 0.813200]\n",
      "972 [D loss: 0.649792, acc.: 68.75%] [G loss: 0.806614]\n",
      "973 [D loss: 0.624660, acc.: 71.88%] [G loss: 0.804455]\n",
      "974 [D loss: 0.659046, acc.: 65.62%] [G loss: 0.766538]\n",
      "975 [D loss: 0.587158, acc.: 81.25%] [G loss: 0.743379]\n",
      "976 [D loss: 0.620946, acc.: 62.50%] [G loss: 0.793632]\n",
      "977 [D loss: 0.632162, acc.: 68.75%] [G loss: 0.798196]\n",
      "978 [D loss: 0.584581, acc.: 87.50%] [G loss: 0.762663]\n",
      "979 [D loss: 0.588777, acc.: 71.88%] [G loss: 0.797060]\n",
      "980 [D loss: 0.649996, acc.: 68.75%] [G loss: 0.790985]\n",
      "981 [D loss: 0.656613, acc.: 56.25%] [G loss: 0.763995]\n",
      "982 [D loss: 0.626540, acc.: 65.62%] [G loss: 0.767182]\n",
      "983 [D loss: 0.594480, acc.: 78.12%] [G loss: 0.765954]\n",
      "984 [D loss: 0.608653, acc.: 75.00%] [G loss: 0.768501]\n",
      "985 [D loss: 0.661237, acc.: 59.38%] [G loss: 0.821393]\n",
      "986 [D loss: 0.579353, acc.: 78.12%] [G loss: 0.782424]\n",
      "987 [D loss: 0.614839, acc.: 68.75%] [G loss: 0.796159]\n",
      "988 [D loss: 0.615260, acc.: 68.75%] [G loss: 0.799518]\n",
      "989 [D loss: 0.562370, acc.: 78.12%] [G loss: 0.814669]\n",
      "990 [D loss: 0.627110, acc.: 62.50%] [G loss: 0.800560]\n",
      "991 [D loss: 0.643792, acc.: 65.62%] [G loss: 0.781009]\n",
      "992 [D loss: 0.654974, acc.: 59.38%] [G loss: 0.768265]\n",
      "993 [D loss: 0.639102, acc.: 59.38%] [G loss: 0.788534]\n",
      "994 [D loss: 0.625217, acc.: 68.75%] [G loss: 0.798718]\n",
      "995 [D loss: 0.644717, acc.: 62.50%] [G loss: 0.776872]\n",
      "996 [D loss: 0.614469, acc.: 59.38%] [G loss: 0.759262]\n",
      "997 [D loss: 0.641743, acc.: 62.50%] [G loss: 0.733036]\n",
      "998 [D loss: 0.680547, acc.: 62.50%] [G loss: 0.754156]\n",
      "999 [D loss: 0.596668, acc.: 84.38%] [G loss: 0.758237]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        #mnist\n",
    "        self.img_rows = 28 \n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        #  \n",
    "        self.z_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Generator\n",
    "        self.generator = self.build_generator()\n",
    "        # generator\n",
    "        #self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        self.combined = self.build_combined1()\n",
    "        #self.combined = self.build_combined2()\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (self.z_dim,)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_combined1(self):\n",
    "        self.discriminator.trainable = False\n",
    "        model = Sequential([self.generator, self.discriminator])\n",
    "        return model\n",
    "\n",
    "    def build_combined2(self):\n",
    "        z = Input(shape=(self.z_dim,))\n",
    "        img = self.generator(z)\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator(img)\n",
    "        model = Model(z, valid)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # mnist\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Generator\n",
    "            noise = np.random.normal(0, 1, (half_batch, self.z_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "\n",
    "            # \n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # discriminator\n",
    "            # \n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            # \n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "\n",
    "            # 1 \n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # \n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # \n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        # \n",
    "        r, c = 5, 5\n",
    "\n",
    "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # 0-1\n",
    "        gen_imgs = 0.5 * gen_imgs +  0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/gan/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=1000, batch_size=32, save_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
