{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(28, 28, 1..., strides=(2, 2), padding=\"same\")`\n",
      "W0902 17:57:37.607595 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0902 17:57:37.609777 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:72: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), strides=(2, 2))`\n",
      "W0902 17:57:37.664798 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0902 17:57:37.672465 4692112832 deprecation.py:506] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0902 17:57:37.712115 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0902 17:57:37.717858 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0902 17:57:37.722594 4692112832 deprecation.py:323] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0902 17:57:37.953158 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "W0902 17:57:38.163233 4692112832 deprecation_wrapper.py:119] From /Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6272)              6428800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 6,768,129\n",
      "Trainable params: 6,753,409\n",
      "Non-trainable params: 14,720\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.670552, acc.: 46.88%] [G loss: 0.537502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sungwoo/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.926850, acc.: 50.00%] [G loss: 0.502425]\n",
      "2 [D loss: 0.712755, acc.: 50.00%] [G loss: 0.598763]\n",
      "3 [D loss: 0.622661, acc.: 53.12%] [G loss: 0.675114]\n",
      "4 [D loss: 0.662687, acc.: 50.00%] [G loss: 0.597554]\n",
      "5 [D loss: 0.577654, acc.: 59.38%] [G loss: 0.454302]\n",
      "6 [D loss: 0.317154, acc.: 100.00%] [G loss: 0.370394]\n",
      "7 [D loss: 0.176194, acc.: 100.00%] [G loss: 0.202387]\n",
      "8 [D loss: 0.260024, acc.: 87.50%] [G loss: 0.104367]\n",
      "9 [D loss: 1.389304, acc.: 50.00%] [G loss: 0.236735]\n",
      "10 [D loss: 0.704517, acc.: 50.00%] [G loss: 0.616106]\n",
      "11 [D loss: 0.719627, acc.: 34.38%] [G loss: 0.649072]\n",
      "12 [D loss: 0.510647, acc.: 87.50%] [G loss: 0.520975]\n",
      "13 [D loss: 0.267875, acc.: 100.00%] [G loss: 0.408986]\n",
      "14 [D loss: 0.326711, acc.: 100.00%] [G loss: 0.296245]\n",
      "15 [D loss: 0.812876, acc.: 46.88%] [G loss: 0.331643]\n",
      "16 [D loss: 0.711654, acc.: 50.00%] [G loss: 0.396956]\n",
      "17 [D loss: 0.690297, acc.: 46.88%] [G loss: 0.456904]\n",
      "18 [D loss: 0.492561, acc.: 87.50%] [G loss: 0.447109]\n",
      "19 [D loss: 0.678568, acc.: 50.00%] [G loss: 0.472631]\n",
      "20 [D loss: 0.685668, acc.: 53.12%] [G loss: 0.454076]\n",
      "21 [D loss: 0.630009, acc.: 59.38%] [G loss: 0.448999]\n",
      "22 [D loss: 0.567348, acc.: 81.25%] [G loss: 0.401811]\n",
      "23 [D loss: 0.731990, acc.: 40.62%] [G loss: 0.499993]\n",
      "24 [D loss: 0.593653, acc.: 84.38%] [G loss: 0.513120]\n",
      "25 [D loss: 0.494515, acc.: 100.00%] [G loss: 0.513057]\n",
      "26 [D loss: 0.808138, acc.: 46.88%] [G loss: 0.364862]\n",
      "27 [D loss: 0.377179, acc.: 93.75%] [G loss: 0.536234]\n",
      "28 [D loss: 0.640543, acc.: 68.75%] [G loss: 0.397877]\n",
      "29 [D loss: 0.478779, acc.: 90.62%] [G loss: 0.430849]\n",
      "30 [D loss: 0.662534, acc.: 56.25%] [G loss: 0.383482]\n",
      "31 [D loss: 0.591506, acc.: 78.12%] [G loss: 0.579509]\n",
      "32 [D loss: 0.532566, acc.: 81.25%] [G loss: 0.413774]\n",
      "33 [D loss: 0.468238, acc.: 90.62%] [G loss: 0.431239]\n",
      "34 [D loss: 0.675741, acc.: 56.25%] [G loss: 0.367190]\n",
      "35 [D loss: 0.693654, acc.: 56.25%] [G loss: 0.500349]\n",
      "36 [D loss: 0.791158, acc.: 34.38%] [G loss: 0.472416]\n",
      "37 [D loss: 0.400039, acc.: 96.88%] [G loss: 0.466824]\n",
      "38 [D loss: 0.375745, acc.: 93.75%] [G loss: 0.440509]\n",
      "39 [D loss: 0.428122, acc.: 78.12%] [G loss: 0.289106]\n",
      "40 [D loss: 1.210722, acc.: 37.50%] [G loss: 0.379258]\n",
      "41 [D loss: 0.914286, acc.: 34.38%] [G loss: 0.499820]\n",
      "42 [D loss: 0.559900, acc.: 87.50%] [G loss: 0.479412]\n",
      "43 [D loss: 0.504368, acc.: 84.38%] [G loss: 0.466745]\n",
      "44 [D loss: 0.586255, acc.: 81.25%] [G loss: 0.358978]\n",
      "45 [D loss: 0.768396, acc.: 37.50%] [G loss: 0.376815]\n",
      "46 [D loss: 0.898649, acc.: 40.62%] [G loss: 0.440244]\n",
      "47 [D loss: 0.819575, acc.: 31.25%] [G loss: 0.517545]\n",
      "48 [D loss: 0.715062, acc.: 50.00%] [G loss: 0.501141]\n",
      "49 [D loss: 0.537371, acc.: 90.62%] [G loss: 0.481457]\n",
      "50 [D loss: 0.527628, acc.: 90.62%] [G loss: 0.461772]\n",
      "51 [D loss: 0.702787, acc.: 46.88%] [G loss: 0.447311]\n",
      "52 [D loss: 0.859108, acc.: 34.38%] [G loss: 0.435177]\n",
      "53 [D loss: 0.843641, acc.: 18.75%] [G loss: 0.518520]\n",
      "54 [D loss: 0.710165, acc.: 40.62%] [G loss: 0.529717]\n",
      "55 [D loss: 0.606321, acc.: 75.00%] [G loss: 0.524988]\n",
      "56 [D loss: 0.643541, acc.: 62.50%] [G loss: 0.510132]\n",
      "57 [D loss: 0.670890, acc.: 53.12%] [G loss: 0.503642]\n",
      "58 [D loss: 0.755231, acc.: 31.25%] [G loss: 0.510979]\n",
      "59 [D loss: 0.842047, acc.: 21.88%] [G loss: 0.550503]\n",
      "60 [D loss: 0.757676, acc.: 40.62%] [G loss: 0.595419]\n",
      "61 [D loss: 0.639743, acc.: 65.62%] [G loss: 0.549803]\n",
      "62 [D loss: 0.619123, acc.: 65.62%] [G loss: 0.525195]\n",
      "63 [D loss: 0.587369, acc.: 84.38%] [G loss: 0.518701]\n",
      "64 [D loss: 0.641724, acc.: 65.62%] [G loss: 0.470937]\n",
      "65 [D loss: 0.763593, acc.: 43.75%] [G loss: 0.462686]\n",
      "66 [D loss: 0.730488, acc.: 34.38%] [G loss: 0.489983]\n",
      "67 [D loss: 0.733346, acc.: 43.75%] [G loss: 0.506492]\n",
      "68 [D loss: 0.686748, acc.: 46.88%] [G loss: 0.550076]\n",
      "69 [D loss: 0.675719, acc.: 56.25%] [G loss: 0.537721]\n",
      "70 [D loss: 0.691954, acc.: 37.50%] [G loss: 0.551982]\n",
      "71 [D loss: 0.677499, acc.: 46.88%] [G loss: 0.520309]\n",
      "72 [D loss: 0.646454, acc.: 65.62%] [G loss: 0.516051]\n",
      "73 [D loss: 0.728007, acc.: 31.25%] [G loss: 0.488190]\n",
      "74 [D loss: 0.732949, acc.: 28.12%] [G loss: 0.498063]\n",
      "75 [D loss: 0.694377, acc.: 43.75%] [G loss: 0.520537]\n",
      "76 [D loss: 0.711772, acc.: 53.12%] [G loss: 0.550565]\n",
      "77 [D loss: 0.671450, acc.: 53.12%] [G loss: 0.569518]\n",
      "78 [D loss: 0.699929, acc.: 46.88%] [G loss: 0.582750]\n",
      "79 [D loss: 0.687359, acc.: 46.88%] [G loss: 0.586740]\n",
      "80 [D loss: 0.668619, acc.: 50.00%] [G loss: 0.597552]\n",
      "81 [D loss: 0.687240, acc.: 50.00%] [G loss: 0.585400]\n",
      "82 [D loss: 0.671620, acc.: 65.62%] [G loss: 0.571001]\n",
      "83 [D loss: 0.720904, acc.: 43.75%] [G loss: 0.566661]\n",
      "84 [D loss: 0.729962, acc.: 31.25%] [G loss: 0.590646]\n",
      "85 [D loss: 0.705909, acc.: 53.12%] [G loss: 0.634989]\n",
      "86 [D loss: 0.645064, acc.: 68.75%] [G loss: 0.696099]\n",
      "87 [D loss: 0.676831, acc.: 56.25%] [G loss: 0.671970]\n",
      "88 [D loss: 0.694258, acc.: 56.25%] [G loss: 0.706298]\n",
      "89 [D loss: 0.644450, acc.: 65.62%] [G loss: 0.654976]\n",
      "90 [D loss: 0.600199, acc.: 81.25%] [G loss: 0.592653]\n",
      "91 [D loss: 0.611942, acc.: 87.50%] [G loss: 0.536983]\n",
      "92 [D loss: 0.637708, acc.: 65.62%] [G loss: 0.504658]\n",
      "93 [D loss: 0.741417, acc.: 46.88%] [G loss: 0.492263]\n",
      "94 [D loss: 0.696943, acc.: 43.75%] [G loss: 0.555547]\n",
      "95 [D loss: 0.615400, acc.: 68.75%] [G loss: 0.781340]\n",
      "96 [D loss: 0.645593, acc.: 62.50%] [G loss: 0.681646]\n",
      "97 [D loss: 0.627963, acc.: 65.62%] [G loss: 0.585180]\n",
      "98 [D loss: 0.609255, acc.: 62.50%] [G loss: 0.509877]\n",
      "99 [D loss: 0.502053, acc.: 84.38%] [G loss: 0.491213]\n",
      "100 [D loss: 0.553913, acc.: 78.12%] [G loss: 0.400403]\n",
      "101 [D loss: 0.763411, acc.: 46.88%] [G loss: 0.379652]\n",
      "102 [D loss: 0.832937, acc.: 43.75%] [G loss: 0.413270]\n",
      "103 [D loss: 0.694702, acc.: 59.38%] [G loss: 0.559101]\n",
      "104 [D loss: 0.607654, acc.: 75.00%] [G loss: 0.774265]\n",
      "105 [D loss: 0.573018, acc.: 81.25%] [G loss: 0.521947]\n",
      "106 [D loss: 0.428371, acc.: 90.62%] [G loss: 0.495021]\n",
      "107 [D loss: 0.394476, acc.: 93.75%] [G loss: 0.443248]\n",
      "108 [D loss: 0.498590, acc.: 78.12%] [G loss: 0.335548]\n",
      "109 [D loss: 0.699924, acc.: 56.25%] [G loss: 0.309707]\n",
      "110 [D loss: 0.801738, acc.: 40.62%] [G loss: 0.350453]\n",
      "111 [D loss: 0.647425, acc.: 53.12%] [G loss: 0.510811]\n",
      "112 [D loss: 0.708702, acc.: 53.12%] [G loss: 0.624930]\n",
      "113 [D loss: 0.718399, acc.: 50.00%] [G loss: 0.455654]\n",
      "114 [D loss: 0.382572, acc.: 90.62%] [G loss: 0.462514]\n",
      "115 [D loss: 0.438503, acc.: 87.50%] [G loss: 0.439206]\n",
      "116 [D loss: 0.574828, acc.: 75.00%] [G loss: 0.345541]\n",
      "117 [D loss: 0.857863, acc.: 40.62%] [G loss: 0.330628]\n",
      "118 [D loss: 0.671418, acc.: 53.12%] [G loss: 0.474874]\n",
      "119 [D loss: 0.681492, acc.: 53.12%] [G loss: 0.685756]\n",
      "120 [D loss: 0.704009, acc.: 53.12%] [G loss: 0.457362]\n",
      "121 [D loss: 0.392881, acc.: 96.88%] [G loss: 0.485263]\n",
      "122 [D loss: 0.445278, acc.: 84.38%] [G loss: 0.449988]\n",
      "123 [D loss: 0.644091, acc.: 62.50%] [G loss: 0.342514]\n",
      "124 [D loss: 0.905981, acc.: 40.62%] [G loss: 0.368218]\n",
      "125 [D loss: 0.865814, acc.: 21.88%] [G loss: 0.449698]\n",
      "126 [D loss: 0.635876, acc.: 65.62%] [G loss: 0.647371]\n",
      "127 [D loss: 0.693058, acc.: 53.12%] [G loss: 0.671397]\n",
      "128 [D loss: 0.634013, acc.: 71.88%] [G loss: 0.536345]\n",
      "129 [D loss: 0.408981, acc.: 90.62%] [G loss: 0.525907]\n",
      "130 [D loss: 0.444554, acc.: 87.50%] [G loss: 0.505986]\n",
      "131 [D loss: 0.458136, acc.: 81.25%] [G loss: 0.382650]\n",
      "132 [D loss: 0.815242, acc.: 43.75%] [G loss: 0.389275]\n",
      "133 [D loss: 0.727675, acc.: 53.12%] [G loss: 0.445387]\n",
      "134 [D loss: 0.553632, acc.: 81.25%] [G loss: 0.638328]\n",
      "135 [D loss: 0.705970, acc.: 53.12%] [G loss: 0.695526]\n",
      "136 [D loss: 0.685439, acc.: 56.25%] [G loss: 0.541729]\n",
      "137 [D loss: 0.475250, acc.: 81.25%] [G loss: 0.539134]\n",
      "138 [D loss: 0.416201, acc.: 90.62%] [G loss: 0.556583]\n",
      "139 [D loss: 0.660968, acc.: 59.38%] [G loss: 0.429810]\n",
      "140 [D loss: 0.761341, acc.: 43.75%] [G loss: 0.402331]\n",
      "141 [D loss: 0.679152, acc.: 56.25%] [G loss: 0.557457]\n",
      "142 [D loss: 0.660734, acc.: 65.62%] [G loss: 0.743823]\n",
      "143 [D loss: 0.661373, acc.: 53.12%] [G loss: 0.612556]\n",
      "144 [D loss: 0.621491, acc.: 71.88%] [G loss: 0.510668]\n",
      "145 [D loss: 0.466616, acc.: 81.25%] [G loss: 0.595611]\n",
      "146 [D loss: 0.484574, acc.: 84.38%] [G loss: 0.494064]\n",
      "147 [D loss: 0.746047, acc.: 43.75%] [G loss: 0.421538]\n",
      "148 [D loss: 0.713321, acc.: 46.88%] [G loss: 0.502705]\n",
      "149 [D loss: 0.656051, acc.: 68.75%] [G loss: 0.641905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.601584, acc.: 71.88%] [G loss: 0.728379]\n",
      "151 [D loss: 0.910456, acc.: 31.25%] [G loss: 0.535139]\n",
      "152 [D loss: 0.532625, acc.: 84.38%] [G loss: 0.494297]\n",
      "153 [D loss: 0.396972, acc.: 87.50%] [G loss: 0.535485]\n",
      "154 [D loss: 0.465591, acc.: 87.50%] [G loss: 0.598874]\n",
      "155 [D loss: 0.769406, acc.: 50.00%] [G loss: 0.440861]\n",
      "156 [D loss: 0.683315, acc.: 62.50%] [G loss: 0.434806]\n",
      "157 [D loss: 0.408061, acc.: 93.75%] [G loss: 0.578193]\n",
      "158 [D loss: 0.411713, acc.: 87.50%] [G loss: 0.642163]\n",
      "159 [D loss: 0.867611, acc.: 34.38%] [G loss: 0.499100]\n",
      "160 [D loss: 0.692575, acc.: 62.50%] [G loss: 0.439804]\n",
      "161 [D loss: 0.498850, acc.: 81.25%] [G loss: 0.456170]\n",
      "162 [D loss: 0.508497, acc.: 81.25%] [G loss: 0.493709]\n",
      "163 [D loss: 0.859021, acc.: 34.38%] [G loss: 0.344728]\n",
      "164 [D loss: 0.758102, acc.: 46.88%] [G loss: 0.391821]\n",
      "165 [D loss: 0.581569, acc.: 78.12%] [G loss: 0.508416]\n",
      "166 [D loss: 0.611311, acc.: 68.75%] [G loss: 0.553252]\n",
      "167 [D loss: 0.702758, acc.: 50.00%] [G loss: 0.526777]\n",
      "168 [D loss: 0.742565, acc.: 40.62%] [G loss: 0.444752]\n",
      "169 [D loss: 0.571054, acc.: 75.00%] [G loss: 0.429255]\n",
      "170 [D loss: 0.503849, acc.: 78.12%] [G loss: 0.449141]\n",
      "171 [D loss: 0.510638, acc.: 75.00%] [G loss: 0.438580]\n",
      "172 [D loss: 0.865295, acc.: 37.50%] [G loss: 0.392928]\n",
      "173 [D loss: 0.728876, acc.: 37.50%] [G loss: 0.437908]\n",
      "174 [D loss: 0.636019, acc.: 59.38%] [G loss: 0.475458]\n",
      "175 [D loss: 0.617780, acc.: 68.75%] [G loss: 0.595556]\n",
      "176 [D loss: 0.715597, acc.: 65.62%] [G loss: 0.573482]\n",
      "177 [D loss: 0.752208, acc.: 43.75%] [G loss: 0.576356]\n",
      "178 [D loss: 0.734718, acc.: 43.75%] [G loss: 0.581849]\n",
      "179 [D loss: 0.675004, acc.: 62.50%] [G loss: 0.575139]\n",
      "180 [D loss: 0.565807, acc.: 81.25%] [G loss: 0.606560]\n",
      "181 [D loss: 0.804729, acc.: 37.50%] [G loss: 0.530253]\n",
      "182 [D loss: 0.830070, acc.: 25.00%] [G loss: 0.505191]\n",
      "183 [D loss: 0.593370, acc.: 78.12%] [G loss: 0.569281]\n",
      "184 [D loss: 0.610886, acc.: 78.12%] [G loss: 0.636790]\n",
      "185 [D loss: 0.681712, acc.: 71.88%] [G loss: 0.624154]\n",
      "186 [D loss: 0.716515, acc.: 50.00%] [G loss: 0.588387]\n",
      "187 [D loss: 0.728341, acc.: 62.50%] [G loss: 0.540567]\n",
      "188 [D loss: 0.614718, acc.: 81.25%] [G loss: 0.566781]\n",
      "189 [D loss: 0.561675, acc.: 68.75%] [G loss: 0.603713]\n",
      "190 [D loss: 0.596439, acc.: 68.75%] [G loss: 0.646628]\n",
      "191 [D loss: 0.734728, acc.: 37.50%] [G loss: 0.538758]\n",
      "192 [D loss: 0.563145, acc.: 81.25%] [G loss: 0.580558]\n",
      "193 [D loss: 0.441430, acc.: 87.50%] [G loss: 0.646454]\n",
      "194 [D loss: 0.567790, acc.: 75.00%] [G loss: 0.690859]\n",
      "195 [D loss: 0.705381, acc.: 50.00%] [G loss: 0.563110]\n",
      "196 [D loss: 0.646392, acc.: 65.62%] [G loss: 0.550672]\n",
      "197 [D loss: 0.508378, acc.: 84.38%] [G loss: 0.626958]\n",
      "198 [D loss: 0.807833, acc.: 34.38%] [G loss: 0.503937]\n",
      "199 [D loss: 0.766852, acc.: 37.50%] [G loss: 0.511860]\n",
      "200 [D loss: 0.524931, acc.: 78.12%] [G loss: 0.592846]\n",
      "201 [D loss: 0.461057, acc.: 90.62%] [G loss: 0.673549]\n",
      "202 [D loss: 0.647263, acc.: 62.50%] [G loss: 0.768713]\n",
      "203 [D loss: 0.828143, acc.: 53.12%] [G loss: 0.529469]\n",
      "204 [D loss: 0.409547, acc.: 93.75%] [G loss: 0.599469]\n",
      "205 [D loss: 0.435456, acc.: 90.62%] [G loss: 0.608496]\n",
      "206 [D loss: 0.714798, acc.: 46.88%] [G loss: 0.577660]\n",
      "207 [D loss: 0.738894, acc.: 59.38%] [G loss: 0.551879]\n",
      "208 [D loss: 0.520149, acc.: 71.88%] [G loss: 0.731888]\n",
      "209 [D loss: 0.527295, acc.: 84.38%] [G loss: 0.713691]\n",
      "210 [D loss: 0.808571, acc.: 34.38%] [G loss: 0.599140]\n",
      "211 [D loss: 0.577655, acc.: 75.00%] [G loss: 0.523266]\n",
      "212 [D loss: 0.386531, acc.: 93.75%] [G loss: 0.668334]\n",
      "213 [D loss: 0.552346, acc.: 81.25%] [G loss: 0.585629]\n",
      "214 [D loss: 0.804585, acc.: 40.62%] [G loss: 0.503022]\n",
      "215 [D loss: 0.608894, acc.: 68.75%] [G loss: 0.598349]\n",
      "216 [D loss: 0.390788, acc.: 90.62%] [G loss: 0.637434]\n",
      "217 [D loss: 0.602188, acc.: 68.75%] [G loss: 0.633794]\n",
      "218 [D loss: 0.750452, acc.: 43.75%] [G loss: 0.521258]\n",
      "219 [D loss: 0.540318, acc.: 78.12%] [G loss: 0.590703]\n",
      "220 [D loss: 0.445252, acc.: 87.50%] [G loss: 0.665550]\n",
      "221 [D loss: 0.711995, acc.: 53.12%] [G loss: 0.537422]\n",
      "222 [D loss: 0.812546, acc.: 31.25%] [G loss: 0.461075]\n",
      "223 [D loss: 0.591918, acc.: 68.75%] [G loss: 0.626332]\n",
      "224 [D loss: 0.702670, acc.: 46.88%] [G loss: 0.693909]\n",
      "225 [D loss: 0.694517, acc.: 59.38%] [G loss: 0.518404]\n",
      "226 [D loss: 0.497342, acc.: 84.38%] [G loss: 0.623933]\n",
      "227 [D loss: 0.446514, acc.: 84.38%] [G loss: 0.536689]\n",
      "228 [D loss: 0.651451, acc.: 59.38%] [G loss: 0.508433]\n",
      "229 [D loss: 0.905876, acc.: 25.00%] [G loss: 0.486806]\n",
      "230 [D loss: 0.633718, acc.: 65.62%] [G loss: 0.535671]\n",
      "231 [D loss: 0.554469, acc.: 78.12%] [G loss: 0.661635]\n",
      "232 [D loss: 0.769840, acc.: 43.75%] [G loss: 0.623074]\n",
      "233 [D loss: 0.726339, acc.: 56.25%] [G loss: 0.542428]\n",
      "234 [D loss: 0.569616, acc.: 81.25%] [G loss: 0.536960]\n",
      "235 [D loss: 0.374451, acc.: 93.75%] [G loss: 0.582304]\n",
      "236 [D loss: 0.535178, acc.: 78.12%] [G loss: 0.456524]\n",
      "237 [D loss: 0.921647, acc.: 37.50%] [G loss: 0.431674]\n",
      "238 [D loss: 0.802477, acc.: 40.62%] [G loss: 0.454334]\n",
      "239 [D loss: 0.579142, acc.: 84.38%] [G loss: 0.558267]\n",
      "240 [D loss: 0.616572, acc.: 71.88%] [G loss: 0.580179]\n",
      "241 [D loss: 0.646516, acc.: 56.25%] [G loss: 0.613190]\n",
      "242 [D loss: 0.732251, acc.: 43.75%] [G loss: 0.563120]\n",
      "243 [D loss: 0.590034, acc.: 71.88%] [G loss: 0.534651]\n",
      "244 [D loss: 0.509681, acc.: 87.50%] [G loss: 0.585162]\n",
      "245 [D loss: 0.611034, acc.: 75.00%] [G loss: 0.590119]\n",
      "246 [D loss: 0.623425, acc.: 62.50%] [G loss: 0.525886]\n",
      "247 [D loss: 0.682934, acc.: 56.25%] [G loss: 0.471835]\n",
      "248 [D loss: 0.642220, acc.: 65.62%] [G loss: 0.460414]\n",
      "249 [D loss: 0.453807, acc.: 84.38%] [G loss: 0.572978]\n",
      "250 [D loss: 0.556710, acc.: 75.00%] [G loss: 0.565650]\n",
      "251 [D loss: 0.600191, acc.: 68.75%] [G loss: 0.600458]\n",
      "252 [D loss: 0.727365, acc.: 46.88%] [G loss: 0.574779]\n",
      "253 [D loss: 0.646743, acc.: 68.75%] [G loss: 0.494808]\n",
      "254 [D loss: 0.478470, acc.: 90.62%] [G loss: 0.510166]\n",
      "255 [D loss: 0.447322, acc.: 87.50%] [G loss: 0.594505]\n",
      "256 [D loss: 0.646296, acc.: 75.00%] [G loss: 0.560085]\n",
      "257 [D loss: 0.866303, acc.: 40.62%] [G loss: 0.449237]\n",
      "258 [D loss: 0.554531, acc.: 75.00%] [G loss: 0.474437]\n",
      "259 [D loss: 0.553972, acc.: 75.00%] [G loss: 0.577234]\n",
      "260 [D loss: 0.582429, acc.: 71.88%] [G loss: 0.548017]\n",
      "261 [D loss: 0.641073, acc.: 56.25%] [G loss: 0.591248]\n",
      "262 [D loss: 0.758477, acc.: 43.75%] [G loss: 0.545936]\n",
      "263 [D loss: 0.661268, acc.: 68.75%] [G loss: 0.493039]\n",
      "264 [D loss: 0.434223, acc.: 87.50%] [G loss: 0.558724]\n",
      "265 [D loss: 0.612083, acc.: 65.62%] [G loss: 0.523200]\n",
      "266 [D loss: 0.896446, acc.: 37.50%] [G loss: 0.471705]\n",
      "267 [D loss: 0.597131, acc.: 75.00%] [G loss: 0.469429]\n",
      "268 [D loss: 0.569584, acc.: 78.12%] [G loss: 0.581103]\n",
      "269 [D loss: 0.564524, acc.: 78.12%] [G loss: 0.661620]\n",
      "270 [D loss: 0.775729, acc.: 37.50%] [G loss: 0.555062]\n",
      "271 [D loss: 0.640766, acc.: 65.62%] [G loss: 0.497323]\n",
      "272 [D loss: 0.540339, acc.: 75.00%] [G loss: 0.556371]\n",
      "273 [D loss: 0.503602, acc.: 81.25%] [G loss: 0.517384]\n",
      "274 [D loss: 0.576223, acc.: 75.00%] [G loss: 0.458628]\n",
      "275 [D loss: 0.786551, acc.: 34.38%] [G loss: 0.443010]\n",
      "276 [D loss: 0.750390, acc.: 40.62%] [G loss: 0.472582]\n",
      "277 [D loss: 0.616828, acc.: 68.75%] [G loss: 0.582753]\n",
      "278 [D loss: 0.620649, acc.: 62.50%] [G loss: 0.673742]\n",
      "279 [D loss: 0.631311, acc.: 65.62%] [G loss: 0.729977]\n",
      "280 [D loss: 0.736875, acc.: 50.00%] [G loss: 0.593440]\n",
      "281 [D loss: 0.599842, acc.: 78.12%] [G loss: 0.655246]\n",
      "282 [D loss: 0.530233, acc.: 81.25%] [G loss: 0.677898]\n",
      "283 [D loss: 0.711677, acc.: 46.88%] [G loss: 0.602781]\n",
      "284 [D loss: 0.672308, acc.: 56.25%] [G loss: 0.547904]\n",
      "285 [D loss: 0.506429, acc.: 87.50%] [G loss: 0.638458]\n",
      "286 [D loss: 0.447661, acc.: 87.50%] [G loss: 0.629322]\n",
      "287 [D loss: 0.553380, acc.: 75.00%] [G loss: 0.586829]\n",
      "288 [D loss: 0.765625, acc.: 50.00%] [G loss: 0.561278]\n",
      "289 [D loss: 0.682702, acc.: 53.12%] [G loss: 0.565408]\n",
      "290 [D loss: 0.512782, acc.: 75.00%] [G loss: 0.638800]\n",
      "291 [D loss: 0.543363, acc.: 78.12%] [G loss: 0.686962]\n",
      "292 [D loss: 0.725015, acc.: 56.25%] [G loss: 0.540443]\n",
      "293 [D loss: 0.589150, acc.: 71.88%] [G loss: 0.543559]\n",
      "294 [D loss: 0.556734, acc.: 78.12%] [G loss: 0.538407]\n",
      "295 [D loss: 0.497573, acc.: 81.25%] [G loss: 0.668536]\n",
      "296 [D loss: 0.653081, acc.: 65.62%] [G loss: 0.758083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.720559, acc.: 59.38%] [G loss: 0.634566]\n",
      "298 [D loss: 0.585133, acc.: 68.75%] [G loss: 0.606935]\n",
      "299 [D loss: 0.497159, acc.: 87.50%] [G loss: 0.636418]\n",
      "300 [D loss: 0.599915, acc.: 71.88%] [G loss: 0.649275]\n",
      "301 [D loss: 0.731195, acc.: 50.00%] [G loss: 0.571555]\n",
      "302 [D loss: 0.573800, acc.: 75.00%] [G loss: 0.571850]\n",
      "303 [D loss: 0.502262, acc.: 75.00%] [G loss: 0.629014]\n",
      "304 [D loss: 0.475763, acc.: 84.38%] [G loss: 0.617083]\n",
      "305 [D loss: 0.608954, acc.: 65.62%] [G loss: 0.626717]\n",
      "306 [D loss: 0.731175, acc.: 56.25%] [G loss: 0.511843]\n",
      "307 [D loss: 0.599962, acc.: 68.75%] [G loss: 0.573445]\n",
      "308 [D loss: 0.562996, acc.: 59.38%] [G loss: 0.468838]\n",
      "309 [D loss: 0.679782, acc.: 53.12%] [G loss: 0.535441]\n",
      "310 [D loss: 0.722687, acc.: 50.00%] [G loss: 0.453437]\n",
      "311 [D loss: 0.681029, acc.: 56.25%] [G loss: 0.456189]\n",
      "312 [D loss: 0.471370, acc.: 84.38%] [G loss: 0.546408]\n",
      "313 [D loss: 0.498520, acc.: 84.38%] [G loss: 0.498911]\n",
      "314 [D loss: 0.671498, acc.: 59.38%] [G loss: 0.527886]\n",
      "315 [D loss: 0.955854, acc.: 31.25%] [G loss: 0.471491]\n",
      "316 [D loss: 0.702096, acc.: 34.38%] [G loss: 0.394104]\n",
      "317 [D loss: 0.696596, acc.: 68.75%] [G loss: 0.433221]\n",
      "318 [D loss: 0.500598, acc.: 78.12%] [G loss: 0.537464]\n",
      "319 [D loss: 0.807424, acc.: 37.50%] [G loss: 0.429088]\n",
      "320 [D loss: 0.708247, acc.: 53.12%] [G loss: 0.428267]\n",
      "321 [D loss: 0.670155, acc.: 53.12%] [G loss: 0.478430]\n",
      "322 [D loss: 0.559521, acc.: 71.88%] [G loss: 0.490768]\n",
      "323 [D loss: 0.556593, acc.: 75.00%] [G loss: 0.525080]\n",
      "324 [D loss: 0.625120, acc.: 65.62%] [G loss: 0.586184]\n",
      "325 [D loss: 0.749325, acc.: 43.75%] [G loss: 0.542505]\n",
      "326 [D loss: 0.730617, acc.: 53.12%] [G loss: 0.438380]\n",
      "327 [D loss: 0.668217, acc.: 62.50%] [G loss: 0.434945]\n",
      "328 [D loss: 0.641546, acc.: 68.75%] [G loss: 0.483960]\n",
      "329 [D loss: 0.707382, acc.: 59.38%] [G loss: 0.533557]\n",
      "330 [D loss: 0.763615, acc.: 37.50%] [G loss: 0.562015]\n",
      "331 [D loss: 0.651877, acc.: 53.12%] [G loss: 0.513648]\n",
      "332 [D loss: 0.615561, acc.: 68.75%] [G loss: 0.579864]\n",
      "333 [D loss: 0.743944, acc.: 46.88%] [G loss: 0.598371]\n",
      "334 [D loss: 0.586022, acc.: 78.12%] [G loss: 0.648421]\n",
      "335 [D loss: 0.620110, acc.: 65.62%] [G loss: 0.613437]\n",
      "336 [D loss: 0.658885, acc.: 62.50%] [G loss: 0.608431]\n",
      "337 [D loss: 0.649197, acc.: 59.38%] [G loss: 0.568500]\n",
      "338 [D loss: 0.573358, acc.: 71.88%] [G loss: 0.602484]\n",
      "339 [D loss: 0.658665, acc.: 65.62%] [G loss: 0.588400]\n",
      "340 [D loss: 0.708432, acc.: 40.62%] [G loss: 0.530098]\n",
      "341 [D loss: 0.700821, acc.: 56.25%] [G loss: 0.590359]\n",
      "342 [D loss: 0.761565, acc.: 40.62%] [G loss: 0.631621]\n",
      "343 [D loss: 0.638411, acc.: 71.88%] [G loss: 0.784493]\n",
      "344 [D loss: 0.672389, acc.: 62.50%] [G loss: 0.810689]\n",
      "345 [D loss: 0.692577, acc.: 50.00%] [G loss: 0.686598]\n",
      "346 [D loss: 0.633594, acc.: 59.38%] [G loss: 0.614950]\n",
      "347 [D loss: 0.577748, acc.: 81.25%] [G loss: 0.674717]\n",
      "348 [D loss: 0.439852, acc.: 93.75%] [G loss: 0.621017]\n",
      "349 [D loss: 0.612936, acc.: 59.38%] [G loss: 0.560175]\n",
      "350 [D loss: 0.762918, acc.: 46.88%] [G loss: 0.496112]\n",
      "351 [D loss: 0.648610, acc.: 59.38%] [G loss: 0.547412]\n",
      "352 [D loss: 0.816518, acc.: 40.62%] [G loss: 0.544798]\n",
      "353 [D loss: 0.672751, acc.: 68.75%] [G loss: 0.662091]\n",
      "354 [D loss: 0.677104, acc.: 53.12%] [G loss: 0.668434]\n",
      "355 [D loss: 0.672004, acc.: 50.00%] [G loss: 0.682770]\n",
      "356 [D loss: 0.708718, acc.: 50.00%] [G loss: 0.659884]\n",
      "357 [D loss: 0.650646, acc.: 62.50%] [G loss: 0.618749]\n",
      "358 [D loss: 0.566392, acc.: 78.12%] [G loss: 0.586260]\n",
      "359 [D loss: 0.559803, acc.: 75.00%] [G loss: 0.536504]\n",
      "360 [D loss: 0.589657, acc.: 68.75%] [G loss: 0.521918]\n",
      "361 [D loss: 0.605041, acc.: 75.00%] [G loss: 0.492186]\n",
      "362 [D loss: 0.695357, acc.: 53.12%] [G loss: 0.487447]\n",
      "363 [D loss: 0.669019, acc.: 62.50%] [G loss: 0.487411]\n",
      "364 [D loss: 0.647755, acc.: 56.25%] [G loss: 0.606387]\n",
      "365 [D loss: 0.603103, acc.: 65.62%] [G loss: 0.792763]\n",
      "366 [D loss: 0.644978, acc.: 59.38%] [G loss: 0.766303]\n",
      "367 [D loss: 0.646261, acc.: 68.75%] [G loss: 0.740921]\n",
      "368 [D loss: 0.698771, acc.: 56.25%] [G loss: 0.678329]\n",
      "369 [D loss: 0.619721, acc.: 75.00%] [G loss: 0.663943]\n",
      "370 [D loss: 0.613978, acc.: 56.25%] [G loss: 0.628172]\n",
      "371 [D loss: 0.529214, acc.: 78.12%] [G loss: 0.612265]\n",
      "372 [D loss: 0.743858, acc.: 46.88%] [G loss: 0.521322]\n",
      "373 [D loss: 0.666639, acc.: 62.50%] [G loss: 0.522482]\n",
      "374 [D loss: 0.592844, acc.: 75.00%] [G loss: 0.681090]\n",
      "375 [D loss: 0.554208, acc.: 78.12%] [G loss: 0.673386]\n",
      "376 [D loss: 0.697652, acc.: 46.88%] [G loss: 0.709256]\n",
      "377 [D loss: 0.612552, acc.: 65.62%] [G loss: 0.829890]\n",
      "378 [D loss: 0.611274, acc.: 65.62%] [G loss: 0.811277]\n",
      "379 [D loss: 0.619043, acc.: 62.50%] [G loss: 0.773116]\n",
      "380 [D loss: 0.574444, acc.: 78.12%] [G loss: 0.735261]\n",
      "381 [D loss: 0.566578, acc.: 71.88%] [G loss: 0.660802]\n",
      "382 [D loss: 0.572910, acc.: 75.00%] [G loss: 0.548990]\n",
      "383 [D loss: 0.580684, acc.: 68.75%] [G loss: 0.543058]\n",
      "384 [D loss: 0.534472, acc.: 75.00%] [G loss: 0.519767]\n",
      "385 [D loss: 0.680936, acc.: 59.38%] [G loss: 0.510898]\n",
      "386 [D loss: 0.767376, acc.: 56.25%] [G loss: 0.530073]\n",
      "387 [D loss: 0.654578, acc.: 65.62%] [G loss: 0.598854]\n",
      "388 [D loss: 0.610953, acc.: 62.50%] [G loss: 0.726376]\n",
      "389 [D loss: 0.627020, acc.: 71.88%] [G loss: 0.798138]\n",
      "390 [D loss: 0.658197, acc.: 62.50%] [G loss: 0.739079]\n",
      "391 [D loss: 0.751448, acc.: 43.75%] [G loss: 0.635204]\n",
      "392 [D loss: 0.627528, acc.: 59.38%] [G loss: 0.569417]\n",
      "393 [D loss: 0.513035, acc.: 78.12%] [G loss: 0.539629]\n",
      "394 [D loss: 0.410414, acc.: 87.50%] [G loss: 0.458651]\n",
      "395 [D loss: 0.431615, acc.: 78.12%] [G loss: 0.418132]\n",
      "396 [D loss: 0.458190, acc.: 84.38%] [G loss: 0.385971]\n",
      "397 [D loss: 0.595924, acc.: 62.50%] [G loss: 0.305346]\n",
      "398 [D loss: 0.773203, acc.: 53.12%] [G loss: 0.340837]\n",
      "399 [D loss: 0.846356, acc.: 43.75%] [G loss: 0.413671]\n",
      "400 [D loss: 0.700924, acc.: 53.12%] [G loss: 0.499508]\n",
      "401 [D loss: 0.671545, acc.: 65.62%] [G loss: 0.643806]\n",
      "402 [D loss: 0.664190, acc.: 62.50%] [G loss: 0.678379]\n",
      "403 [D loss: 0.600815, acc.: 62.50%] [G loss: 0.666087]\n",
      "404 [D loss: 0.598144, acc.: 75.00%] [G loss: 0.633479]\n",
      "405 [D loss: 0.519599, acc.: 71.88%] [G loss: 0.559638]\n",
      "406 [D loss: 0.470510, acc.: 90.62%] [G loss: 0.443280]\n",
      "407 [D loss: 0.495127, acc.: 75.00%] [G loss: 0.407944]\n",
      "408 [D loss: 0.409420, acc.: 87.50%] [G loss: 0.360085]\n",
      "409 [D loss: 0.512685, acc.: 65.62%] [G loss: 0.274169]\n",
      "410 [D loss: 0.526504, acc.: 78.12%] [G loss: 0.325506]\n",
      "411 [D loss: 0.763316, acc.: 53.12%] [G loss: 0.299316]\n",
      "412 [D loss: 0.721853, acc.: 46.88%] [G loss: 0.323861]\n",
      "413 [D loss: 0.713517, acc.: 56.25%] [G loss: 0.396438]\n",
      "414 [D loss: 0.598870, acc.: 75.00%] [G loss: 0.451052]\n",
      "415 [D loss: 0.642575, acc.: 62.50%] [G loss: 0.552007]\n",
      "416 [D loss: 0.667701, acc.: 62.50%] [G loss: 0.632352]\n",
      "417 [D loss: 0.618659, acc.: 62.50%] [G loss: 0.685817]\n",
      "418 [D loss: 0.589166, acc.: 68.75%] [G loss: 0.617276]\n",
      "419 [D loss: 0.608573, acc.: 65.62%] [G loss: 0.515092]\n",
      "420 [D loss: 0.605889, acc.: 65.62%] [G loss: 0.476551]\n",
      "421 [D loss: 0.488329, acc.: 90.62%] [G loss: 0.481956]\n",
      "422 [D loss: 0.564028, acc.: 68.75%] [G loss: 0.430986]\n",
      "423 [D loss: 0.628605, acc.: 68.75%] [G loss: 0.416603]\n",
      "424 [D loss: 0.525517, acc.: 75.00%] [G loss: 0.405369]\n",
      "425 [D loss: 0.731541, acc.: 56.25%] [G loss: 0.486154]\n",
      "426 [D loss: 0.711962, acc.: 56.25%] [G loss: 0.575022]\n",
      "427 [D loss: 0.688292, acc.: 53.12%] [G loss: 0.752088]\n",
      "428 [D loss: 0.505737, acc.: 81.25%] [G loss: 0.741419]\n",
      "429 [D loss: 0.605625, acc.: 68.75%] [G loss: 0.785968]\n",
      "430 [D loss: 0.702295, acc.: 40.62%] [G loss: 0.651768]\n",
      "431 [D loss: 0.530575, acc.: 78.12%] [G loss: 0.572212]\n",
      "432 [D loss: 0.431831, acc.: 87.50%] [G loss: 0.456247]\n",
      "433 [D loss: 0.535052, acc.: 75.00%] [G loss: 0.331981]\n",
      "434 [D loss: 0.499949, acc.: 75.00%] [G loss: 0.272771]\n",
      "435 [D loss: 0.487841, acc.: 75.00%] [G loss: 0.258480]\n",
      "436 [D loss: 0.722537, acc.: 59.38%] [G loss: 0.274031]\n",
      "437 [D loss: 0.908861, acc.: 34.38%] [G loss: 0.333831]\n",
      "438 [D loss: 0.791319, acc.: 40.62%] [G loss: 0.410729]\n",
      "439 [D loss: 0.643940, acc.: 53.12%] [G loss: 0.619245]\n",
      "440 [D loss: 0.656554, acc.: 53.12%] [G loss: 0.755711]\n",
      "441 [D loss: 0.638577, acc.: 65.62%] [G loss: 0.670263]\n",
      "442 [D loss: 0.560765, acc.: 68.75%] [G loss: 0.724502]\n",
      "443 [D loss: 0.466397, acc.: 81.25%] [G loss: 0.543786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 0.450114, acc.: 81.25%] [G loss: 0.454380]\n",
      "445 [D loss: 0.392234, acc.: 87.50%] [G loss: 0.355939]\n",
      "446 [D loss: 0.537588, acc.: 75.00%] [G loss: 0.306921]\n",
      "447 [D loss: 0.538266, acc.: 68.75%] [G loss: 0.359956]\n",
      "448 [D loss: 0.587381, acc.: 65.62%] [G loss: 0.376507]\n",
      "449 [D loss: 0.671446, acc.: 50.00%] [G loss: 0.428036]\n",
      "450 [D loss: 0.805127, acc.: 53.12%] [G loss: 0.562454]\n",
      "451 [D loss: 0.686995, acc.: 65.62%] [G loss: 0.706618]\n",
      "452 [D loss: 0.613240, acc.: 68.75%] [G loss: 0.821261]\n",
      "453 [D loss: 0.633106, acc.: 71.88%] [G loss: 0.811379]\n",
      "454 [D loss: 0.708692, acc.: 53.12%] [G loss: 0.732788]\n",
      "455 [D loss: 0.554131, acc.: 71.88%] [G loss: 0.544124]\n",
      "456 [D loss: 0.403427, acc.: 90.62%] [G loss: 0.500440]\n",
      "457 [D loss: 0.490519, acc.: 75.00%] [G loss: 0.489628]\n",
      "458 [D loss: 0.572076, acc.: 68.75%] [G loss: 0.464934]\n",
      "459 [D loss: 0.676254, acc.: 56.25%] [G loss: 0.522751]\n",
      "460 [D loss: 0.654096, acc.: 59.38%] [G loss: 0.615965]\n",
      "461 [D loss: 0.653960, acc.: 59.38%] [G loss: 0.758430]\n",
      "462 [D loss: 0.531599, acc.: 71.88%] [G loss: 0.819546]\n",
      "463 [D loss: 0.698707, acc.: 59.38%] [G loss: 0.733871]\n",
      "464 [D loss: 0.623080, acc.: 62.50%] [G loss: 0.688241]\n",
      "465 [D loss: 0.579877, acc.: 65.62%] [G loss: 0.556603]\n",
      "466 [D loss: 0.443706, acc.: 90.62%] [G loss: 0.535518]\n",
      "467 [D loss: 0.444635, acc.: 81.25%] [G loss: 0.450493]\n",
      "468 [D loss: 0.630563, acc.: 68.75%] [G loss: 0.443763]\n",
      "469 [D loss: 0.692981, acc.: 59.38%] [G loss: 0.435316]\n",
      "470 [D loss: 0.531032, acc.: 84.38%] [G loss: 0.605537]\n",
      "471 [D loss: 0.520815, acc.: 84.38%] [G loss: 0.739269]\n",
      "472 [D loss: 0.516957, acc.: 81.25%] [G loss: 0.711855]\n",
      "473 [D loss: 0.752402, acc.: 40.62%] [G loss: 0.790417]\n",
      "474 [D loss: 0.542188, acc.: 78.12%] [G loss: 0.686739]\n",
      "475 [D loss: 0.352409, acc.: 87.50%] [G loss: 0.582790]\n",
      "476 [D loss: 0.434733, acc.: 87.50%] [G loss: 0.509869]\n",
      "477 [D loss: 0.584154, acc.: 68.75%] [G loss: 0.437342]\n",
      "478 [D loss: 0.742611, acc.: 46.88%] [G loss: 0.543853]\n",
      "479 [D loss: 0.613069, acc.: 65.62%] [G loss: 0.588478]\n",
      "480 [D loss: 0.649747, acc.: 62.50%] [G loss: 0.703059]\n",
      "481 [D loss: 0.694606, acc.: 68.75%] [G loss: 0.889774]\n",
      "482 [D loss: 0.593386, acc.: 71.88%] [G loss: 0.840025]\n",
      "483 [D loss: 0.598123, acc.: 71.88%] [G loss: 0.635349]\n",
      "484 [D loss: 0.434619, acc.: 81.25%] [G loss: 0.566535]\n",
      "485 [D loss: 0.477969, acc.: 81.25%] [G loss: 0.469623]\n",
      "486 [D loss: 0.610785, acc.: 62.50%] [G loss: 0.380552]\n",
      "487 [D loss: 0.696644, acc.: 53.12%] [G loss: 0.427088]\n",
      "488 [D loss: 0.540196, acc.: 71.88%] [G loss: 0.630756]\n",
      "489 [D loss: 0.619447, acc.: 71.88%] [G loss: 0.735387]\n",
      "490 [D loss: 0.776391, acc.: 50.00%] [G loss: 0.629739]\n",
      "491 [D loss: 0.600306, acc.: 62.50%] [G loss: 0.680066]\n",
      "492 [D loss: 0.612363, acc.: 62.50%] [G loss: 0.600802]\n",
      "493 [D loss: 0.397702, acc.: 93.75%] [G loss: 0.494226]\n",
      "494 [D loss: 0.477507, acc.: 84.38%] [G loss: 0.503411]\n",
      "495 [D loss: 0.672929, acc.: 59.38%] [G loss: 0.459866]\n",
      "496 [D loss: 0.832657, acc.: 37.50%] [G loss: 0.500519]\n",
      "497 [D loss: 0.746893, acc.: 68.75%] [G loss: 0.595161]\n",
      "498 [D loss: 0.512030, acc.: 84.38%] [G loss: 0.765592]\n",
      "499 [D loss: 0.637721, acc.: 62.50%] [G loss: 0.828873]\n",
      "500 [D loss: 0.706936, acc.: 62.50%] [G loss: 0.718411]\n",
      "501 [D loss: 0.621030, acc.: 62.50%] [G loss: 0.657009]\n",
      "502 [D loss: 0.494266, acc.: 81.25%] [G loss: 0.544587]\n",
      "503 [D loss: 0.509870, acc.: 75.00%] [G loss: 0.525940]\n",
      "504 [D loss: 0.537768, acc.: 71.88%] [G loss: 0.504326]\n",
      "505 [D loss: 0.738823, acc.: 50.00%] [G loss: 0.483594]\n",
      "506 [D loss: 0.673764, acc.: 71.88%] [G loss: 0.519808]\n",
      "507 [D loss: 0.539584, acc.: 84.38%] [G loss: 0.624472]\n",
      "508 [D loss: 0.600369, acc.: 65.62%] [G loss: 0.754017]\n",
      "509 [D loss: 0.556665, acc.: 71.88%] [G loss: 0.788495]\n",
      "510 [D loss: 0.842774, acc.: 28.12%] [G loss: 0.517525]\n",
      "511 [D loss: 0.520810, acc.: 71.88%] [G loss: 0.541374]\n",
      "512 [D loss: 0.487209, acc.: 81.25%] [G loss: 0.591430]\n",
      "513 [D loss: 0.580147, acc.: 71.88%] [G loss: 0.484261]\n",
      "514 [D loss: 0.732603, acc.: 46.88%] [G loss: 0.513166]\n",
      "515 [D loss: 0.677130, acc.: 56.25%] [G loss: 0.495502]\n",
      "516 [D loss: 0.610763, acc.: 65.62%] [G loss: 0.574359]\n",
      "517 [D loss: 0.512145, acc.: 90.62%] [G loss: 0.676256]\n",
      "518 [D loss: 0.531677, acc.: 84.38%] [G loss: 0.791756]\n",
      "519 [D loss: 0.584427, acc.: 71.88%] [G loss: 0.779201]\n",
      "520 [D loss: 0.754531, acc.: 53.12%] [G loss: 0.626282]\n",
      "521 [D loss: 0.481742, acc.: 84.38%] [G loss: 0.637309]\n",
      "522 [D loss: 0.452896, acc.: 81.25%] [G loss: 0.750986]\n",
      "523 [D loss: 0.595526, acc.: 71.88%] [G loss: 0.651957]\n",
      "524 [D loss: 0.716884, acc.: 59.38%] [G loss: 0.604289]\n",
      "525 [D loss: 0.532853, acc.: 81.25%] [G loss: 0.654187]\n",
      "526 [D loss: 0.561074, acc.: 71.88%] [G loss: 0.728164]\n",
      "527 [D loss: 0.452403, acc.: 90.62%] [G loss: 0.712721]\n",
      "528 [D loss: 0.687298, acc.: 56.25%] [G loss: 0.804592]\n",
      "529 [D loss: 0.704991, acc.: 53.12%] [G loss: 0.736912]\n",
      "530 [D loss: 0.761987, acc.: 59.38%] [G loss: 0.708955]\n",
      "531 [D loss: 0.642601, acc.: 62.50%] [G loss: 0.785835]\n",
      "532 [D loss: 0.675472, acc.: 53.12%] [G loss: 0.872768]\n",
      "533 [D loss: 0.721789, acc.: 50.00%] [G loss: 0.699721]\n",
      "534 [D loss: 0.587217, acc.: 68.75%] [G loss: 0.825570]\n",
      "535 [D loss: 0.532487, acc.: 78.12%] [G loss: 0.919265]\n",
      "536 [D loss: 0.600878, acc.: 65.62%] [G loss: 0.740902]\n",
      "537 [D loss: 0.724112, acc.: 50.00%] [G loss: 0.696732]\n",
      "538 [D loss: 0.678977, acc.: 50.00%] [G loss: 0.729398]\n",
      "539 [D loss: 0.615116, acc.: 65.62%] [G loss: 0.796939]\n",
      "540 [D loss: 0.590248, acc.: 56.25%] [G loss: 0.926735]\n",
      "541 [D loss: 0.636056, acc.: 56.25%] [G loss: 0.843451]\n",
      "542 [D loss: 0.754333, acc.: 50.00%] [G loss: 0.719732]\n",
      "543 [D loss: 0.648043, acc.: 56.25%] [G loss: 0.764580]\n",
      "544 [D loss: 0.477600, acc.: 84.38%] [G loss: 0.917562]\n",
      "545 [D loss: 0.553934, acc.: 68.75%] [G loss: 0.907425]\n",
      "546 [D loss: 0.624678, acc.: 68.75%] [G loss: 0.801470]\n",
      "547 [D loss: 0.647650, acc.: 62.50%] [G loss: 0.747076]\n",
      "548 [D loss: 0.524473, acc.: 81.25%] [G loss: 0.747307]\n",
      "549 [D loss: 0.453766, acc.: 90.62%] [G loss: 0.752179]\n",
      "550 [D loss: 0.643415, acc.: 56.25%] [G loss: 0.716931]\n",
      "551 [D loss: 0.628470, acc.: 62.50%] [G loss: 0.645685]\n",
      "552 [D loss: 0.598572, acc.: 68.75%] [G loss: 0.773921]\n",
      "553 [D loss: 0.505418, acc.: 81.25%] [G loss: 0.829461]\n",
      "554 [D loss: 0.577102, acc.: 62.50%] [G loss: 0.845728]\n",
      "555 [D loss: 0.634387, acc.: 59.38%] [G loss: 0.858730]\n",
      "556 [D loss: 0.667797, acc.: 59.38%] [G loss: 0.734007]\n",
      "557 [D loss: 0.621440, acc.: 65.62%] [G loss: 0.676688]\n",
      "558 [D loss: 0.513959, acc.: 71.88%] [G loss: 0.725587]\n",
      "559 [D loss: 0.507824, acc.: 75.00%] [G loss: 0.652844]\n",
      "560 [D loss: 0.716302, acc.: 37.50%] [G loss: 0.547231]\n",
      "561 [D loss: 0.563953, acc.: 75.00%] [G loss: 0.601429]\n",
      "562 [D loss: 0.508410, acc.: 71.88%] [G loss: 0.599904]\n",
      "563 [D loss: 0.515007, acc.: 75.00%] [G loss: 0.677280]\n",
      "564 [D loss: 0.544295, acc.: 75.00%] [G loss: 0.604009]\n",
      "565 [D loss: 0.724163, acc.: 59.38%] [G loss: 0.511012]\n",
      "566 [D loss: 0.482010, acc.: 81.25%] [G loss: 0.492903]\n",
      "567 [D loss: 0.427713, acc.: 84.38%] [G loss: 0.508367]\n",
      "568 [D loss: 0.538191, acc.: 71.88%] [G loss: 0.435543]\n",
      "569 [D loss: 0.694589, acc.: 71.88%] [G loss: 0.384073]\n",
      "570 [D loss: 0.752282, acc.: 46.88%] [G loss: 0.518538]\n",
      "571 [D loss: 0.636698, acc.: 62.50%] [G loss: 0.562115]\n",
      "572 [D loss: 0.598124, acc.: 65.62%] [G loss: 0.631995]\n",
      "573 [D loss: 0.721284, acc.: 65.62%] [G loss: 0.591783]\n",
      "574 [D loss: 0.581812, acc.: 68.75%] [G loss: 0.531760]\n",
      "575 [D loss: 0.582028, acc.: 71.88%] [G loss: 0.439973]\n",
      "576 [D loss: 0.582235, acc.: 68.75%] [G loss: 0.373971]\n",
      "577 [D loss: 0.451055, acc.: 81.25%] [G loss: 0.379394]\n",
      "578 [D loss: 0.434187, acc.: 87.50%] [G loss: 0.379862]\n",
      "579 [D loss: 0.836968, acc.: 31.25%] [G loss: 0.333110]\n",
      "580 [D loss: 0.778914, acc.: 46.88%] [G loss: 0.377321]\n",
      "581 [D loss: 0.648590, acc.: 59.38%] [G loss: 0.503488]\n",
      "582 [D loss: 0.644321, acc.: 59.38%] [G loss: 0.644919]\n",
      "583 [D loss: 0.591614, acc.: 75.00%] [G loss: 0.709427]\n",
      "584 [D loss: 0.661234, acc.: 59.38%] [G loss: 0.691736]\n",
      "585 [D loss: 0.566330, acc.: 75.00%] [G loss: 0.524923]\n",
      "586 [D loss: 0.561624, acc.: 71.88%] [G loss: 0.449603]\n",
      "587 [D loss: 0.398793, acc.: 84.38%] [G loss: 0.411985]\n",
      "588 [D loss: 0.363643, acc.: 87.50%] [G loss: 0.388659]\n",
      "589 [D loss: 0.578875, acc.: 75.00%] [G loss: 0.410544]\n",
      "590 [D loss: 0.771608, acc.: 50.00%] [G loss: 0.412876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.776995, acc.: 46.88%] [G loss: 0.596335]\n",
      "592 [D loss: 0.539344, acc.: 71.88%] [G loss: 0.804362]\n",
      "593 [D loss: 0.676422, acc.: 62.50%] [G loss: 0.773072]\n",
      "594 [D loss: 0.644549, acc.: 65.62%] [G loss: 0.793167]\n",
      "595 [D loss: 0.587966, acc.: 71.88%] [G loss: 0.640528]\n",
      "596 [D loss: 0.528049, acc.: 71.88%] [G loss: 0.646976]\n",
      "597 [D loss: 0.453025, acc.: 81.25%] [G loss: 0.534260]\n",
      "598 [D loss: 0.381834, acc.: 90.62%] [G loss: 0.564864]\n",
      "599 [D loss: 0.533173, acc.: 68.75%] [G loss: 0.451503]\n",
      "600 [D loss: 0.678373, acc.: 62.50%] [G loss: 0.489263]\n",
      "601 [D loss: 0.740703, acc.: 37.50%] [G loss: 0.577756]\n",
      "602 [D loss: 0.821635, acc.: 43.75%] [G loss: 0.680765]\n",
      "603 [D loss: 0.683611, acc.: 59.38%] [G loss: 0.895083]\n",
      "604 [D loss: 0.664655, acc.: 65.62%] [G loss: 0.964255]\n",
      "605 [D loss: 0.655577, acc.: 65.62%] [G loss: 0.871146]\n",
      "606 [D loss: 0.618092, acc.: 65.62%] [G loss: 0.842987]\n",
      "607 [D loss: 0.501506, acc.: 71.88%] [G loss: 0.775239]\n",
      "608 [D loss: 0.331259, acc.: 90.62%] [G loss: 0.629118]\n",
      "609 [D loss: 0.361759, acc.: 87.50%] [G loss: 0.527454]\n",
      "610 [D loss: 0.226856, acc.: 100.00%] [G loss: 0.346764]\n",
      "611 [D loss: 0.363422, acc.: 84.38%] [G loss: 0.368724]\n",
      "612 [D loss: 0.352605, acc.: 87.50%] [G loss: 0.333517]\n",
      "613 [D loss: 0.698492, acc.: 56.25%] [G loss: 0.405121]\n",
      "614 [D loss: 0.531398, acc.: 65.62%] [G loss: 0.534558]\n",
      "615 [D loss: 0.701706, acc.: 56.25%] [G loss: 0.630589]\n",
      "616 [D loss: 0.658010, acc.: 65.62%] [G loss: 0.612044]\n",
      "617 [D loss: 0.571381, acc.: 68.75%] [G loss: 0.759601]\n",
      "618 [D loss: 0.555233, acc.: 68.75%] [G loss: 1.008034]\n",
      "619 [D loss: 0.601254, acc.: 78.12%] [G loss: 0.869740]\n",
      "620 [D loss: 0.531246, acc.: 65.62%] [G loss: 0.834082]\n",
      "621 [D loss: 0.490879, acc.: 81.25%] [G loss: 0.719772]\n",
      "622 [D loss: 0.518944, acc.: 65.62%] [G loss: 0.590722]\n",
      "623 [D loss: 0.448124, acc.: 81.25%] [G loss: 0.505242]\n",
      "624 [D loss: 0.513178, acc.: 81.25%] [G loss: 0.525971]\n",
      "625 [D loss: 0.463799, acc.: 81.25%] [G loss: 0.459851]\n",
      "626 [D loss: 0.643409, acc.: 65.62%] [G loss: 0.456439]\n",
      "627 [D loss: 0.917843, acc.: 40.62%] [G loss: 0.500900]\n",
      "628 [D loss: 0.716341, acc.: 50.00%] [G loss: 0.565577]\n",
      "629 [D loss: 0.785112, acc.: 43.75%] [G loss: 0.740137]\n",
      "630 [D loss: 0.578640, acc.: 71.88%] [G loss: 0.811519]\n",
      "631 [D loss: 0.672052, acc.: 50.00%] [G loss: 0.657382]\n",
      "632 [D loss: 0.588435, acc.: 68.75%] [G loss: 0.704981]\n",
      "633 [D loss: 0.492345, acc.: 81.25%] [G loss: 0.683782]\n",
      "634 [D loss: 0.493583, acc.: 75.00%] [G loss: 0.600959]\n",
      "635 [D loss: 0.527546, acc.: 71.88%] [G loss: 0.590359]\n",
      "636 [D loss: 0.580548, acc.: 68.75%] [G loss: 0.580049]\n",
      "637 [D loss: 0.704783, acc.: 62.50%] [G loss: 0.506480]\n",
      "638 [D loss: 0.742723, acc.: 53.12%] [G loss: 0.534295]\n",
      "639 [D loss: 0.575309, acc.: 65.62%] [G loss: 0.622743]\n",
      "640 [D loss: 0.653671, acc.: 62.50%] [G loss: 0.765719]\n",
      "641 [D loss: 0.615709, acc.: 68.75%] [G loss: 0.754072]\n",
      "642 [D loss: 0.596748, acc.: 75.00%] [G loss: 0.868657]\n",
      "643 [D loss: 0.605931, acc.: 71.88%] [G loss: 0.757618]\n",
      "644 [D loss: 0.624636, acc.: 65.62%] [G loss: 0.756984]\n",
      "645 [D loss: 0.682245, acc.: 71.88%] [G loss: 0.753578]\n",
      "646 [D loss: 0.643230, acc.: 62.50%] [G loss: 0.733772]\n",
      "647 [D loss: 0.713988, acc.: 56.25%] [G loss: 0.756065]\n",
      "648 [D loss: 0.493187, acc.: 81.25%] [G loss: 0.733266]\n",
      "649 [D loss: 0.505058, acc.: 84.38%] [G loss: 0.811870]\n",
      "650 [D loss: 0.709652, acc.: 62.50%] [G loss: 0.691218]\n",
      "651 [D loss: 0.594438, acc.: 68.75%] [G loss: 0.711530]\n",
      "652 [D loss: 0.680347, acc.: 62.50%] [G loss: 0.777242]\n",
      "653 [D loss: 0.589164, acc.: 75.00%] [G loss: 0.817861]\n",
      "654 [D loss: 0.538459, acc.: 81.25%] [G loss: 0.757065]\n",
      "655 [D loss: 0.676784, acc.: 56.25%] [G loss: 0.675177]\n",
      "656 [D loss: 0.689263, acc.: 59.38%] [G loss: 0.745795]\n",
      "657 [D loss: 0.621664, acc.: 59.38%] [G loss: 0.757526]\n",
      "658 [D loss: 0.678346, acc.: 59.38%] [G loss: 0.786901]\n",
      "659 [D loss: 0.598705, acc.: 68.75%] [G loss: 0.812221]\n",
      "660 [D loss: 0.763660, acc.: 40.62%] [G loss: 0.823051]\n",
      "661 [D loss: 0.685900, acc.: 59.38%] [G loss: 0.728125]\n",
      "662 [D loss: 0.571734, acc.: 78.12%] [G loss: 0.794684]\n",
      "663 [D loss: 0.611232, acc.: 68.75%] [G loss: 0.779917]\n",
      "664 [D loss: 0.640393, acc.: 62.50%] [G loss: 0.836669]\n",
      "665 [D loss: 0.589032, acc.: 68.75%] [G loss: 0.777556]\n",
      "666 [D loss: 0.691608, acc.: 50.00%] [G loss: 0.696037]\n",
      "667 [D loss: 0.545472, acc.: 78.12%] [G loss: 0.646862]\n",
      "668 [D loss: 0.652933, acc.: 62.50%] [G loss: 0.663742]\n",
      "669 [D loss: 0.532035, acc.: 84.38%] [G loss: 0.758260]\n",
      "670 [D loss: 0.676736, acc.: 59.38%] [G loss: 0.666900]\n",
      "671 [D loss: 0.710165, acc.: 56.25%] [G loss: 0.655432]\n",
      "672 [D loss: 0.682991, acc.: 62.50%] [G loss: 0.680800]\n",
      "673 [D loss: 0.667939, acc.: 56.25%] [G loss: 0.736300]\n",
      "674 [D loss: 0.593715, acc.: 78.12%] [G loss: 0.826520]\n",
      "675 [D loss: 0.694803, acc.: 59.38%] [G loss: 0.820129]\n",
      "676 [D loss: 0.689728, acc.: 53.12%] [G loss: 0.769346]\n",
      "677 [D loss: 0.657417, acc.: 62.50%] [G loss: 0.670084]\n",
      "678 [D loss: 0.690215, acc.: 62.50%] [G loss: 0.769453]\n",
      "679 [D loss: 0.546450, acc.: 75.00%] [G loss: 0.705273]\n",
      "680 [D loss: 0.647126, acc.: 65.62%] [G loss: 0.749525]\n",
      "681 [D loss: 0.537151, acc.: 75.00%] [G loss: 0.771188]\n",
      "682 [D loss: 0.548041, acc.: 65.62%] [G loss: 0.700945]\n",
      "683 [D loss: 0.548154, acc.: 71.88%] [G loss: 0.719756]\n",
      "684 [D loss: 0.741013, acc.: 53.12%] [G loss: 0.774389]\n",
      "685 [D loss: 0.752715, acc.: 43.75%] [G loss: 0.672102]\n",
      "686 [D loss: 0.732562, acc.: 46.88%] [G loss: 0.754723]\n",
      "687 [D loss: 0.741917, acc.: 53.12%] [G loss: 0.877171]\n",
      "688 [D loss: 0.585893, acc.: 68.75%] [G loss: 0.904035]\n",
      "689 [D loss: 0.599888, acc.: 65.62%] [G loss: 0.959796]\n",
      "690 [D loss: 0.647246, acc.: 56.25%] [G loss: 0.891654]\n",
      "691 [D loss: 0.683782, acc.: 53.12%] [G loss: 0.878395]\n",
      "692 [D loss: 0.610982, acc.: 62.50%] [G loss: 0.780263]\n",
      "693 [D loss: 0.515066, acc.: 87.50%] [G loss: 0.712112]\n",
      "694 [D loss: 0.561848, acc.: 68.75%] [G loss: 0.681542]\n",
      "695 [D loss: 0.626040, acc.: 56.25%] [G loss: 0.681400]\n",
      "696 [D loss: 0.591495, acc.: 75.00%] [G loss: 0.673193]\n",
      "697 [D loss: 0.669503, acc.: 59.38%] [G loss: 0.775715]\n",
      "698 [D loss: 0.579430, acc.: 68.75%] [G loss: 0.767283]\n",
      "699 [D loss: 0.620833, acc.: 68.75%] [G loss: 0.822657]\n",
      "700 [D loss: 0.621800, acc.: 62.50%] [G loss: 0.871327]\n",
      "701 [D loss: 0.651875, acc.: 65.62%] [G loss: 0.863832]\n",
      "702 [D loss: 0.590069, acc.: 68.75%] [G loss: 0.843797]\n",
      "703 [D loss: 0.592258, acc.: 71.88%] [G loss: 0.813509]\n",
      "704 [D loss: 0.628473, acc.: 75.00%] [G loss: 0.761093]\n",
      "705 [D loss: 0.609833, acc.: 71.88%] [G loss: 0.703391]\n",
      "706 [D loss: 0.444440, acc.: 84.38%] [G loss: 0.756066]\n",
      "707 [D loss: 0.615682, acc.: 68.75%] [G loss: 0.820608]\n",
      "708 [D loss: 0.575998, acc.: 71.88%] [G loss: 0.703984]\n",
      "709 [D loss: 0.667749, acc.: 62.50%] [G loss: 0.657505]\n",
      "710 [D loss: 0.720662, acc.: 53.12%] [G loss: 0.685119]\n",
      "711 [D loss: 0.691045, acc.: 62.50%] [G loss: 0.671808]\n",
      "712 [D loss: 0.736681, acc.: 59.38%] [G loss: 0.602541]\n",
      "713 [D loss: 0.621246, acc.: 59.38%] [G loss: 0.743288]\n",
      "714 [D loss: 0.713668, acc.: 59.38%] [G loss: 0.724044]\n",
      "715 [D loss: 0.653381, acc.: 65.62%] [G loss: 0.729572]\n",
      "716 [D loss: 0.585043, acc.: 78.12%] [G loss: 0.698811]\n",
      "717 [D loss: 0.732295, acc.: 43.75%] [G loss: 0.661618]\n",
      "718 [D loss: 0.614463, acc.: 65.62%] [G loss: 0.626604]\n",
      "719 [D loss: 0.726853, acc.: 43.75%] [G loss: 0.561398]\n",
      "720 [D loss: 0.643904, acc.: 59.38%] [G loss: 0.548308]\n",
      "721 [D loss: 0.589350, acc.: 65.62%] [G loss: 0.656259]\n",
      "722 [D loss: 0.685623, acc.: 50.00%] [G loss: 0.571951]\n",
      "723 [D loss: 0.719335, acc.: 46.88%] [G loss: 0.604937]\n",
      "724 [D loss: 0.715023, acc.: 43.75%] [G loss: 0.666782]\n",
      "725 [D loss: 0.684667, acc.: 56.25%] [G loss: 0.704946]\n",
      "726 [D loss: 0.591064, acc.: 68.75%] [G loss: 0.741200]\n",
      "727 [D loss: 0.665532, acc.: 62.50%] [G loss: 0.792321]\n",
      "728 [D loss: 0.583359, acc.: 75.00%] [G loss: 0.799744]\n",
      "729 [D loss: 0.672819, acc.: 53.12%] [G loss: 0.758290]\n",
      "730 [D loss: 0.632620, acc.: 62.50%] [G loss: 0.734205]\n",
      "731 [D loss: 0.711246, acc.: 53.12%] [G loss: 0.767965]\n",
      "732 [D loss: 0.625639, acc.: 68.75%] [G loss: 0.744076]\n",
      "733 [D loss: 0.661931, acc.: 65.62%] [G loss: 0.772227]\n",
      "734 [D loss: 0.676246, acc.: 56.25%] [G loss: 0.803616]\n",
      "735 [D loss: 0.727855, acc.: 40.62%] [G loss: 0.784503]\n",
      "736 [D loss: 0.591319, acc.: 65.62%] [G loss: 0.801529]\n",
      "737 [D loss: 0.555499, acc.: 81.25%] [G loss: 0.784578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 0.587827, acc.: 78.12%] [G loss: 0.757252]\n",
      "739 [D loss: 0.510378, acc.: 81.25%] [G loss: 0.778481]\n",
      "740 [D loss: 0.604578, acc.: 71.88%] [G loss: 0.742220]\n",
      "741 [D loss: 0.564570, acc.: 75.00%] [G loss: 0.773871]\n",
      "742 [D loss: 0.524164, acc.: 84.38%] [G loss: 0.797483]\n",
      "743 [D loss: 0.626596, acc.: 65.62%] [G loss: 0.839729]\n",
      "744 [D loss: 0.639117, acc.: 59.38%] [G loss: 0.789906]\n",
      "745 [D loss: 0.534216, acc.: 75.00%] [G loss: 0.812982]\n",
      "746 [D loss: 0.563732, acc.: 62.50%] [G loss: 0.797398]\n",
      "747 [D loss: 0.664294, acc.: 59.38%] [G loss: 0.771454]\n",
      "748 [D loss: 0.595396, acc.: 71.88%] [G loss: 0.690536]\n",
      "749 [D loss: 0.617203, acc.: 68.75%] [G loss: 0.703737]\n",
      "750 [D loss: 0.695555, acc.: 65.62%] [G loss: 0.747285]\n",
      "751 [D loss: 0.746182, acc.: 50.00%] [G loss: 0.726678]\n",
      "752 [D loss: 0.584967, acc.: 71.88%] [G loss: 0.761830]\n",
      "753 [D loss: 0.605675, acc.: 71.88%] [G loss: 0.912273]\n",
      "754 [D loss: 0.611235, acc.: 65.62%] [G loss: 0.902084]\n",
      "755 [D loss: 0.573542, acc.: 78.12%] [G loss: 0.706289]\n",
      "756 [D loss: 0.617267, acc.: 65.62%] [G loss: 0.689558]\n",
      "757 [D loss: 0.513460, acc.: 78.12%] [G loss: 0.676599]\n",
      "758 [D loss: 0.486409, acc.: 71.88%] [G loss: 0.600200]\n",
      "759 [D loss: 0.621033, acc.: 62.50%] [G loss: 0.585616]\n",
      "760 [D loss: 0.532302, acc.: 68.75%] [G loss: 0.458976]\n",
      "761 [D loss: 0.621894, acc.: 59.38%] [G loss: 0.479393]\n",
      "762 [D loss: 0.590984, acc.: 68.75%] [G loss: 0.619361]\n",
      "763 [D loss: 0.650943, acc.: 65.62%] [G loss: 0.688327]\n",
      "764 [D loss: 0.486157, acc.: 81.25%] [G loss: 0.852840]\n",
      "765 [D loss: 0.519998, acc.: 78.12%] [G loss: 0.880082]\n",
      "766 [D loss: 0.654273, acc.: 65.62%] [G loss: 0.820382]\n",
      "767 [D loss: 0.625524, acc.: 71.88%] [G loss: 0.641014]\n",
      "768 [D loss: 0.438388, acc.: 87.50%] [G loss: 0.454953]\n",
      "769 [D loss: 0.447689, acc.: 87.50%] [G loss: 0.536672]\n",
      "770 [D loss: 0.492954, acc.: 78.12%] [G loss: 0.404345]\n",
      "771 [D loss: 0.477921, acc.: 75.00%] [G loss: 0.401975]\n",
      "772 [D loss: 0.671975, acc.: 68.75%] [G loss: 0.377888]\n",
      "773 [D loss: 0.659131, acc.: 68.75%] [G loss: 0.474944]\n",
      "774 [D loss: 0.603459, acc.: 62.50%] [G loss: 0.616267]\n",
      "775 [D loss: 0.623176, acc.: 71.88%] [G loss: 0.792134]\n",
      "776 [D loss: 0.508127, acc.: 78.12%] [G loss: 0.853587]\n",
      "777 [D loss: 0.552023, acc.: 75.00%] [G loss: 0.786016]\n",
      "778 [D loss: 0.669736, acc.: 65.62%] [G loss: 0.754687]\n",
      "779 [D loss: 0.584217, acc.: 68.75%] [G loss: 0.674715]\n",
      "780 [D loss: 0.575020, acc.: 71.88%] [G loss: 0.589442]\n",
      "781 [D loss: 0.388063, acc.: 93.75%] [G loss: 0.449004]\n",
      "782 [D loss: 0.528169, acc.: 78.12%] [G loss: 0.426936]\n",
      "783 [D loss: 0.598372, acc.: 62.50%] [G loss: 0.439128]\n",
      "784 [D loss: 0.602915, acc.: 68.75%] [G loss: 0.449742]\n",
      "785 [D loss: 0.665824, acc.: 62.50%] [G loss: 0.679401]\n",
      "786 [D loss: 0.729975, acc.: 56.25%] [G loss: 0.743793]\n",
      "787 [D loss: 0.585524, acc.: 65.62%] [G loss: 0.959261]\n",
      "788 [D loss: 0.577875, acc.: 68.75%] [G loss: 1.047161]\n",
      "789 [D loss: 0.550082, acc.: 71.88%] [G loss: 0.904257]\n",
      "790 [D loss: 0.483129, acc.: 81.25%] [G loss: 0.766429]\n",
      "791 [D loss: 0.489825, acc.: 78.12%] [G loss: 0.601470]\n",
      "792 [D loss: 0.406889, acc.: 93.75%] [G loss: 0.558782]\n",
      "793 [D loss: 0.443407, acc.: 87.50%] [G loss: 0.501903]\n",
      "794 [D loss: 0.627979, acc.: 65.62%] [G loss: 0.469350]\n",
      "795 [D loss: 0.528649, acc.: 75.00%] [G loss: 0.533823]\n",
      "796 [D loss: 0.825843, acc.: 46.88%] [G loss: 0.658076]\n",
      "797 [D loss: 0.599858, acc.: 62.50%] [G loss: 0.850209]\n",
      "798 [D loss: 0.540250, acc.: 81.25%] [G loss: 0.951728]\n",
      "799 [D loss: 0.629937, acc.: 62.50%] [G loss: 0.776376]\n",
      "800 [D loss: 0.555823, acc.: 78.12%] [G loss: 0.899528]\n",
      "801 [D loss: 0.603242, acc.: 65.62%] [G loss: 0.825112]\n",
      "802 [D loss: 0.507427, acc.: 68.75%] [G loss: 0.744943]\n",
      "803 [D loss: 0.379562, acc.: 96.88%] [G loss: 0.552329]\n",
      "804 [D loss: 0.530534, acc.: 78.12%] [G loss: 0.519077]\n",
      "805 [D loss: 0.621040, acc.: 71.88%] [G loss: 0.508298]\n",
      "806 [D loss: 0.606167, acc.: 71.88%] [G loss: 0.694943]\n",
      "807 [D loss: 0.619432, acc.: 62.50%] [G loss: 0.811044]\n",
      "808 [D loss: 0.565034, acc.: 68.75%] [G loss: 1.013153]\n",
      "809 [D loss: 0.517516, acc.: 78.12%] [G loss: 0.998644]\n",
      "810 [D loss: 0.614142, acc.: 65.62%] [G loss: 0.949181]\n",
      "811 [D loss: 0.684148, acc.: 56.25%] [G loss: 1.041634]\n",
      "812 [D loss: 0.604517, acc.: 75.00%] [G loss: 0.874247]\n",
      "813 [D loss: 0.544898, acc.: 65.62%] [G loss: 0.717384]\n",
      "814 [D loss: 0.511014, acc.: 71.88%] [G loss: 0.752736]\n",
      "815 [D loss: 0.532938, acc.: 71.88%] [G loss: 0.715233]\n",
      "816 [D loss: 0.466492, acc.: 84.38%] [G loss: 0.787626]\n",
      "817 [D loss: 0.593114, acc.: 81.25%] [G loss: 0.753787]\n",
      "818 [D loss: 0.535312, acc.: 75.00%] [G loss: 0.866888]\n",
      "819 [D loss: 0.554442, acc.: 65.62%] [G loss: 0.932068]\n",
      "820 [D loss: 0.558276, acc.: 68.75%] [G loss: 0.850067]\n",
      "821 [D loss: 0.591252, acc.: 71.88%] [G loss: 0.883415]\n",
      "822 [D loss: 0.593275, acc.: 62.50%] [G loss: 0.928274]\n",
      "823 [D loss: 0.548552, acc.: 78.12%] [G loss: 0.899565]\n",
      "824 [D loss: 0.463871, acc.: 87.50%] [G loss: 0.750739]\n",
      "825 [D loss: 0.428415, acc.: 84.38%] [G loss: 0.770316]\n",
      "826 [D loss: 0.592199, acc.: 75.00%] [G loss: 0.612396]\n",
      "827 [D loss: 0.683435, acc.: 62.50%] [G loss: 0.608024]\n",
      "828 [D loss: 0.681940, acc.: 56.25%] [G loss: 0.955347]\n",
      "829 [D loss: 0.579761, acc.: 68.75%] [G loss: 0.974566]\n",
      "830 [D loss: 0.488554, acc.: 81.25%] [G loss: 0.967099]\n",
      "831 [D loss: 0.632781, acc.: 68.75%] [G loss: 0.820713]\n",
      "832 [D loss: 0.570135, acc.: 68.75%] [G loss: 0.817102]\n",
      "833 [D loss: 0.590643, acc.: 65.62%] [G loss: 0.920586]\n",
      "834 [D loss: 0.517702, acc.: 78.12%] [G loss: 0.728056]\n",
      "835 [D loss: 0.622270, acc.: 81.25%] [G loss: 0.798132]\n",
      "836 [D loss: 0.665886, acc.: 59.38%] [G loss: 0.813659]\n",
      "837 [D loss: 0.604027, acc.: 59.38%] [G loss: 0.910098]\n",
      "838 [D loss: 0.513657, acc.: 71.88%] [G loss: 0.905519]\n",
      "839 [D loss: 0.411709, acc.: 93.75%] [G loss: 0.945428]\n",
      "840 [D loss: 0.594672, acc.: 75.00%] [G loss: 0.778251]\n",
      "841 [D loss: 0.619605, acc.: 75.00%] [G loss: 0.724068]\n",
      "842 [D loss: 0.607046, acc.: 59.38%] [G loss: 0.945828]\n",
      "843 [D loss: 0.631399, acc.: 62.50%] [G loss: 0.917350]\n",
      "844 [D loss: 0.737096, acc.: 46.88%] [G loss: 0.862097]\n",
      "845 [D loss: 0.674027, acc.: 65.62%] [G loss: 0.841707]\n",
      "846 [D loss: 0.536226, acc.: 71.88%] [G loss: 0.898283]\n",
      "847 [D loss: 0.647552, acc.: 62.50%] [G loss: 0.855048]\n",
      "848 [D loss: 0.825869, acc.: 50.00%] [G loss: 0.744870]\n",
      "849 [D loss: 0.569425, acc.: 68.75%] [G loss: 0.808367]\n",
      "850 [D loss: 0.595248, acc.: 71.88%] [G loss: 0.799600]\n",
      "851 [D loss: 0.592438, acc.: 65.62%] [G loss: 0.896958]\n",
      "852 [D loss: 0.595168, acc.: 68.75%] [G loss: 0.736311]\n",
      "853 [D loss: 0.509570, acc.: 78.12%] [G loss: 0.637027]\n",
      "854 [D loss: 0.502975, acc.: 78.12%] [G loss: 0.743262]\n",
      "855 [D loss: 0.754227, acc.: 65.62%] [G loss: 0.711931]\n",
      "856 [D loss: 0.489947, acc.: 84.38%] [G loss: 0.710769]\n",
      "857 [D loss: 0.538547, acc.: 68.75%] [G loss: 0.782915]\n",
      "858 [D loss: 0.639426, acc.: 62.50%] [G loss: 0.803189]\n",
      "859 [D loss: 0.609705, acc.: 71.88%] [G loss: 0.796278]\n",
      "860 [D loss: 0.725240, acc.: 56.25%] [G loss: 0.920260]\n",
      "861 [D loss: 0.520898, acc.: 78.12%] [G loss: 1.029387]\n",
      "862 [D loss: 0.574585, acc.: 75.00%] [G loss: 0.822091]\n",
      "863 [D loss: 0.599323, acc.: 75.00%] [G loss: 0.838230]\n",
      "864 [D loss: 0.595385, acc.: 68.75%] [G loss: 0.825227]\n",
      "865 [D loss: 0.638455, acc.: 65.62%] [G loss: 0.823973]\n",
      "866 [D loss: 0.594781, acc.: 71.88%] [G loss: 0.778920]\n",
      "867 [D loss: 0.608519, acc.: 62.50%] [G loss: 0.848744]\n",
      "868 [D loss: 0.580735, acc.: 71.88%] [G loss: 0.799160]\n",
      "869 [D loss: 0.812132, acc.: 43.75%] [G loss: 0.706444]\n",
      "870 [D loss: 0.740367, acc.: 43.75%] [G loss: 0.720733]\n",
      "871 [D loss: 0.731781, acc.: 50.00%] [G loss: 0.683733]\n",
      "872 [D loss: 0.745494, acc.: 50.00%] [G loss: 0.731769]\n",
      "873 [D loss: 0.640221, acc.: 65.62%] [G loss: 0.844058]\n",
      "874 [D loss: 0.604852, acc.: 71.88%] [G loss: 0.839484]\n",
      "875 [D loss: 0.548521, acc.: 78.12%] [G loss: 0.917671]\n",
      "876 [D loss: 0.627533, acc.: 62.50%] [G loss: 0.856039]\n",
      "877 [D loss: 0.686057, acc.: 56.25%] [G loss: 0.861178]\n",
      "878 [D loss: 0.576198, acc.: 65.62%] [G loss: 0.820695]\n",
      "879 [D loss: 0.690012, acc.: 56.25%] [G loss: 0.836393]\n",
      "880 [D loss: 0.577455, acc.: 68.75%] [G loss: 0.889527]\n",
      "881 [D loss: 0.656982, acc.: 68.75%] [G loss: 0.992503]\n",
      "882 [D loss: 0.552294, acc.: 65.62%] [G loss: 0.920932]\n",
      "883 [D loss: 0.556087, acc.: 75.00%] [G loss: 0.980120]\n",
      "884 [D loss: 0.626310, acc.: 71.88%] [G loss: 0.944957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.637986, acc.: 59.38%] [G loss: 0.835599]\n",
      "886 [D loss: 0.580726, acc.: 78.12%] [G loss: 0.966500]\n",
      "887 [D loss: 0.654777, acc.: 53.12%] [G loss: 1.036336]\n",
      "888 [D loss: 0.634794, acc.: 68.75%] [G loss: 0.972688]\n",
      "889 [D loss: 0.669850, acc.: 56.25%] [G loss: 0.841989]\n",
      "890 [D loss: 0.625755, acc.: 68.75%] [G loss: 0.819389]\n",
      "891 [D loss: 0.577399, acc.: 71.88%] [G loss: 0.780542]\n",
      "892 [D loss: 0.598433, acc.: 75.00%] [G loss: 0.777673]\n",
      "893 [D loss: 0.583108, acc.: 71.88%] [G loss: 0.876898]\n",
      "894 [D loss: 0.678227, acc.: 50.00%] [G loss: 0.827656]\n",
      "895 [D loss: 0.650488, acc.: 62.50%] [G loss: 0.873708]\n",
      "896 [D loss: 0.607754, acc.: 65.62%] [G loss: 0.932703]\n",
      "897 [D loss: 0.726486, acc.: 59.38%] [G loss: 0.987862]\n",
      "898 [D loss: 0.767125, acc.: 56.25%] [G loss: 0.911525]\n",
      "899 [D loss: 0.645815, acc.: 56.25%] [G loss: 0.984620]\n",
      "900 [D loss: 0.751375, acc.: 46.88%] [G loss: 0.899482]\n",
      "901 [D loss: 0.754176, acc.: 53.12%] [G loss: 0.935513]\n",
      "902 [D loss: 0.567285, acc.: 68.75%] [G loss: 0.931821]\n",
      "903 [D loss: 0.593964, acc.: 68.75%] [G loss: 0.951327]\n",
      "904 [D loss: 0.606641, acc.: 65.62%] [G loss: 0.855091]\n",
      "905 [D loss: 0.642007, acc.: 68.75%] [G loss: 0.752076]\n",
      "906 [D loss: 0.618408, acc.: 56.25%] [G loss: 0.765136]\n",
      "907 [D loss: 0.646156, acc.: 62.50%] [G loss: 0.767205]\n",
      "908 [D loss: 0.634469, acc.: 65.62%] [G loss: 0.776735]\n",
      "909 [D loss: 0.699646, acc.: 43.75%] [G loss: 0.886991]\n",
      "910 [D loss: 0.704966, acc.: 65.62%] [G loss: 0.755058]\n",
      "911 [D loss: 0.802464, acc.: 40.62%] [G loss: 0.791159]\n",
      "912 [D loss: 0.719500, acc.: 56.25%] [G loss: 0.796398]\n",
      "913 [D loss: 0.599288, acc.: 71.88%] [G loss: 0.788877]\n",
      "914 [D loss: 0.668058, acc.: 59.38%] [G loss: 0.719161]\n",
      "915 [D loss: 0.655317, acc.: 65.62%] [G loss: 0.777567]\n",
      "916 [D loss: 0.629331, acc.: 62.50%] [G loss: 0.794817]\n",
      "917 [D loss: 0.574911, acc.: 78.12%] [G loss: 0.750970]\n",
      "918 [D loss: 0.738432, acc.: 43.75%] [G loss: 0.696940]\n",
      "919 [D loss: 0.525370, acc.: 71.88%] [G loss: 0.758528]\n",
      "920 [D loss: 0.560072, acc.: 78.12%] [G loss: 0.683417]\n",
      "921 [D loss: 0.614878, acc.: 65.62%] [G loss: 0.720517]\n",
      "922 [D loss: 0.643285, acc.: 62.50%] [G loss: 0.654126]\n",
      "923 [D loss: 0.657526, acc.: 59.38%] [G loss: 0.770286]\n",
      "924 [D loss: 0.691522, acc.: 50.00%] [G loss: 0.825466]\n",
      "925 [D loss: 0.704742, acc.: 50.00%] [G loss: 0.741544]\n",
      "926 [D loss: 0.682192, acc.: 59.38%] [G loss: 0.752058]\n",
      "927 [D loss: 0.561914, acc.: 68.75%] [G loss: 0.733391]\n",
      "928 [D loss: 0.650039, acc.: 65.62%] [G loss: 0.735714]\n",
      "929 [D loss: 0.635158, acc.: 65.62%] [G loss: 0.714798]\n",
      "930 [D loss: 0.518986, acc.: 84.38%] [G loss: 0.689446]\n",
      "931 [D loss: 0.604482, acc.: 62.50%] [G loss: 0.657841]\n",
      "932 [D loss: 0.609123, acc.: 65.62%] [G loss: 0.714648]\n",
      "933 [D loss: 0.678576, acc.: 50.00%] [G loss: 0.690137]\n",
      "934 [D loss: 0.612332, acc.: 62.50%] [G loss: 0.675866]\n",
      "935 [D loss: 0.541832, acc.: 78.12%] [G loss: 0.791171]\n",
      "936 [D loss: 0.596525, acc.: 75.00%] [G loss: 0.714424]\n",
      "937 [D loss: 0.609639, acc.: 68.75%] [G loss: 0.789415]\n",
      "938 [D loss: 0.674107, acc.: 65.62%] [G loss: 0.787815]\n",
      "939 [D loss: 0.674315, acc.: 56.25%] [G loss: 0.829163]\n",
      "940 [D loss: 0.665461, acc.: 68.75%] [G loss: 0.775602]\n",
      "941 [D loss: 0.605375, acc.: 59.38%] [G loss: 0.758249]\n",
      "942 [D loss: 0.587818, acc.: 75.00%] [G loss: 0.798009]\n",
      "943 [D loss: 0.754997, acc.: 56.25%] [G loss: 0.831297]\n",
      "944 [D loss: 0.584288, acc.: 71.88%] [G loss: 0.798918]\n",
      "945 [D loss: 0.689044, acc.: 46.88%] [G loss: 0.685738]\n",
      "946 [D loss: 0.642109, acc.: 62.50%] [G loss: 0.765159]\n",
      "947 [D loss: 0.658162, acc.: 56.25%] [G loss: 0.706368]\n",
      "948 [D loss: 0.576892, acc.: 71.88%] [G loss: 0.755665]\n",
      "949 [D loss: 0.448879, acc.: 90.62%] [G loss: 0.782199]\n",
      "950 [D loss: 0.654644, acc.: 56.25%] [G loss: 0.811310]\n",
      "951 [D loss: 0.716559, acc.: 56.25%] [G loss: 0.772409]\n",
      "952 [D loss: 0.598858, acc.: 65.62%] [G loss: 0.904669]\n",
      "953 [D loss: 0.686395, acc.: 56.25%] [G loss: 0.992954]\n",
      "954 [D loss: 0.660687, acc.: 56.25%] [G loss: 0.846644]\n",
      "955 [D loss: 0.681505, acc.: 65.62%] [G loss: 1.015111]\n",
      "956 [D loss: 0.566828, acc.: 71.88%] [G loss: 0.969353]\n",
      "957 [D loss: 0.570345, acc.: 68.75%] [G loss: 0.970416]\n",
      "958 [D loss: 0.601902, acc.: 68.75%] [G loss: 0.850809]\n",
      "959 [D loss: 0.626343, acc.: 65.62%] [G loss: 0.860296]\n",
      "960 [D loss: 0.660946, acc.: 65.62%] [G loss: 0.966513]\n",
      "961 [D loss: 0.656402, acc.: 59.38%] [G loss: 0.848275]\n",
      "962 [D loss: 0.707708, acc.: 56.25%] [G loss: 0.795111]\n",
      "963 [D loss: 0.786671, acc.: 46.88%] [G loss: 0.893675]\n",
      "964 [D loss: 0.647750, acc.: 65.62%] [G loss: 0.870578]\n",
      "965 [D loss: 0.640703, acc.: 65.62%] [G loss: 0.874691]\n",
      "966 [D loss: 0.576112, acc.: 78.12%] [G loss: 0.878456]\n",
      "967 [D loss: 0.654006, acc.: 59.38%] [G loss: 0.912337]\n",
      "968 [D loss: 0.635276, acc.: 56.25%] [G loss: 0.847255]\n",
      "969 [D loss: 0.597911, acc.: 65.62%] [G loss: 0.949011]\n",
      "970 [D loss: 0.631083, acc.: 56.25%] [G loss: 1.043290]\n",
      "971 [D loss: 0.573467, acc.: 71.88%] [G loss: 0.834718]\n",
      "972 [D loss: 0.642321, acc.: 62.50%] [G loss: 0.830662]\n",
      "973 [D loss: 0.620467, acc.: 65.62%] [G loss: 0.758718]\n",
      "974 [D loss: 0.610382, acc.: 65.62%] [G loss: 0.709805]\n",
      "975 [D loss: 0.586609, acc.: 62.50%] [G loss: 0.741981]\n",
      "976 [D loss: 0.599633, acc.: 71.88%] [G loss: 0.846468]\n",
      "977 [D loss: 0.488028, acc.: 81.25%] [G loss: 0.837726]\n",
      "978 [D loss: 0.541604, acc.: 78.12%] [G loss: 0.911653]\n",
      "979 [D loss: 0.729236, acc.: 50.00%] [G loss: 0.861296]\n",
      "980 [D loss: 0.672962, acc.: 62.50%] [G loss: 0.861718]\n",
      "981 [D loss: 0.684522, acc.: 53.12%] [G loss: 0.895944]\n",
      "982 [D loss: 0.619763, acc.: 71.88%] [G loss: 1.021731]\n",
      "983 [D loss: 0.684118, acc.: 65.62%] [G loss: 1.015684]\n",
      "984 [D loss: 0.670499, acc.: 68.75%] [G loss: 0.930826]\n",
      "985 [D loss: 0.710530, acc.: 62.50%] [G loss: 0.949479]\n",
      "986 [D loss: 0.704071, acc.: 46.88%] [G loss: 0.810635]\n",
      "987 [D loss: 0.747455, acc.: 53.12%] [G loss: 0.858884]\n",
      "988 [D loss: 0.680120, acc.: 50.00%] [G loss: 0.804987]\n",
      "989 [D loss: 0.529311, acc.: 81.25%] [G loss: 0.978577]\n",
      "990 [D loss: 0.669372, acc.: 53.12%] [G loss: 0.857642]\n",
      "991 [D loss: 0.595053, acc.: 62.50%] [G loss: 0.877985]\n",
      "992 [D loss: 0.718048, acc.: 62.50%] [G loss: 0.775127]\n",
      "993 [D loss: 0.736211, acc.: 53.12%] [G loss: 0.628372]\n",
      "994 [D loss: 0.791667, acc.: 56.25%] [G loss: 0.706984]\n",
      "995 [D loss: 0.701695, acc.: 59.38%] [G loss: 0.840870]\n",
      "996 [D loss: 0.547116, acc.: 75.00%] [G loss: 0.812089]\n",
      "997 [D loss: 0.574766, acc.: 84.38%] [G loss: 0.898865]\n",
      "998 [D loss: 0.695578, acc.: 59.38%] [G loss: 0.900915]\n",
      "999 [D loss: 0.636153, acc.: 59.38%] [G loss: 0.865697]\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Convolution2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        #mnistデータ用の入力データサイズ\n",
    "        self.img_rows = 28 \n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        # 潜在変数の次元数 \n",
    "        self.z_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # discriminatorモデル\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Generatorモデル\n",
    "        self.generator = self.build_generator()\n",
    "        # generatorは単体で学習しないのでコンパイルは必要ない\n",
    "        #self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        self.combined = self.build_combined1()\n",
    "        #self.combined = self.build_combined2()\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_shape = (self.z_dim,)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1024, input_shape=noise_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(128*7*7))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Reshape((7,7,128), input_shape=(128*7*7,)))\n",
    "        model.add(UpSampling2D((2,2)))\n",
    "        model.add(Convolution2D(64,5,5,border_mode='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(UpSampling2D((2,2)))\n",
    "        model.add(Convolution2D(1,5,5,border_mode='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(64,5,5, subsample=(2,2),\\\n",
    "                  border_mode='same', input_shape=img_shape))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Convolution2D(128,5,5,subsample=(2,2)))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))   \n",
    "        return model\n",
    "    \n",
    "    def build_combined1(self):\n",
    "        self.discriminator.trainable = False\n",
    "        model = Sequential([self.generator, self.discriminator])\n",
    "        return model\n",
    "\n",
    "    def build_combined2(self):\n",
    "        z = Input(shape=(self.z_dim,))\n",
    "        img = self.generator(z)\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator(img)\n",
    "        model = Model(z, valid)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # mnistデータの読み込み\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # 値を-1 to 1に規格化\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Discriminatorの学習\n",
    "            # ---------------------\n",
    "\n",
    "            # バッチサイズの半数をGeneratorから生成\n",
    "            noise = np.random.normal(0, 1, (half_batch, self.z_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "\n",
    "            # バッチサイズの半数を教師データからピックアップ\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # discriminatorを学習\n",
    "            # 本物データと偽物データは別々に学習させる\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            # それぞれの損失関数を平均\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Generatorの学習\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "\n",
    "            # 生成データの正解ラベルは本物（1） \n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # 進捗の表示\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # 指定した間隔で生成画像を保存\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        # 生成画像を敷き詰めるときの行数、列数\n",
    "        r, c = 5, 5\n",
    "\n",
    "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # 生成画像を0-1に再スケール\n",
    "        gen_imgs = 0.5 * gen_imgs +  0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j]a.xis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/gcgan/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=1000, batch_size=32, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
