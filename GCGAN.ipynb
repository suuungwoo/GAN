{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(28, 28, 1..., strides=(2, 2), padding=\"same\")`\n",
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:72: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), strides=(2, 2))`\n",
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6272)              6428800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 6,768,129\n",
      "Trainable params: 6,753,409\n",
      "Non-trainable params: 14,720\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.679790, acc.: 43.75%] [G loss: 0.602608]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s-kim/anaconda3/envs/keras/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.723139, acc.: 50.00%] [G loss: 0.535291]\n",
      "2 [D loss: 0.749040, acc.: 50.00%] [G loss: 0.588050]\n",
      "3 [D loss: 0.609804, acc.: 50.00%] [G loss: 0.640486]\n",
      "4 [D loss: 0.637524, acc.: 53.12%] [G loss: 0.558427]\n",
      "5 [D loss: 0.475876, acc.: 100.00%] [G loss: 0.452320]\n",
      "6 [D loss: 0.259969, acc.: 100.00%] [G loss: 0.306431]\n",
      "7 [D loss: 0.134273, acc.: 100.00%] [G loss: 0.152187]\n",
      "8 [D loss: 0.704814, acc.: 50.00%] [G loss: 0.140937]\n",
      "9 [D loss: 1.128959, acc.: 50.00%] [G loss: 0.374712]\n",
      "10 [D loss: 0.662062, acc.: 50.00%] [G loss: 0.679359]\n",
      "11 [D loss: 0.732388, acc.: 37.50%] [G loss: 0.642520]\n",
      "12 [D loss: 0.340690, acc.: 96.88%] [G loss: 0.470818]\n",
      "13 [D loss: 0.226770, acc.: 93.75%] [G loss: 0.392800]\n",
      "14 [D loss: 0.291762, acc.: 100.00%] [G loss: 0.274160]\n",
      "15 [D loss: 0.850814, acc.: 46.88%] [G loss: 0.325305]\n",
      "16 [D loss: 0.774884, acc.: 46.88%] [G loss: 0.398667]\n",
      "17 [D loss: 0.765256, acc.: 50.00%] [G loss: 0.438745]\n",
      "18 [D loss: 0.460355, acc.: 90.62%] [G loss: 0.494910]\n",
      "19 [D loss: 0.719689, acc.: 46.88%] [G loss: 0.462573]\n",
      "20 [D loss: 0.658696, acc.: 56.25%] [G loss: 0.530035]\n",
      "21 [D loss: 0.714392, acc.: 50.00%] [G loss: 0.502615]\n",
      "22 [D loss: 0.515362, acc.: 78.12%] [G loss: 0.643650]\n",
      "23 [D loss: 0.607833, acc.: 65.62%] [G loss: 0.526107]\n",
      "24 [D loss: 0.563996, acc.: 90.62%] [G loss: 0.560537]\n",
      "25 [D loss: 0.526613, acc.: 93.75%] [G loss: 0.437358]\n",
      "26 [D loss: 0.745069, acc.: 46.88%] [G loss: 0.534801]\n",
      "27 [D loss: 0.689930, acc.: 62.50%] [G loss: 0.399957]\n",
      "28 [D loss: 0.582129, acc.: 84.38%] [G loss: 0.456064]\n",
      "29 [D loss: 0.704206, acc.: 46.88%] [G loss: 0.434863]\n",
      "30 [D loss: 0.709181, acc.: 50.00%] [G loss: 0.565586]\n",
      "31 [D loss: 0.569485, acc.: 84.38%] [G loss: 0.466616]\n",
      "32 [D loss: 0.476777, acc.: 93.75%] [G loss: 0.460107]\n",
      "33 [D loss: 0.468176, acc.: 90.62%] [G loss: 0.336577]\n",
      "34 [D loss: 0.601679, acc.: 65.62%] [G loss: 0.301856]\n",
      "35 [D loss: 0.719848, acc.: 53.12%] [G loss: 0.359020]\n",
      "36 [D loss: 0.723241, acc.: 43.75%] [G loss: 0.375869]\n",
      "37 [D loss: 0.665602, acc.: 56.25%] [G loss: 0.381281]\n",
      "38 [D loss: 0.693116, acc.: 56.25%] [G loss: 0.356617]\n",
      "39 [D loss: 0.750415, acc.: 43.75%] [G loss: 0.361063]\n",
      "40 [D loss: 0.775205, acc.: 34.38%] [G loss: 0.390740]\n",
      "41 [D loss: 0.811338, acc.: 31.25%] [G loss: 0.504413]\n",
      "42 [D loss: 0.604160, acc.: 71.88%] [G loss: 0.535987]\n",
      "43 [D loss: 0.617014, acc.: 68.75%] [G loss: 0.446177]\n",
      "44 [D loss: 0.611985, acc.: 65.62%] [G loss: 0.389926]\n",
      "45 [D loss: 0.638227, acc.: 65.62%] [G loss: 0.325464]\n",
      "46 [D loss: 0.809437, acc.: 34.38%] [G loss: 0.356991]\n",
      "47 [D loss: 0.912556, acc.: 15.62%] [G loss: 0.417987]\n",
      "48 [D loss: 0.650593, acc.: 50.00%] [G loss: 0.454147]\n",
      "49 [D loss: 0.575026, acc.: 87.50%] [G loss: 0.436343]\n",
      "50 [D loss: 0.607919, acc.: 81.25%] [G loss: 0.386980]\n",
      "51 [D loss: 0.787915, acc.: 37.50%] [G loss: 0.374993]\n",
      "52 [D loss: 0.851613, acc.: 34.38%] [G loss: 0.476561]\n",
      "53 [D loss: 0.767670, acc.: 21.88%] [G loss: 0.509657]\n",
      "54 [D loss: 0.548592, acc.: 84.38%] [G loss: 0.494815]\n",
      "55 [D loss: 0.564854, acc.: 81.25%] [G loss: 0.417466]\n",
      "56 [D loss: 0.567376, acc.: 84.38%] [G loss: 0.356185]\n",
      "57 [D loss: 0.691256, acc.: 56.25%] [G loss: 0.311459]\n",
      "58 [D loss: 0.896347, acc.: 37.50%] [G loss: 0.398387]\n",
      "59 [D loss: 0.735380, acc.: 34.38%] [G loss: 0.467994]\n",
      "60 [D loss: 0.588097, acc.: 78.12%] [G loss: 0.460288]\n",
      "61 [D loss: 0.560379, acc.: 81.25%] [G loss: 0.427102]\n",
      "62 [D loss: 0.664937, acc.: 59.38%] [G loss: 0.389111]\n",
      "63 [D loss: 0.722045, acc.: 43.75%] [G loss: 0.361997]\n",
      "64 [D loss: 0.765175, acc.: 43.75%] [G loss: 0.408232]\n",
      "65 [D loss: 0.766656, acc.: 43.75%] [G loss: 0.443347]\n",
      "66 [D loss: 0.688867, acc.: 53.12%] [G loss: 0.436043]\n",
      "67 [D loss: 0.703313, acc.: 43.75%] [G loss: 0.441553]\n",
      "68 [D loss: 0.679808, acc.: 56.25%] [G loss: 0.421830]\n",
      "69 [D loss: 0.797929, acc.: 28.12%] [G loss: 0.464343]\n",
      "70 [D loss: 0.729341, acc.: 50.00%] [G loss: 0.500542]\n",
      "71 [D loss: 0.710313, acc.: 43.75%] [G loss: 0.515130]\n",
      "72 [D loss: 0.700849, acc.: 65.62%] [G loss: 0.526828]\n",
      "73 [D loss: 0.661425, acc.: 62.50%] [G loss: 0.521117]\n",
      "74 [D loss: 0.648551, acc.: 65.62%] [G loss: 0.516086]\n",
      "75 [D loss: 0.684348, acc.: 62.50%] [G loss: 0.491911]\n",
      "76 [D loss: 0.712740, acc.: 43.75%] [G loss: 0.498535]\n",
      "77 [D loss: 0.708306, acc.: 40.62%] [G loss: 0.524729]\n",
      "78 [D loss: 0.715029, acc.: 50.00%] [G loss: 0.557884]\n",
      "79 [D loss: 0.705484, acc.: 50.00%] [G loss: 0.539294]\n",
      "80 [D loss: 0.688328, acc.: 43.75%] [G loss: 0.555516]\n",
      "81 [D loss: 0.691179, acc.: 56.25%] [G loss: 0.536979]\n",
      "82 [D loss: 0.649613, acc.: 71.88%] [G loss: 0.537294]\n",
      "83 [D loss: 0.647119, acc.: 62.50%] [G loss: 0.494498]\n",
      "84 [D loss: 0.704879, acc.: 34.38%] [G loss: 0.492016]\n",
      "85 [D loss: 0.691096, acc.: 46.88%] [G loss: 0.532104]\n",
      "86 [D loss: 0.713690, acc.: 40.62%] [G loss: 0.572843]\n",
      "87 [D loss: 0.690318, acc.: 56.25%] [G loss: 0.601444]\n",
      "88 [D loss: 0.645698, acc.: 68.75%] [G loss: 0.577269]\n",
      "89 [D loss: 0.622909, acc.: 81.25%] [G loss: 0.539242]\n",
      "90 [D loss: 0.614458, acc.: 78.12%] [G loss: 0.484785]\n",
      "91 [D loss: 0.631488, acc.: 65.62%] [G loss: 0.427262]\n",
      "92 [D loss: 0.765769, acc.: 43.75%] [G loss: 0.449164]\n",
      "93 [D loss: 0.736305, acc.: 43.75%] [G loss: 0.529678]\n",
      "94 [D loss: 0.688874, acc.: 46.88%] [G loss: 0.649654]\n",
      "95 [D loss: 0.627380, acc.: 75.00%] [G loss: 0.653368]\n",
      "96 [D loss: 0.543579, acc.: 78.12%] [G loss: 0.550669]\n",
      "97 [D loss: 0.510901, acc.: 78.12%] [G loss: 0.497778]\n",
      "98 [D loss: 0.488506, acc.: 90.62%] [G loss: 0.404512]\n",
      "99 [D loss: 0.720222, acc.: 43.75%] [G loss: 0.362882]\n",
      "100 [D loss: 0.850121, acc.: 43.75%] [G loss: 0.452662]\n",
      "101 [D loss: 0.727982, acc.: 40.62%] [G loss: 0.669481]\n",
      "102 [D loss: 0.624968, acc.: 65.62%] [G loss: 0.624177]\n",
      "103 [D loss: 0.466308, acc.: 87.50%] [G loss: 0.552168]\n",
      "104 [D loss: 0.432145, acc.: 87.50%] [G loss: 0.495594]\n",
      "105 [D loss: 0.444480, acc.: 93.75%] [G loss: 0.407924]\n",
      "106 [D loss: 0.852386, acc.: 40.62%] [G loss: 0.413625]\n",
      "107 [D loss: 0.718162, acc.: 46.88%] [G loss: 0.590770]\n",
      "108 [D loss: 0.664509, acc.: 56.25%] [G loss: 0.858741]\n",
      "109 [D loss: 0.430585, acc.: 87.50%] [G loss: 0.596368]\n",
      "110 [D loss: 0.448123, acc.: 87.50%] [G loss: 0.669941]\n",
      "111 [D loss: 0.363860, acc.: 93.75%] [G loss: 0.490148]\n",
      "112 [D loss: 0.790441, acc.: 40.62%] [G loss: 0.398556]\n",
      "113 [D loss: 0.652748, acc.: 65.62%] [G loss: 0.641278]\n",
      "114 [D loss: 0.744700, acc.: 40.62%] [G loss: 0.652335]\n",
      "115 [D loss: 0.414188, acc.: 93.75%] [G loss: 0.584430]\n",
      "116 [D loss: 0.416243, acc.: 78.12%] [G loss: 0.586321]\n",
      "117 [D loss: 0.596967, acc.: 75.00%] [G loss: 0.413325]\n",
      "118 [D loss: 0.825428, acc.: 34.38%] [G loss: 0.417536]\n",
      "119 [D loss: 0.610619, acc.: 65.62%] [G loss: 0.642964]\n",
      "120 [D loss: 0.693911, acc.: 56.25%] [G loss: 0.606369]\n",
      "121 [D loss: 0.425413, acc.: 84.38%] [G loss: 0.570597]\n",
      "122 [D loss: 0.437974, acc.: 81.25%] [G loss: 0.611320]\n",
      "123 [D loss: 0.784207, acc.: 46.88%] [G loss: 0.466910]\n",
      "124 [D loss: 0.570488, acc.: 81.25%] [G loss: 0.518065]\n",
      "125 [D loss: 0.472861, acc.: 84.38%] [G loss: 0.667661]\n",
      "126 [D loss: 0.645384, acc.: 59.38%] [G loss: 0.456827]\n",
      "127 [D loss: 0.430206, acc.: 87.50%] [G loss: 0.581045]\n",
      "128 [D loss: 0.586242, acc.: 71.88%] [G loss: 0.455554]\n",
      "129 [D loss: 0.790589, acc.: 43.75%] [G loss: 0.427827]\n",
      "130 [D loss: 0.489906, acc.: 68.75%] [G loss: 0.657666]\n",
      "131 [D loss: 0.802830, acc.: 40.62%] [G loss: 0.532360]\n",
      "132 [D loss: 0.538375, acc.: 84.38%] [G loss: 0.551887]\n",
      "133 [D loss: 0.472133, acc.: 78.12%] [G loss: 0.506713]\n",
      "134 [D loss: 0.774298, acc.: 40.62%] [G loss: 0.486001]\n",
      "135 [D loss: 0.714415, acc.: 59.38%] [G loss: 0.428907]\n",
      "136 [D loss: 0.339102, acc.: 100.00%] [G loss: 0.557540]\n",
      "137 [D loss: 0.700952, acc.: 46.88%] [G loss: 0.505252]\n",
      "138 [D loss: 0.686533, acc.: 50.00%] [G loss: 0.488999]\n",
      "139 [D loss: 0.496455, acc.: 78.12%] [G loss: 0.590322]\n",
      "140 [D loss: 0.555788, acc.: 78.12%] [G loss: 0.567388]\n",
      "141 [D loss: 0.665465, acc.: 53.12%] [G loss: 0.468650]\n",
      "142 [D loss: 0.484069, acc.: 87.50%] [G loss: 0.527804]\n",
      "143 [D loss: 0.727245, acc.: 53.12%] [G loss: 0.561223]\n",
      "144 [D loss: 0.617513, acc.: 62.50%] [G loss: 0.474715]\n",
      "145 [D loss: 0.507236, acc.: 84.38%] [G loss: 0.599368]\n",
      "146 [D loss: 0.509098, acc.: 81.25%] [G loss: 0.496664]\n",
      "147 [D loss: 0.918926, acc.: 37.50%] [G loss: 0.378785]\n",
      "148 [D loss: 0.438939, acc.: 87.50%] [G loss: 0.613882]\n",
      "149 [D loss: 0.678617, acc.: 50.00%] [G loss: 0.731949]\n",
      "150 [D loss: 0.705426, acc.: 53.12%] [G loss: 0.500368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 [D loss: 0.463902, acc.: 84.38%] [G loss: 0.492100]\n",
      "152 [D loss: 0.310937, acc.: 100.00%] [G loss: 0.429779]\n",
      "153 [D loss: 0.789003, acc.: 53.12%] [G loss: 0.387524]\n",
      "154 [D loss: 0.751383, acc.: 46.88%] [G loss: 0.341615]\n",
      "155 [D loss: 0.577995, acc.: 65.62%] [G loss: 0.445442]\n",
      "156 [D loss: 0.461816, acc.: 87.50%] [G loss: 0.630386]\n",
      "157 [D loss: 0.745546, acc.: 46.88%] [G loss: 0.662896]\n",
      "158 [D loss: 0.618476, acc.: 71.88%] [G loss: 0.442732]\n",
      "159 [D loss: 0.440689, acc.: 81.25%] [G loss: 0.505979]\n",
      "160 [D loss: 0.383972, acc.: 96.88%] [G loss: 0.477057]\n",
      "161 [D loss: 0.955576, acc.: 37.50%] [G loss: 0.349443]\n",
      "162 [D loss: 0.715627, acc.: 53.12%] [G loss: 0.404488]\n",
      "163 [D loss: 0.420031, acc.: 87.50%] [G loss: 0.527239]\n",
      "164 [D loss: 0.664609, acc.: 59.38%] [G loss: 0.618135]\n",
      "165 [D loss: 0.820015, acc.: 31.25%] [G loss: 0.543220]\n",
      "166 [D loss: 0.505501, acc.: 90.62%] [G loss: 0.494614]\n",
      "167 [D loss: 0.373321, acc.: 93.75%] [G loss: 0.570128]\n",
      "168 [D loss: 0.563521, acc.: 81.25%] [G loss: 0.496105]\n",
      "169 [D loss: 0.977442, acc.: 31.25%] [G loss: 0.410096]\n",
      "170 [D loss: 0.527691, acc.: 84.38%] [G loss: 0.577466]\n",
      "171 [D loss: 0.662640, acc.: 56.25%] [G loss: 0.762281]\n",
      "172 [D loss: 0.782633, acc.: 50.00%] [G loss: 0.513196]\n",
      "173 [D loss: 0.488189, acc.: 84.38%] [G loss: 0.558457]\n",
      "174 [D loss: 0.466968, acc.: 87.50%] [G loss: 0.626374]\n",
      "175 [D loss: 0.707925, acc.: 53.12%] [G loss: 0.545027]\n",
      "176 [D loss: 0.792007, acc.: 37.50%] [G loss: 0.518387]\n",
      "177 [D loss: 0.502221, acc.: 71.88%] [G loss: 0.661699]\n",
      "178 [D loss: 0.618690, acc.: 71.88%] [G loss: 0.776929]\n",
      "179 [D loss: 0.864874, acc.: 28.12%] [G loss: 0.479973]\n",
      "180 [D loss: 0.483202, acc.: 84.38%] [G loss: 0.505234]\n",
      "181 [D loss: 0.449792, acc.: 78.12%] [G loss: 0.549721]\n",
      "182 [D loss: 0.620942, acc.: 68.75%] [G loss: 0.448787]\n",
      "183 [D loss: 0.984853, acc.: 37.50%] [G loss: 0.411657]\n",
      "184 [D loss: 0.598397, acc.: 65.62%] [G loss: 0.572638]\n",
      "185 [D loss: 0.553697, acc.: 81.25%] [G loss: 0.635360]\n",
      "186 [D loss: 0.718055, acc.: 53.12%] [G loss: 0.724892]\n",
      "187 [D loss: 0.795621, acc.: 50.00%] [G loss: 0.549245]\n",
      "188 [D loss: 0.567073, acc.: 68.75%] [G loss: 0.585090]\n",
      "189 [D loss: 0.395504, acc.: 90.62%] [G loss: 0.636673]\n",
      "190 [D loss: 0.550962, acc.: 78.12%] [G loss: 0.551541]\n",
      "191 [D loss: 0.753405, acc.: 53.12%] [G loss: 0.484047]\n",
      "192 [D loss: 0.564908, acc.: 65.62%] [G loss: 0.598290]\n",
      "193 [D loss: 0.444433, acc.: 87.50%] [G loss: 0.786157]\n",
      "194 [D loss: 0.697356, acc.: 59.38%] [G loss: 0.689505]\n",
      "195 [D loss: 0.740092, acc.: 53.12%] [G loss: 0.528420]\n",
      "196 [D loss: 0.641742, acc.: 78.12%] [G loss: 0.639814]\n",
      "197 [D loss: 0.538761, acc.: 75.00%] [G loss: 0.610121]\n",
      "198 [D loss: 0.738245, acc.: 50.00%] [G loss: 0.546593]\n",
      "199 [D loss: 0.695144, acc.: 56.25%] [G loss: 0.562511]\n",
      "200 [D loss: 0.525605, acc.: 81.25%] [G loss: 0.630862]\n",
      "201 [D loss: 0.595328, acc.: 62.50%] [G loss: 0.728920]\n",
      "202 [D loss: 0.839925, acc.: 40.62%] [G loss: 0.630861]\n",
      "203 [D loss: 0.651614, acc.: 78.12%] [G loss: 0.544945]\n",
      "204 [D loss: 0.564704, acc.: 78.12%] [G loss: 0.601465]\n",
      "205 [D loss: 0.538372, acc.: 71.88%] [G loss: 0.602305]\n",
      "206 [D loss: 0.855333, acc.: 37.50%] [G loss: 0.542586]\n",
      "207 [D loss: 0.758664, acc.: 46.88%] [G loss: 0.542865]\n",
      "208 [D loss: 0.584203, acc.: 65.62%] [G loss: 0.621238]\n",
      "209 [D loss: 0.548486, acc.: 84.38%] [G loss: 0.653184]\n",
      "210 [D loss: 0.682619, acc.: 62.50%] [G loss: 0.631029]\n",
      "211 [D loss: 0.805429, acc.: 37.50%] [G loss: 0.558156]\n",
      "212 [D loss: 0.654485, acc.: 56.25%] [G loss: 0.556231]\n",
      "213 [D loss: 0.564540, acc.: 78.12%] [G loss: 0.593563]\n",
      "214 [D loss: 0.618199, acc.: 68.75%] [G loss: 0.605309]\n",
      "215 [D loss: 0.748066, acc.: 53.12%] [G loss: 0.515025]\n",
      "216 [D loss: 0.801211, acc.: 46.88%] [G loss: 0.515945]\n",
      "217 [D loss: 0.613557, acc.: 59.38%] [G loss: 0.581557]\n",
      "218 [D loss: 0.631742, acc.: 65.62%] [G loss: 0.661341]\n",
      "219 [D loss: 0.668785, acc.: 65.62%] [G loss: 0.642432]\n",
      "220 [D loss: 0.702672, acc.: 50.00%] [G loss: 0.596168]\n",
      "221 [D loss: 0.644059, acc.: 59.38%] [G loss: 0.517287]\n",
      "222 [D loss: 0.625611, acc.: 65.62%] [G loss: 0.593744]\n",
      "223 [D loss: 0.638918, acc.: 65.62%] [G loss: 0.562097]\n",
      "224 [D loss: 0.610561, acc.: 68.75%] [G loss: 0.532375]\n",
      "225 [D loss: 0.705963, acc.: 46.88%] [G loss: 0.510750]\n",
      "226 [D loss: 0.701717, acc.: 53.12%] [G loss: 0.562879]\n",
      "227 [D loss: 0.670272, acc.: 62.50%] [G loss: 0.638421]\n",
      "228 [D loss: 0.652731, acc.: 62.50%] [G loss: 0.680304]\n",
      "229 [D loss: 0.638456, acc.: 65.62%] [G loss: 0.652796]\n",
      "230 [D loss: 0.742842, acc.: 50.00%] [G loss: 0.607973]\n",
      "231 [D loss: 0.615591, acc.: 68.75%] [G loss: 0.545664]\n",
      "232 [D loss: 0.574766, acc.: 81.25%] [G loss: 0.564721]\n",
      "233 [D loss: 0.481689, acc.: 84.38%] [G loss: 0.550552]\n",
      "234 [D loss: 0.625295, acc.: 62.50%] [G loss: 0.538166]\n",
      "235 [D loss: 0.709311, acc.: 50.00%] [G loss: 0.495759]\n",
      "236 [D loss: 0.703229, acc.: 46.88%] [G loss: 0.557774]\n",
      "237 [D loss: 0.621000, acc.: 75.00%] [G loss: 0.589158]\n",
      "238 [D loss: 0.606667, acc.: 78.12%] [G loss: 0.668739]\n",
      "239 [D loss: 0.615920, acc.: 62.50%] [G loss: 0.631939]\n",
      "240 [D loss: 0.652422, acc.: 56.25%] [G loss: 0.553483]\n",
      "241 [D loss: 0.616565, acc.: 65.62%] [G loss: 0.558684]\n",
      "242 [D loss: 0.529225, acc.: 75.00%] [G loss: 0.555391]\n",
      "243 [D loss: 0.588931, acc.: 65.62%] [G loss: 0.590019]\n",
      "244 [D loss: 0.712398, acc.: 50.00%] [G loss: 0.523497]\n",
      "245 [D loss: 0.675262, acc.: 50.00%] [G loss: 0.551485]\n",
      "246 [D loss: 0.652226, acc.: 65.62%] [G loss: 0.613307]\n",
      "247 [D loss: 0.592353, acc.: 75.00%] [G loss: 0.675985]\n",
      "248 [D loss: 0.640732, acc.: 56.25%] [G loss: 0.678541]\n",
      "249 [D loss: 0.640629, acc.: 68.75%] [G loss: 0.639612]\n",
      "250 [D loss: 0.657765, acc.: 62.50%] [G loss: 0.629518]\n",
      "251 [D loss: 0.664440, acc.: 62.50%] [G loss: 0.627588]\n",
      "252 [D loss: 0.705103, acc.: 62.50%] [G loss: 0.558067]\n",
      "253 [D loss: 0.837990, acc.: 34.38%] [G loss: 0.573559]\n",
      "254 [D loss: 0.686905, acc.: 46.88%] [G loss: 0.644532]\n",
      "255 [D loss: 0.820149, acc.: 31.25%] [G loss: 0.649316]\n",
      "256 [D loss: 0.698237, acc.: 56.25%] [G loss: 0.694621]\n",
      "257 [D loss: 0.662815, acc.: 59.38%] [G loss: 0.676279]\n",
      "258 [D loss: 0.683230, acc.: 50.00%] [G loss: 0.664941]\n",
      "259 [D loss: 0.635634, acc.: 65.62%] [G loss: 0.673669]\n",
      "260 [D loss: 0.577908, acc.: 87.50%] [G loss: 0.697458]\n",
      "261 [D loss: 0.753531, acc.: 34.38%] [G loss: 0.642965]\n",
      "262 [D loss: 0.745272, acc.: 43.75%] [G loss: 0.627847]\n",
      "263 [D loss: 0.657498, acc.: 62.50%] [G loss: 0.708128]\n",
      "264 [D loss: 0.659652, acc.: 68.75%] [G loss: 0.731929]\n",
      "265 [D loss: 0.666065, acc.: 59.38%] [G loss: 0.762560]\n",
      "266 [D loss: 0.687397, acc.: 59.38%] [G loss: 0.749844]\n",
      "267 [D loss: 0.682286, acc.: 56.25%] [G loss: 0.681059]\n",
      "268 [D loss: 0.602318, acc.: 75.00%] [G loss: 0.692010]\n",
      "269 [D loss: 0.638417, acc.: 62.50%] [G loss: 0.766218]\n",
      "270 [D loss: 0.596920, acc.: 81.25%] [G loss: 0.737085]\n",
      "271 [D loss: 0.737433, acc.: 46.88%] [G loss: 0.683630]\n",
      "272 [D loss: 0.741846, acc.: 43.75%] [G loss: 0.641626]\n",
      "273 [D loss: 0.682734, acc.: 59.38%] [G loss: 0.758093]\n",
      "274 [D loss: 0.563978, acc.: 68.75%] [G loss: 0.870975]\n",
      "275 [D loss: 0.585076, acc.: 75.00%] [G loss: 0.910752]\n",
      "276 [D loss: 0.768301, acc.: 46.88%] [G loss: 0.724141]\n",
      "277 [D loss: 0.685144, acc.: 56.25%] [G loss: 0.731597]\n",
      "278 [D loss: 0.604123, acc.: 62.50%] [G loss: 0.670358]\n",
      "279 [D loss: 0.546758, acc.: 84.38%] [G loss: 0.742096]\n",
      "280 [D loss: 0.615806, acc.: 75.00%] [G loss: 0.776690]\n",
      "281 [D loss: 0.734135, acc.: 50.00%] [G loss: 0.695397]\n",
      "282 [D loss: 0.689297, acc.: 56.25%] [G loss: 0.652506]\n",
      "283 [D loss: 0.647020, acc.: 65.62%] [G loss: 0.680737]\n",
      "284 [D loss: 0.612868, acc.: 75.00%] [G loss: 0.818450]\n",
      "285 [D loss: 0.618987, acc.: 68.75%] [G loss: 0.829116]\n",
      "286 [D loss: 0.733959, acc.: 46.88%] [G loss: 0.697329]\n",
      "287 [D loss: 0.622377, acc.: 65.62%] [G loss: 0.679192]\n",
      "288 [D loss: 0.543238, acc.: 81.25%] [G loss: 0.652086]\n",
      "289 [D loss: 0.516519, acc.: 78.12%] [G loss: 0.680826]\n",
      "290 [D loss: 0.617613, acc.: 71.88%] [G loss: 0.520060]\n",
      "291 [D loss: 0.692931, acc.: 46.88%] [G loss: 0.556340]\n",
      "292 [D loss: 0.622636, acc.: 75.00%] [G loss: 0.585147]\n",
      "293 [D loss: 0.636347, acc.: 65.62%] [G loss: 0.645916]\n",
      "294 [D loss: 0.670736, acc.: 46.88%] [G loss: 0.665892]\n",
      "295 [D loss: 0.710299, acc.: 62.50%] [G loss: 0.610505]\n",
      "296 [D loss: 0.604998, acc.: 81.25%] [G loss: 0.568465]\n",
      "297 [D loss: 0.436983, acc.: 90.62%] [G loss: 0.606603]\n",
      "298 [D loss: 0.659224, acc.: 65.62%] [G loss: 0.544775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 [D loss: 0.729697, acc.: 56.25%] [G loss: 0.522760]\n",
      "300 [D loss: 0.653680, acc.: 59.38%] [G loss: 0.515625]\n",
      "301 [D loss: 0.574621, acc.: 75.00%] [G loss: 0.559991]\n",
      "302 [D loss: 0.467990, acc.: 87.50%] [G loss: 0.590580]\n",
      "303 [D loss: 0.506918, acc.: 84.38%] [G loss: 0.687122]\n",
      "304 [D loss: 0.659304, acc.: 59.38%] [G loss: 0.605138]\n",
      "305 [D loss: 0.762595, acc.: 43.75%] [G loss: 0.473519]\n",
      "306 [D loss: 0.560452, acc.: 78.12%] [G loss: 0.463409]\n",
      "307 [D loss: 0.475062, acc.: 93.75%] [G loss: 0.539960]\n",
      "308 [D loss: 0.618023, acc.: 59.38%] [G loss: 0.552833]\n",
      "309 [D loss: 0.842460, acc.: 37.50%] [G loss: 0.438467]\n",
      "310 [D loss: 0.543950, acc.: 81.25%] [G loss: 0.514176]\n",
      "311 [D loss: 0.467466, acc.: 78.12%] [G loss: 0.566512]\n",
      "312 [D loss: 0.543305, acc.: 81.25%] [G loss: 0.525534]\n",
      "313 [D loss: 0.640340, acc.: 62.50%] [G loss: 0.517448]\n",
      "314 [D loss: 0.709880, acc.: 46.88%] [G loss: 0.513265]\n",
      "315 [D loss: 0.598062, acc.: 71.88%] [G loss: 0.512807]\n",
      "316 [D loss: 0.527684, acc.: 78.12%] [G loss: 0.531174]\n",
      "317 [D loss: 0.548610, acc.: 75.00%] [G loss: 0.484177]\n",
      "318 [D loss: 0.771482, acc.: 43.75%] [G loss: 0.504683]\n",
      "319 [D loss: 0.586556, acc.: 71.88%] [G loss: 0.517484]\n",
      "320 [D loss: 0.501118, acc.: 78.12%] [G loss: 0.703630]\n",
      "321 [D loss: 0.651690, acc.: 59.38%] [G loss: 0.650520]\n",
      "322 [D loss: 0.724728, acc.: 56.25%] [G loss: 0.525162]\n",
      "323 [D loss: 0.561116, acc.: 56.25%] [G loss: 0.548442]\n",
      "324 [D loss: 0.446501, acc.: 84.38%] [G loss: 0.663454]\n",
      "325 [D loss: 0.686306, acc.: 53.12%] [G loss: 0.508778]\n",
      "326 [D loss: 0.838025, acc.: 34.38%] [G loss: 0.502782]\n",
      "327 [D loss: 0.524550, acc.: 81.25%] [G loss: 0.628423]\n",
      "328 [D loss: 0.602827, acc.: 71.88%] [G loss: 0.777769]\n",
      "329 [D loss: 0.680435, acc.: 46.88%] [G loss: 0.642047]\n",
      "330 [D loss: 0.505516, acc.: 81.25%] [G loss: 0.547359]\n",
      "331 [D loss: 0.429450, acc.: 84.38%] [G loss: 0.547942]\n",
      "332 [D loss: 0.432627, acc.: 78.12%] [G loss: 0.393930]\n",
      "333 [D loss: 0.491897, acc.: 81.25%] [G loss: 0.354730]\n",
      "334 [D loss: 0.553075, acc.: 71.88%] [G loss: 0.351358]\n",
      "335 [D loss: 0.735951, acc.: 50.00%] [G loss: 0.406064]\n",
      "336 [D loss: 0.793647, acc.: 31.25%] [G loss: 0.508704]\n",
      "337 [D loss: 0.814246, acc.: 40.62%] [G loss: 0.558427]\n",
      "338 [D loss: 0.614966, acc.: 78.12%] [G loss: 0.768156]\n",
      "339 [D loss: 0.672414, acc.: 53.12%] [G loss: 0.764312]\n",
      "340 [D loss: 0.657535, acc.: 65.62%] [G loss: 0.548420]\n",
      "341 [D loss: 0.435089, acc.: 81.25%] [G loss: 0.470146]\n",
      "342 [D loss: 0.333649, acc.: 93.75%] [G loss: 0.421436]\n",
      "343 [D loss: 0.293987, acc.: 100.00%] [G loss: 0.358751]\n",
      "344 [D loss: 0.504420, acc.: 81.25%] [G loss: 0.278657]\n",
      "345 [D loss: 0.567072, acc.: 71.88%] [G loss: 0.297450]\n",
      "346 [D loss: 0.777011, acc.: 50.00%] [G loss: 0.334776]\n",
      "347 [D loss: 1.032135, acc.: 31.25%] [G loss: 0.391020]\n",
      "348 [D loss: 0.708393, acc.: 50.00%] [G loss: 0.631937]\n",
      "349 [D loss: 0.739949, acc.: 62.50%] [G loss: 0.805832]\n",
      "350 [D loss: 0.702448, acc.: 50.00%] [G loss: 0.656407]\n",
      "351 [D loss: 0.503080, acc.: 78.12%] [G loss: 0.592800]\n",
      "352 [D loss: 0.450023, acc.: 78.12%] [G loss: 0.482836]\n",
      "353 [D loss: 0.438832, acc.: 87.50%] [G loss: 0.409672]\n",
      "354 [D loss: 0.406642, acc.: 93.75%] [G loss: 0.353965]\n",
      "355 [D loss: 0.372058, acc.: 90.62%] [G loss: 0.322234]\n",
      "356 [D loss: 0.552085, acc.: 78.12%] [G loss: 0.270901]\n",
      "357 [D loss: 0.812711, acc.: 53.12%] [G loss: 0.288430]\n",
      "358 [D loss: 0.823582, acc.: 34.38%] [G loss: 0.339700]\n",
      "359 [D loss: 0.627643, acc.: 56.25%] [G loss: 0.449484]\n",
      "360 [D loss: 0.727487, acc.: 50.00%] [G loss: 0.533367]\n",
      "361 [D loss: 0.629737, acc.: 65.62%] [G loss: 0.624180]\n",
      "362 [D loss: 0.671476, acc.: 50.00%] [G loss: 0.559243]\n",
      "363 [D loss: 0.593021, acc.: 71.88%] [G loss: 0.510828]\n",
      "364 [D loss: 0.525521, acc.: 84.38%] [G loss: 0.519280]\n",
      "365 [D loss: 0.671136, acc.: 53.12%] [G loss: 0.478918]\n",
      "366 [D loss: 0.633639, acc.: 68.75%] [G loss: 0.443545]\n",
      "367 [D loss: 0.772376, acc.: 50.00%] [G loss: 0.484997]\n",
      "368 [D loss: 0.648845, acc.: 59.38%] [G loss: 0.539412]\n",
      "369 [D loss: 0.629690, acc.: 59.38%] [G loss: 0.553719]\n",
      "370 [D loss: 0.686704, acc.: 53.12%] [G loss: 0.498654]\n",
      "371 [D loss: 0.643765, acc.: 56.25%] [G loss: 0.625539]\n",
      "372 [D loss: 0.502912, acc.: 90.62%] [G loss: 0.565919]\n",
      "373 [D loss: 0.606220, acc.: 68.75%] [G loss: 0.516132]\n",
      "374 [D loss: 0.550515, acc.: 75.00%] [G loss: 0.552827]\n",
      "375 [D loss: 0.640746, acc.: 78.12%] [G loss: 0.533119]\n",
      "376 [D loss: 0.635416, acc.: 56.25%] [G loss: 0.534662]\n",
      "377 [D loss: 0.650250, acc.: 56.25%] [G loss: 0.592374]\n",
      "378 [D loss: 0.619291, acc.: 65.62%] [G loss: 0.653110]\n",
      "379 [D loss: 0.753949, acc.: 50.00%] [G loss: 0.668015]\n",
      "380 [D loss: 0.730764, acc.: 46.88%] [G loss: 0.660526]\n",
      "381 [D loss: 0.620822, acc.: 68.75%] [G loss: 0.679633]\n",
      "382 [D loss: 0.672403, acc.: 56.25%] [G loss: 0.746116]\n",
      "383 [D loss: 0.660305, acc.: 62.50%] [G loss: 0.675737]\n",
      "384 [D loss: 0.523222, acc.: 81.25%] [G loss: 0.593363]\n",
      "385 [D loss: 0.517430, acc.: 81.25%] [G loss: 0.520616]\n",
      "386 [D loss: 0.653817, acc.: 59.38%] [G loss: 0.556596]\n",
      "387 [D loss: 0.640473, acc.: 65.62%] [G loss: 0.488267]\n",
      "388 [D loss: 0.748445, acc.: 43.75%] [G loss: 0.561542]\n",
      "389 [D loss: 0.708105, acc.: 56.25%] [G loss: 0.630180]\n",
      "390 [D loss: 0.625144, acc.: 68.75%] [G loss: 0.805610]\n",
      "391 [D loss: 0.686539, acc.: 56.25%] [G loss: 0.876663]\n",
      "392 [D loss: 0.672915, acc.: 62.50%] [G loss: 0.683994]\n",
      "393 [D loss: 0.546009, acc.: 68.75%] [G loss: 0.606082]\n",
      "394 [D loss: 0.455062, acc.: 84.38%] [G loss: 0.600384]\n",
      "395 [D loss: 0.418410, acc.: 84.38%] [G loss: 0.531233]\n",
      "396 [D loss: 0.654349, acc.: 53.12%] [G loss: 0.478958]\n",
      "397 [D loss: 0.682962, acc.: 53.12%] [G loss: 0.566037]\n",
      "398 [D loss: 0.509637, acc.: 81.25%] [G loss: 0.795532]\n",
      "399 [D loss: 0.533239, acc.: 78.12%] [G loss: 0.786989]\n",
      "400 [D loss: 0.655812, acc.: 59.38%] [G loss: 0.693374]\n",
      "401 [D loss: 0.458328, acc.: 78.12%] [G loss: 0.577810]\n",
      "402 [D loss: 0.372738, acc.: 90.62%] [G loss: 0.596484]\n",
      "403 [D loss: 0.438463, acc.: 81.25%] [G loss: 0.556656]\n",
      "404 [D loss: 0.733878, acc.: 43.75%] [G loss: 0.554614]\n",
      "405 [D loss: 0.668552, acc.: 56.25%] [G loss: 0.631298]\n",
      "406 [D loss: 0.474379, acc.: 81.25%] [G loss: 0.736898]\n",
      "407 [D loss: 0.524173, acc.: 78.12%] [G loss: 0.742837]\n",
      "408 [D loss: 0.631647, acc.: 59.38%] [G loss: 0.765601]\n",
      "409 [D loss: 0.542338, acc.: 75.00%] [G loss: 0.598431]\n",
      "410 [D loss: 0.422426, acc.: 87.50%] [G loss: 0.523835]\n",
      "411 [D loss: 0.503738, acc.: 71.88%] [G loss: 0.540731]\n",
      "412 [D loss: 0.849507, acc.: 37.50%] [G loss: 0.551285]\n",
      "413 [D loss: 0.565585, acc.: 78.12%] [G loss: 0.692055]\n",
      "414 [D loss: 0.626395, acc.: 62.50%] [G loss: 0.736704]\n",
      "415 [D loss: 0.540768, acc.: 78.12%] [G loss: 0.693258]\n",
      "416 [D loss: 0.711708, acc.: 53.12%] [G loss: 0.658227]\n",
      "417 [D loss: 0.580264, acc.: 65.62%] [G loss: 0.592369]\n",
      "418 [D loss: 0.574240, acc.: 68.75%] [G loss: 0.570849]\n",
      "419 [D loss: 0.654725, acc.: 50.00%] [G loss: 0.553424]\n",
      "420 [D loss: 0.690807, acc.: 56.25%] [G loss: 0.530731]\n",
      "421 [D loss: 0.632554, acc.: 65.62%] [G loss: 0.574594]\n",
      "422 [D loss: 0.578606, acc.: 71.88%] [G loss: 0.636716]\n",
      "423 [D loss: 0.483940, acc.: 81.25%] [G loss: 0.629646]\n",
      "424 [D loss: 0.642431, acc.: 62.50%] [G loss: 0.593223]\n",
      "425 [D loss: 0.682667, acc.: 59.38%] [G loss: 0.522264]\n",
      "426 [D loss: 0.547554, acc.: 81.25%] [G loss: 0.558085]\n",
      "427 [D loss: 0.610937, acc.: 75.00%] [G loss: 0.607250]\n",
      "428 [D loss: 0.773967, acc.: 50.00%] [G loss: 0.515698]\n",
      "429 [D loss: 0.574810, acc.: 62.50%] [G loss: 0.571791]\n",
      "430 [D loss: 0.543836, acc.: 78.12%] [G loss: 0.745196]\n",
      "431 [D loss: 0.514395, acc.: 75.00%] [G loss: 0.873551]\n",
      "432 [D loss: 0.713539, acc.: 53.12%] [G loss: 0.734464]\n",
      "433 [D loss: 0.529462, acc.: 75.00%] [G loss: 0.601516]\n",
      "434 [D loss: 0.541850, acc.: 78.12%] [G loss: 0.676217]\n",
      "435 [D loss: 0.603147, acc.: 68.75%] [G loss: 0.661709]\n",
      "436 [D loss: 0.893211, acc.: 31.25%] [G loss: 0.543569]\n",
      "437 [D loss: 0.577066, acc.: 71.88%] [G loss: 0.713074]\n",
      "438 [D loss: 0.527005, acc.: 87.50%] [G loss: 0.768685]\n",
      "439 [D loss: 0.577295, acc.: 75.00%] [G loss: 0.806334]\n",
      "440 [D loss: 0.693995, acc.: 53.12%] [G loss: 0.701449]\n",
      "441 [D loss: 0.518111, acc.: 68.75%] [G loss: 0.576647]\n",
      "442 [D loss: 0.449154, acc.: 78.12%] [G loss: 0.638813]\n",
      "443 [D loss: 0.510058, acc.: 75.00%] [G loss: 0.551995]\n",
      "444 [D loss: 0.765702, acc.: 53.12%] [G loss: 0.523354]\n",
      "445 [D loss: 0.726785, acc.: 50.00%] [G loss: 0.606365]\n",
      "446 [D loss: 0.646441, acc.: 62.50%] [G loss: 0.780923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447 [D loss: 0.631819, acc.: 71.88%] [G loss: 1.019301]\n",
      "448 [D loss: 0.694124, acc.: 53.12%] [G loss: 0.836700]\n",
      "449 [D loss: 0.523440, acc.: 71.88%] [G loss: 0.734457]\n",
      "450 [D loss: 0.386636, acc.: 90.62%] [G loss: 0.697042]\n",
      "451 [D loss: 0.491359, acc.: 71.88%] [G loss: 0.542880]\n",
      "452 [D loss: 0.619612, acc.: 62.50%] [G loss: 0.458482]\n",
      "453 [D loss: 0.736099, acc.: 50.00%] [G loss: 0.510305]\n",
      "454 [D loss: 0.694799, acc.: 50.00%] [G loss: 0.608297]\n",
      "455 [D loss: 0.815841, acc.: 37.50%] [G loss: 0.646294]\n",
      "456 [D loss: 0.699968, acc.: 46.88%] [G loss: 0.757479]\n",
      "457 [D loss: 0.575718, acc.: 81.25%] [G loss: 0.877850]\n",
      "458 [D loss: 0.682925, acc.: 50.00%] [G loss: 0.829193]\n",
      "459 [D loss: 0.667107, acc.: 62.50%] [G loss: 0.729000]\n",
      "460 [D loss: 0.577664, acc.: 71.88%] [G loss: 0.731620]\n",
      "461 [D loss: 0.593993, acc.: 71.88%] [G loss: 0.687785]\n",
      "462 [D loss: 0.613915, acc.: 65.62%] [G loss: 0.669252]\n",
      "463 [D loss: 0.790443, acc.: 40.62%] [G loss: 0.683209]\n",
      "464 [D loss: 0.687225, acc.: 46.88%] [G loss: 0.727166]\n",
      "465 [D loss: 0.701075, acc.: 59.38%] [G loss: 0.738087]\n",
      "466 [D loss: 0.670650, acc.: 75.00%] [G loss: 0.695567]\n",
      "467 [D loss: 0.612129, acc.: 71.88%] [G loss: 0.758502]\n",
      "468 [D loss: 0.635574, acc.: 71.88%] [G loss: 0.648850]\n",
      "469 [D loss: 0.703978, acc.: 56.25%] [G loss: 0.585981]\n",
      "470 [D loss: 0.622334, acc.: 65.62%] [G loss: 0.590537]\n",
      "471 [D loss: 0.552977, acc.: 84.38%] [G loss: 0.565392]\n",
      "472 [D loss: 0.690993, acc.: 53.12%] [G loss: 0.492695]\n",
      "473 [D loss: 0.556114, acc.: 75.00%] [G loss: 0.480811]\n",
      "474 [D loss: 0.598235, acc.: 71.88%] [G loss: 0.440550]\n",
      "475 [D loss: 0.755777, acc.: 43.75%] [G loss: 0.525583]\n",
      "476 [D loss: 0.686601, acc.: 59.38%] [G loss: 0.565599]\n",
      "477 [D loss: 0.640357, acc.: 59.38%] [G loss: 0.642689]\n",
      "478 [D loss: 0.682632, acc.: 59.38%] [G loss: 0.783194]\n",
      "479 [D loss: 0.706555, acc.: 43.75%] [G loss: 0.809847]\n",
      "480 [D loss: 0.583091, acc.: 68.75%] [G loss: 0.919920]\n",
      "481 [D loss: 0.618485, acc.: 62.50%] [G loss: 0.845100]\n",
      "482 [D loss: 0.666981, acc.: 62.50%] [G loss: 0.808856]\n",
      "483 [D loss: 0.637110, acc.: 68.75%] [G loss: 0.751227]\n",
      "484 [D loss: 0.502116, acc.: 75.00%] [G loss: 0.744548]\n",
      "485 [D loss: 0.618790, acc.: 62.50%] [G loss: 0.780967]\n",
      "486 [D loss: 0.647943, acc.: 68.75%] [G loss: 0.766802]\n",
      "487 [D loss: 0.600641, acc.: 78.12%] [G loss: 0.781607]\n",
      "488 [D loss: 0.665249, acc.: 59.38%] [G loss: 0.875120]\n",
      "489 [D loss: 0.685368, acc.: 53.12%] [G loss: 0.863514]\n",
      "490 [D loss: 0.716339, acc.: 50.00%] [G loss: 0.868091]\n",
      "491 [D loss: 0.579855, acc.: 81.25%] [G loss: 0.841119]\n",
      "492 [D loss: 0.684862, acc.: 56.25%] [G loss: 0.870341]\n",
      "493 [D loss: 0.642773, acc.: 75.00%] [G loss: 0.895488]\n",
      "494 [D loss: 0.644674, acc.: 59.38%] [G loss: 0.829348]\n",
      "495 [D loss: 0.567693, acc.: 78.12%] [G loss: 0.839091]\n",
      "496 [D loss: 0.707103, acc.: 53.12%] [G loss: 0.851264]\n",
      "497 [D loss: 0.632364, acc.: 62.50%] [G loss: 0.764254]\n",
      "498 [D loss: 0.544595, acc.: 75.00%] [G loss: 0.718526]\n",
      "499 [D loss: 0.701758, acc.: 53.12%] [G loss: 0.632052]\n",
      "500 [D loss: 0.680516, acc.: 53.12%] [G loss: 0.704097]\n",
      "501 [D loss: 0.682168, acc.: 53.12%] [G loss: 0.829862]\n",
      "502 [D loss: 0.645189, acc.: 62.50%] [G loss: 0.854358]\n",
      "503 [D loss: 0.596803, acc.: 68.75%] [G loss: 0.877724]\n",
      "504 [D loss: 0.613834, acc.: 65.62%] [G loss: 0.996805]\n",
      "505 [D loss: 0.648424, acc.: 62.50%] [G loss: 0.928765]\n",
      "506 [D loss: 0.626387, acc.: 71.88%] [G loss: 0.872958]\n",
      "507 [D loss: 0.576104, acc.: 71.88%] [G loss: 0.819328]\n",
      "508 [D loss: 0.596749, acc.: 65.62%] [G loss: 0.830534]\n",
      "509 [D loss: 0.538654, acc.: 81.25%] [G loss: 0.817605]\n",
      "510 [D loss: 0.495045, acc.: 84.38%] [G loss: 0.823365]\n",
      "511 [D loss: 0.614548, acc.: 56.25%] [G loss: 0.775922]\n",
      "512 [D loss: 0.772809, acc.: 46.88%] [G loss: 0.813254]\n",
      "513 [D loss: 0.561376, acc.: 75.00%] [G loss: 0.792002]\n",
      "514 [D loss: 0.715858, acc.: 43.75%] [G loss: 0.843227]\n",
      "515 [D loss: 0.670815, acc.: 50.00%] [G loss: 0.812309]\n",
      "516 [D loss: 0.618710, acc.: 75.00%] [G loss: 0.920891]\n",
      "517 [D loss: 0.658436, acc.: 62.50%] [G loss: 0.911525]\n",
      "518 [D loss: 0.615539, acc.: 71.88%] [G loss: 0.805861]\n",
      "519 [D loss: 0.624613, acc.: 65.62%] [G loss: 0.691390]\n",
      "520 [D loss: 0.489650, acc.: 81.25%] [G loss: 0.723004]\n",
      "521 [D loss: 0.666145, acc.: 65.62%] [G loss: 0.666289]\n",
      "522 [D loss: 0.585181, acc.: 68.75%] [G loss: 0.705801]\n",
      "523 [D loss: 0.674972, acc.: 59.38%] [G loss: 0.676666]\n",
      "524 [D loss: 0.610583, acc.: 65.62%] [G loss: 0.743236]\n",
      "525 [D loss: 0.555957, acc.: 75.00%] [G loss: 0.746286]\n",
      "526 [D loss: 0.566851, acc.: 65.62%] [G loss: 0.779266]\n",
      "527 [D loss: 0.675999, acc.: 46.88%] [G loss: 0.790425]\n",
      "528 [D loss: 0.642662, acc.: 62.50%] [G loss: 0.787967]\n",
      "529 [D loss: 0.712796, acc.: 56.25%] [G loss: 0.633407]\n",
      "530 [D loss: 0.564191, acc.: 71.88%] [G loss: 0.631415]\n",
      "531 [D loss: 0.713744, acc.: 56.25%] [G loss: 0.631887]\n",
      "532 [D loss: 0.713291, acc.: 62.50%] [G loss: 0.564596]\n",
      "533 [D loss: 0.623157, acc.: 65.62%] [G loss: 0.555842]\n",
      "534 [D loss: 0.694291, acc.: 56.25%] [G loss: 0.586766]\n",
      "535 [D loss: 0.634555, acc.: 59.38%] [G loss: 0.722984]\n",
      "536 [D loss: 0.605492, acc.: 65.62%] [G loss: 0.710835]\n",
      "537 [D loss: 0.611987, acc.: 65.62%] [G loss: 0.819030]\n",
      "538 [D loss: 0.630558, acc.: 56.25%] [G loss: 0.804247]\n",
      "539 [D loss: 0.740360, acc.: 43.75%] [G loss: 0.693274]\n",
      "540 [D loss: 0.636723, acc.: 71.88%] [G loss: 0.690196]\n",
      "541 [D loss: 0.654341, acc.: 56.25%] [G loss: 0.643587]\n",
      "542 [D loss: 0.540792, acc.: 78.12%] [G loss: 0.636491]\n",
      "543 [D loss: 0.651312, acc.: 65.62%] [G loss: 0.569088]\n",
      "544 [D loss: 0.612230, acc.: 62.50%] [G loss: 0.686601]\n",
      "545 [D loss: 0.607127, acc.: 68.75%] [G loss: 0.603594]\n",
      "546 [D loss: 0.676693, acc.: 65.62%] [G loss: 0.676811]\n",
      "547 [D loss: 0.726019, acc.: 43.75%] [G loss: 0.780097]\n",
      "548 [D loss: 0.690490, acc.: 68.75%] [G loss: 0.827330]\n",
      "549 [D loss: 0.675795, acc.: 56.25%] [G loss: 0.798387]\n",
      "550 [D loss: 0.529993, acc.: 78.12%] [G loss: 0.900287]\n",
      "551 [D loss: 0.580125, acc.: 78.12%] [G loss: 0.799279]\n",
      "552 [D loss: 0.573996, acc.: 71.88%] [G loss: 0.722728]\n",
      "553 [D loss: 0.515665, acc.: 84.38%] [G loss: 0.679794]\n",
      "554 [D loss: 0.469308, acc.: 84.38%] [G loss: 0.588596]\n",
      "555 [D loss: 0.570200, acc.: 75.00%] [G loss: 0.523051]\n",
      "556 [D loss: 0.653016, acc.: 59.38%] [G loss: 0.451752]\n",
      "557 [D loss: 0.807552, acc.: 40.62%] [G loss: 0.509710]\n",
      "558 [D loss: 0.676015, acc.: 53.12%] [G loss: 0.722453]\n",
      "559 [D loss: 0.701494, acc.: 56.25%] [G loss: 0.852071]\n",
      "560 [D loss: 0.596977, acc.: 65.62%] [G loss: 0.815685]\n",
      "561 [D loss: 0.618089, acc.: 75.00%] [G loss: 0.691615]\n",
      "562 [D loss: 0.476898, acc.: 90.62%] [G loss: 0.653430]\n",
      "563 [D loss: 0.471503, acc.: 75.00%] [G loss: 0.567830]\n",
      "564 [D loss: 0.490988, acc.: 81.25%] [G loss: 0.460207]\n",
      "565 [D loss: 0.523074, acc.: 71.88%] [G loss: 0.437819]\n",
      "566 [D loss: 0.700197, acc.: 53.12%] [G loss: 0.517792]\n",
      "567 [D loss: 0.797758, acc.: 43.75%] [G loss: 0.515844]\n",
      "568 [D loss: 0.623979, acc.: 65.62%] [G loss: 0.696574]\n",
      "569 [D loss: 0.617344, acc.: 62.50%] [G loss: 0.764852]\n",
      "570 [D loss: 0.614993, acc.: 68.75%] [G loss: 0.901513]\n",
      "571 [D loss: 0.656946, acc.: 56.25%] [G loss: 0.774249]\n",
      "572 [D loss: 0.602537, acc.: 65.62%] [G loss: 0.672252]\n",
      "573 [D loss: 0.559840, acc.: 65.62%] [G loss: 0.621257]\n",
      "574 [D loss: 0.579733, acc.: 68.75%] [G loss: 0.610290]\n",
      "575 [D loss: 0.496368, acc.: 78.12%] [G loss: 0.564377]\n",
      "576 [D loss: 0.610017, acc.: 53.12%] [G loss: 0.401666]\n",
      "577 [D loss: 0.638933, acc.: 62.50%] [G loss: 0.465968]\n",
      "578 [D loss: 0.764917, acc.: 53.12%] [G loss: 0.581781]\n",
      "579 [D loss: 0.701555, acc.: 53.12%] [G loss: 0.727420]\n",
      "580 [D loss: 0.614018, acc.: 65.62%] [G loss: 0.811976]\n",
      "581 [D loss: 0.722231, acc.: 46.88%] [G loss: 0.841425]\n",
      "582 [D loss: 0.626111, acc.: 53.12%] [G loss: 0.787417]\n",
      "583 [D loss: 0.740302, acc.: 50.00%] [G loss: 0.691647]\n",
      "584 [D loss: 0.471442, acc.: 84.38%] [G loss: 0.637416]\n",
      "585 [D loss: 0.580027, acc.: 71.88%] [G loss: 0.594671]\n",
      "586 [D loss: 0.521335, acc.: 71.88%] [G loss: 0.561859]\n",
      "587 [D loss: 0.711554, acc.: 62.50%] [G loss: 0.525227]\n",
      "588 [D loss: 0.643155, acc.: 68.75%] [G loss: 0.549142]\n",
      "589 [D loss: 0.931308, acc.: 34.38%] [G loss: 0.744978]\n",
      "590 [D loss: 0.570076, acc.: 75.00%] [G loss: 0.922674]\n",
      "591 [D loss: 0.599768, acc.: 71.88%] [G loss: 0.892225]\n",
      "592 [D loss: 0.514731, acc.: 81.25%] [G loss: 0.790250]\n",
      "593 [D loss: 0.735054, acc.: 50.00%] [G loss: 0.780582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594 [D loss: 0.679709, acc.: 59.38%] [G loss: 0.736798]\n",
      "595 [D loss: 0.495716, acc.: 78.12%] [G loss: 0.695527]\n",
      "596 [D loss: 0.510998, acc.: 71.88%] [G loss: 0.648045]\n",
      "597 [D loss: 0.548328, acc.: 62.50%] [G loss: 0.564053]\n",
      "598 [D loss: 0.621346, acc.: 62.50%] [G loss: 0.549291]\n",
      "599 [D loss: 0.709900, acc.: 59.38%] [G loss: 0.594592]\n",
      "600 [D loss: 0.702818, acc.: 56.25%] [G loss: 0.722911]\n",
      "601 [D loss: 0.590063, acc.: 68.75%] [G loss: 0.822312]\n",
      "602 [D loss: 0.602547, acc.: 65.62%] [G loss: 0.825549]\n",
      "603 [D loss: 0.599898, acc.: 59.38%] [G loss: 0.821343]\n",
      "604 [D loss: 0.659855, acc.: 65.62%] [G loss: 0.751943]\n",
      "605 [D loss: 0.659065, acc.: 56.25%] [G loss: 0.735348]\n",
      "606 [D loss: 0.594298, acc.: 68.75%] [G loss: 0.762591]\n",
      "607 [D loss: 0.552417, acc.: 81.25%] [G loss: 0.693535]\n",
      "608 [D loss: 0.603211, acc.: 68.75%] [G loss: 0.611278]\n",
      "609 [D loss: 0.646463, acc.: 62.50%] [G loss: 0.635883]\n",
      "610 [D loss: 0.735843, acc.: 50.00%] [G loss: 0.680682]\n",
      "611 [D loss: 0.680310, acc.: 56.25%] [G loss: 0.706680]\n",
      "612 [D loss: 0.613225, acc.: 65.62%] [G loss: 0.799089]\n",
      "613 [D loss: 0.592131, acc.: 78.12%] [G loss: 0.843103]\n",
      "614 [D loss: 0.574400, acc.: 68.75%] [G loss: 0.797813]\n",
      "615 [D loss: 0.829407, acc.: 46.88%] [G loss: 0.742130]\n",
      "616 [D loss: 0.756410, acc.: 46.88%] [G loss: 0.769830]\n",
      "617 [D loss: 0.609722, acc.: 68.75%] [G loss: 0.702345]\n",
      "618 [D loss: 0.676921, acc.: 53.12%] [G loss: 0.735848]\n",
      "619 [D loss: 0.616102, acc.: 68.75%] [G loss: 0.644050]\n",
      "620 [D loss: 0.654898, acc.: 56.25%] [G loss: 0.702366]\n",
      "621 [D loss: 0.676274, acc.: 46.88%] [G loss: 0.800575]\n",
      "622 [D loss: 0.600218, acc.: 65.62%] [G loss: 0.754051]\n",
      "623 [D loss: 0.660389, acc.: 59.38%] [G loss: 0.807969]\n",
      "624 [D loss: 0.600002, acc.: 75.00%] [G loss: 0.714438]\n",
      "625 [D loss: 0.685957, acc.: 53.12%] [G loss: 0.736928]\n",
      "626 [D loss: 0.616821, acc.: 65.62%] [G loss: 0.740968]\n",
      "627 [D loss: 0.582641, acc.: 68.75%] [G loss: 0.787068]\n",
      "628 [D loss: 0.614638, acc.: 59.38%] [G loss: 0.809570]\n",
      "629 [D loss: 0.593743, acc.: 71.88%] [G loss: 0.747037]\n",
      "630 [D loss: 0.624115, acc.: 71.88%] [G loss: 0.731111]\n",
      "631 [D loss: 0.620852, acc.: 65.62%] [G loss: 0.662600]\n",
      "632 [D loss: 0.590581, acc.: 75.00%] [G loss: 0.717021]\n",
      "633 [D loss: 0.581492, acc.: 68.75%] [G loss: 0.726982]\n",
      "634 [D loss: 0.678426, acc.: 50.00%] [G loss: 0.794136]\n",
      "635 [D loss: 0.531191, acc.: 81.25%] [G loss: 0.831319]\n",
      "636 [D loss: 0.569069, acc.: 75.00%] [G loss: 0.838080]\n",
      "637 [D loss: 0.607583, acc.: 75.00%] [G loss: 0.843534]\n",
      "638 [D loss: 0.571014, acc.: 71.88%] [G loss: 0.833529]\n",
      "639 [D loss: 0.615732, acc.: 56.25%] [G loss: 0.879869]\n",
      "640 [D loss: 0.604681, acc.: 78.12%] [G loss: 0.985540]\n",
      "641 [D loss: 0.607549, acc.: 59.38%] [G loss: 0.931635]\n",
      "642 [D loss: 0.556691, acc.: 71.88%] [G loss: 0.916836]\n",
      "643 [D loss: 0.588708, acc.: 71.88%] [G loss: 0.834423]\n",
      "644 [D loss: 0.577745, acc.: 68.75%] [G loss: 0.865038]\n",
      "645 [D loss: 0.609023, acc.: 75.00%] [G loss: 0.967442]\n",
      "646 [D loss: 0.598050, acc.: 78.12%] [G loss: 0.871904]\n",
      "647 [D loss: 0.540444, acc.: 71.88%] [G loss: 0.910207]\n",
      "648 [D loss: 0.638666, acc.: 53.12%] [G loss: 0.882723]\n",
      "649 [D loss: 0.618331, acc.: 65.62%] [G loss: 0.868647]\n",
      "650 [D loss: 0.606001, acc.: 62.50%] [G loss: 0.921145]\n",
      "651 [D loss: 0.689155, acc.: 56.25%] [G loss: 1.043648]\n",
      "652 [D loss: 0.706775, acc.: 56.25%] [G loss: 1.022174]\n",
      "653 [D loss: 0.605022, acc.: 59.38%] [G loss: 0.968957]\n",
      "654 [D loss: 0.680593, acc.: 53.12%] [G loss: 0.939955]\n",
      "655 [D loss: 0.668334, acc.: 59.38%] [G loss: 0.895576]\n",
      "656 [D loss: 0.635060, acc.: 65.62%] [G loss: 0.938922]\n",
      "657 [D loss: 0.598446, acc.: 62.50%] [G loss: 0.865045]\n",
      "658 [D loss: 0.524559, acc.: 75.00%] [G loss: 0.835222]\n",
      "659 [D loss: 0.669521, acc.: 65.62%] [G loss: 0.794590]\n",
      "660 [D loss: 0.702604, acc.: 59.38%] [G loss: 0.761678]\n",
      "661 [D loss: 0.640445, acc.: 59.38%] [G loss: 0.773243]\n",
      "662 [D loss: 0.686193, acc.: 62.50%] [G loss: 0.841080]\n",
      "663 [D loss: 0.711175, acc.: 59.38%] [G loss: 0.897917]\n",
      "664 [D loss: 0.648608, acc.: 43.75%] [G loss: 0.951816]\n",
      "665 [D loss: 0.572200, acc.: 68.75%] [G loss: 0.961379]\n",
      "666 [D loss: 0.690665, acc.: 50.00%] [G loss: 0.805227]\n",
      "667 [D loss: 0.559460, acc.: 75.00%] [G loss: 0.839855]\n",
      "668 [D loss: 0.673447, acc.: 56.25%] [G loss: 0.902749]\n",
      "669 [D loss: 0.589353, acc.: 71.88%] [G loss: 0.919170]\n",
      "670 [D loss: 0.727241, acc.: 50.00%] [G loss: 0.878699]\n",
      "671 [D loss: 0.681818, acc.: 50.00%] [G loss: 0.895944]\n",
      "672 [D loss: 0.574369, acc.: 68.75%] [G loss: 0.880197]\n",
      "673 [D loss: 0.689618, acc.: 59.38%] [G loss: 0.854904]\n",
      "674 [D loss: 0.696152, acc.: 53.12%] [G loss: 0.856736]\n",
      "675 [D loss: 0.656002, acc.: 62.50%] [G loss: 0.910055]\n",
      "676 [D loss: 0.692965, acc.: 46.88%] [G loss: 0.844474]\n",
      "677 [D loss: 0.701190, acc.: 50.00%] [G loss: 0.894864]\n",
      "678 [D loss: 0.589446, acc.: 71.88%] [G loss: 0.897076]\n",
      "679 [D loss: 0.743324, acc.: 37.50%] [G loss: 0.803731]\n",
      "680 [D loss: 0.547264, acc.: 75.00%] [G loss: 0.888400]\n",
      "681 [D loss: 0.620265, acc.: 62.50%] [G loss: 0.896868]\n",
      "682 [D loss: 0.691289, acc.: 50.00%] [G loss: 0.890392]\n",
      "683 [D loss: 0.655600, acc.: 50.00%] [G loss: 0.811693]\n",
      "684 [D loss: 0.649567, acc.: 56.25%] [G loss: 0.826580]\n",
      "685 [D loss: 0.674948, acc.: 50.00%] [G loss: 0.752069]\n",
      "686 [D loss: 0.770956, acc.: 46.88%] [G loss: 0.850999]\n",
      "687 [D loss: 0.709923, acc.: 53.12%] [G loss: 0.872209]\n",
      "688 [D loss: 0.684107, acc.: 65.62%] [G loss: 0.839686]\n",
      "689 [D loss: 0.608276, acc.: 68.75%] [G loss: 0.803279]\n",
      "690 [D loss: 0.635612, acc.: 71.88%] [G loss: 0.706502]\n",
      "691 [D loss: 0.803494, acc.: 40.62%] [G loss: 0.706801]\n",
      "692 [D loss: 0.617669, acc.: 81.25%] [G loss: 0.792255]\n",
      "693 [D loss: 0.626158, acc.: 62.50%] [G loss: 0.669464]\n",
      "694 [D loss: 0.680882, acc.: 56.25%] [G loss: 0.776080]\n",
      "695 [D loss: 0.683120, acc.: 56.25%] [G loss: 0.795532]\n",
      "696 [D loss: 0.705226, acc.: 43.75%] [G loss: 0.776587]\n",
      "697 [D loss: 0.645078, acc.: 65.62%] [G loss: 0.750780]\n",
      "698 [D loss: 0.605682, acc.: 65.62%] [G loss: 0.849717]\n",
      "699 [D loss: 0.571340, acc.: 71.88%] [G loss: 0.809777]\n",
      "700 [D loss: 0.697895, acc.: 50.00%] [G loss: 0.850973]\n",
      "701 [D loss: 0.642527, acc.: 62.50%] [G loss: 0.890486]\n",
      "702 [D loss: 0.636425, acc.: 59.38%] [G loss: 0.887022]\n",
      "703 [D loss: 0.698815, acc.: 56.25%] [G loss: 0.906396]\n",
      "704 [D loss: 0.678872, acc.: 62.50%] [G loss: 0.789809]\n",
      "705 [D loss: 0.676155, acc.: 68.75%] [G loss: 0.789516]\n",
      "706 [D loss: 0.545513, acc.: 87.50%] [G loss: 0.825620]\n",
      "707 [D loss: 0.609124, acc.: 71.88%] [G loss: 0.743872]\n",
      "708 [D loss: 0.632473, acc.: 65.62%] [G loss: 0.813460]\n",
      "709 [D loss: 0.673250, acc.: 56.25%] [G loss: 0.887190]\n",
      "710 [D loss: 0.658993, acc.: 65.62%] [G loss: 0.893809]\n",
      "711 [D loss: 0.585719, acc.: 78.12%] [G loss: 0.938367]\n",
      "712 [D loss: 0.658714, acc.: 62.50%] [G loss: 0.862922]\n",
      "713 [D loss: 0.610894, acc.: 68.75%] [G loss: 0.914427]\n",
      "714 [D loss: 0.621273, acc.: 62.50%] [G loss: 0.912063]\n",
      "715 [D loss: 0.574275, acc.: 71.88%] [G loss: 0.908866]\n",
      "716 [D loss: 0.612785, acc.: 68.75%] [G loss: 0.831338]\n",
      "717 [D loss: 0.806183, acc.: 43.75%] [G loss: 0.756120]\n",
      "718 [D loss: 0.635388, acc.: 68.75%] [G loss: 0.784174]\n",
      "719 [D loss: 0.568376, acc.: 81.25%] [G loss: 0.766375]\n",
      "720 [D loss: 0.614236, acc.: 68.75%] [G loss: 0.848755]\n",
      "721 [D loss: 0.618639, acc.: 65.62%] [G loss: 0.808330]\n",
      "722 [D loss: 0.635130, acc.: 59.38%] [G loss: 0.784548]\n",
      "723 [D loss: 0.654235, acc.: 59.38%] [G loss: 0.705367]\n",
      "724 [D loss: 0.781311, acc.: 46.88%] [G loss: 0.811503]\n",
      "725 [D loss: 0.729558, acc.: 50.00%] [G loss: 0.843571]\n",
      "726 [D loss: 0.560949, acc.: 75.00%] [G loss: 0.884251]\n",
      "727 [D loss: 0.583968, acc.: 78.12%] [G loss: 0.912511]\n",
      "728 [D loss: 0.693552, acc.: 59.38%] [G loss: 0.939387]\n",
      "729 [D loss: 0.654380, acc.: 65.62%] [G loss: 0.819969]\n",
      "730 [D loss: 0.764519, acc.: 46.88%] [G loss: 0.828041]\n",
      "731 [D loss: 0.646128, acc.: 65.62%] [G loss: 0.896664]\n",
      "732 [D loss: 0.652488, acc.: 65.62%] [G loss: 0.821980]\n",
      "733 [D loss: 0.588025, acc.: 65.62%] [G loss: 0.780844]\n",
      "734 [D loss: 0.573232, acc.: 62.50%] [G loss: 0.858720]\n",
      "735 [D loss: 0.628345, acc.: 56.25%] [G loss: 0.760476]\n",
      "736 [D loss: 0.713869, acc.: 53.12%] [G loss: 0.760568]\n",
      "737 [D loss: 0.751936, acc.: 37.50%] [G loss: 0.727310]\n",
      "738 [D loss: 0.688317, acc.: 53.12%] [G loss: 0.839817]\n",
      "739 [D loss: 0.742672, acc.: 50.00%] [G loss: 1.033668]\n",
      "740 [D loss: 0.591310, acc.: 68.75%] [G loss: 0.995616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 [D loss: 0.589056, acc.: 65.62%] [G loss: 1.013323]\n",
      "742 [D loss: 0.754154, acc.: 46.88%] [G loss: 0.923599]\n",
      "743 [D loss: 0.728587, acc.: 43.75%] [G loss: 0.788822]\n",
      "744 [D loss: 0.694883, acc.: 53.12%] [G loss: 0.799528]\n",
      "745 [D loss: 0.557714, acc.: 84.38%] [G loss: 0.782013]\n",
      "746 [D loss: 0.510963, acc.: 75.00%] [G loss: 0.716947]\n",
      "747 [D loss: 0.568292, acc.: 75.00%] [G loss: 0.656537]\n",
      "748 [D loss: 0.521367, acc.: 75.00%] [G loss: 0.654833]\n",
      "749 [D loss: 0.606602, acc.: 65.62%] [G loss: 0.513126]\n",
      "750 [D loss: 0.730767, acc.: 62.50%] [G loss: 0.540645]\n",
      "751 [D loss: 0.635189, acc.: 56.25%] [G loss: 0.635925]\n",
      "752 [D loss: 0.782348, acc.: 40.62%] [G loss: 0.732633]\n",
      "753 [D loss: 0.721061, acc.: 53.12%] [G loss: 0.843701]\n",
      "754 [D loss: 0.667423, acc.: 65.62%] [G loss: 0.841965]\n",
      "755 [D loss: 0.757009, acc.: 40.62%] [G loss: 1.021166]\n",
      "756 [D loss: 0.678957, acc.: 40.62%] [G loss: 0.993238]\n",
      "757 [D loss: 0.671498, acc.: 56.25%] [G loss: 0.908365]\n",
      "758 [D loss: 0.761490, acc.: 43.75%] [G loss: 0.766419]\n",
      "759 [D loss: 0.625291, acc.: 68.75%] [G loss: 0.744306]\n",
      "760 [D loss: 0.619875, acc.: 68.75%] [G loss: 0.672413]\n",
      "761 [D loss: 0.509921, acc.: 78.12%] [G loss: 0.752883]\n",
      "762 [D loss: 0.537774, acc.: 78.12%] [G loss: 0.628428]\n",
      "763 [D loss: 0.555658, acc.: 71.88%] [G loss: 0.695752]\n",
      "764 [D loss: 0.706541, acc.: 53.12%] [G loss: 0.705311]\n",
      "765 [D loss: 0.653064, acc.: 59.38%] [G loss: 0.695815]\n",
      "766 [D loss: 0.739678, acc.: 53.12%] [G loss: 0.778533]\n",
      "767 [D loss: 0.640954, acc.: 71.88%] [G loss: 0.847793]\n",
      "768 [D loss: 0.586423, acc.: 71.88%] [G loss: 0.963320]\n",
      "769 [D loss: 0.602276, acc.: 78.12%] [G loss: 0.869374]\n",
      "770 [D loss: 0.550258, acc.: 71.88%] [G loss: 0.864306]\n",
      "771 [D loss: 0.637938, acc.: 53.12%] [G loss: 0.897230]\n",
      "772 [D loss: 0.658849, acc.: 59.38%] [G loss: 0.796126]\n",
      "773 [D loss: 0.611867, acc.: 68.75%] [G loss: 0.867262]\n",
      "774 [D loss: 0.786583, acc.: 34.38%] [G loss: 0.760201]\n",
      "775 [D loss: 0.739496, acc.: 46.88%] [G loss: 0.777240]\n",
      "776 [D loss: 0.570650, acc.: 71.88%] [G loss: 0.748788]\n",
      "777 [D loss: 0.531524, acc.: 75.00%] [G loss: 0.756219]\n",
      "778 [D loss: 0.638972, acc.: 53.12%] [G loss: 0.665298]\n",
      "779 [D loss: 0.744825, acc.: 37.50%] [G loss: 0.572144]\n",
      "780 [D loss: 0.756705, acc.: 50.00%] [G loss: 0.704115]\n",
      "781 [D loss: 0.729795, acc.: 50.00%] [G loss: 0.738052]\n",
      "782 [D loss: 0.813676, acc.: 34.38%] [G loss: 0.753458]\n",
      "783 [D loss: 0.678780, acc.: 56.25%] [G loss: 0.789120]\n",
      "784 [D loss: 0.662170, acc.: 56.25%] [G loss: 0.819807]\n",
      "785 [D loss: 0.651037, acc.: 59.38%] [G loss: 0.890315]\n",
      "786 [D loss: 0.679871, acc.: 62.50%] [G loss: 0.896265]\n",
      "787 [D loss: 0.671087, acc.: 56.25%] [G loss: 0.862489]\n",
      "788 [D loss: 0.678924, acc.: 56.25%] [G loss: 0.843467]\n",
      "789 [D loss: 0.683671, acc.: 56.25%] [G loss: 0.819613]\n",
      "790 [D loss: 0.689114, acc.: 65.62%] [G loss: 0.824711]\n",
      "791 [D loss: 0.650982, acc.: 53.12%] [G loss: 0.820220]\n",
      "792 [D loss: 0.586159, acc.: 71.88%] [G loss: 0.790276]\n",
      "793 [D loss: 0.559609, acc.: 71.88%] [G loss: 0.840260]\n",
      "794 [D loss: 0.648333, acc.: 56.25%] [G loss: 0.832603]\n",
      "795 [D loss: 0.647796, acc.: 65.62%] [G loss: 0.811984]\n",
      "796 [D loss: 0.589844, acc.: 75.00%] [G loss: 0.776512]\n",
      "797 [D loss: 0.679373, acc.: 62.50%] [G loss: 0.825333]\n",
      "798 [D loss: 0.571634, acc.: 71.88%] [G loss: 0.774944]\n",
      "799 [D loss: 0.614126, acc.: 68.75%] [G loss: 0.776325]\n",
      "800 [D loss: 0.665509, acc.: 62.50%] [G loss: 0.660936]\n",
      "801 [D loss: 0.648132, acc.: 62.50%] [G loss: 0.796189]\n",
      "802 [D loss: 0.660124, acc.: 53.12%] [G loss: 0.808758]\n",
      "803 [D loss: 0.647112, acc.: 56.25%] [G loss: 0.735013]\n",
      "804 [D loss: 0.601982, acc.: 62.50%] [G loss: 0.792594]\n",
      "805 [D loss: 0.663097, acc.: 59.38%] [G loss: 0.733561]\n",
      "806 [D loss: 0.661879, acc.: 71.88%] [G loss: 0.726105]\n",
      "807 [D loss: 0.676449, acc.: 59.38%] [G loss: 0.725740]\n",
      "808 [D loss: 0.636643, acc.: 56.25%] [G loss: 0.763547]\n",
      "809 [D loss: 0.693293, acc.: 65.62%] [G loss: 0.780173]\n",
      "810 [D loss: 0.633254, acc.: 56.25%] [G loss: 0.781043]\n",
      "811 [D loss: 0.699907, acc.: 53.12%] [G loss: 0.751884]\n",
      "812 [D loss: 0.693863, acc.: 56.25%] [G loss: 0.805446]\n",
      "813 [D loss: 0.707985, acc.: 56.25%] [G loss: 0.828817]\n",
      "814 [D loss: 0.669113, acc.: 62.50%] [G loss: 0.847159]\n",
      "815 [D loss: 0.701227, acc.: 50.00%] [G loss: 0.771999]\n",
      "816 [D loss: 0.712219, acc.: 50.00%] [G loss: 0.795512]\n",
      "817 [D loss: 0.674189, acc.: 59.38%] [G loss: 0.710173]\n",
      "818 [D loss: 0.624669, acc.: 56.25%] [G loss: 0.771977]\n",
      "819 [D loss: 0.522310, acc.: 81.25%] [G loss: 0.713516]\n",
      "820 [D loss: 0.707268, acc.: 43.75%] [G loss: 0.620504]\n",
      "821 [D loss: 0.658681, acc.: 59.38%] [G loss: 0.702122]\n",
      "822 [D loss: 0.674176, acc.: 59.38%] [G loss: 0.694686]\n",
      "823 [D loss: 0.745726, acc.: 56.25%] [G loss: 0.651713]\n",
      "824 [D loss: 0.696744, acc.: 50.00%] [G loss: 0.725545]\n",
      "825 [D loss: 0.714817, acc.: 50.00%] [G loss: 0.706278]\n",
      "826 [D loss: 0.618090, acc.: 68.75%] [G loss: 0.798346]\n",
      "827 [D loss: 0.653461, acc.: 62.50%] [G loss: 0.838391]\n",
      "828 [D loss: 0.649628, acc.: 62.50%] [G loss: 0.895168]\n",
      "829 [D loss: 0.742018, acc.: 46.88%] [G loss: 0.844451]\n",
      "830 [D loss: 0.584318, acc.: 71.88%] [G loss: 0.870158]\n",
      "831 [D loss: 0.679356, acc.: 56.25%] [G loss: 0.860827]\n",
      "832 [D loss: 0.706857, acc.: 59.38%] [G loss: 0.824419]\n",
      "833 [D loss: 0.669899, acc.: 56.25%] [G loss: 0.764670]\n",
      "834 [D loss: 0.708510, acc.: 56.25%] [G loss: 0.840879]\n",
      "835 [D loss: 0.609338, acc.: 71.88%] [G loss: 0.794044]\n",
      "836 [D loss: 0.566074, acc.: 78.12%] [G loss: 0.793749]\n",
      "837 [D loss: 0.694945, acc.: 53.12%] [G loss: 0.744743]\n",
      "838 [D loss: 0.703071, acc.: 53.12%] [G loss: 0.770617]\n",
      "839 [D loss: 0.677634, acc.: 62.50%] [G loss: 0.763848]\n",
      "840 [D loss: 0.653851, acc.: 56.25%] [G loss: 0.754380]\n",
      "841 [D loss: 0.674523, acc.: 46.88%] [G loss: 0.811169]\n",
      "842 [D loss: 0.668405, acc.: 59.38%] [G loss: 0.815861]\n",
      "843 [D loss: 0.646943, acc.: 59.38%] [G loss: 0.912835]\n",
      "844 [D loss: 0.732543, acc.: 53.12%] [G loss: 0.914563]\n",
      "845 [D loss: 0.658144, acc.: 59.38%] [G loss: 0.885193]\n",
      "846 [D loss: 0.683034, acc.: 56.25%] [G loss: 0.794517]\n",
      "847 [D loss: 0.706748, acc.: 43.75%] [G loss: 0.751344]\n",
      "848 [D loss: 0.654525, acc.: 65.62%] [G loss: 0.842562]\n",
      "849 [D loss: 0.663032, acc.: 56.25%] [G loss: 0.769310]\n",
      "850 [D loss: 0.663203, acc.: 53.12%] [G loss: 0.799262]\n",
      "851 [D loss: 0.647559, acc.: 56.25%] [G loss: 0.751088]\n",
      "852 [D loss: 0.625668, acc.: 65.62%] [G loss: 0.727799]\n",
      "853 [D loss: 0.700960, acc.: 59.38%] [G loss: 0.703731]\n",
      "854 [D loss: 0.700714, acc.: 40.62%] [G loss: 0.720651]\n",
      "855 [D loss: 0.607737, acc.: 65.62%] [G loss: 0.771785]\n",
      "856 [D loss: 0.609495, acc.: 68.75%] [G loss: 0.805918]\n",
      "857 [D loss: 0.655139, acc.: 62.50%] [G loss: 0.819493]\n",
      "858 [D loss: 0.760166, acc.: 40.62%] [G loss: 0.807577]\n",
      "859 [D loss: 0.668225, acc.: 50.00%] [G loss: 0.903975]\n",
      "860 [D loss: 0.694077, acc.: 46.88%] [G loss: 0.811582]\n",
      "861 [D loss: 0.716344, acc.: 46.88%] [G loss: 0.903818]\n",
      "862 [D loss: 0.689784, acc.: 50.00%] [G loss: 0.860014]\n",
      "863 [D loss: 0.656912, acc.: 65.62%] [G loss: 0.853390]\n",
      "864 [D loss: 0.750883, acc.: 37.50%] [G loss: 0.759212]\n",
      "865 [D loss: 0.708340, acc.: 50.00%] [G loss: 0.769158]\n",
      "866 [D loss: 0.636526, acc.: 59.38%] [G loss: 0.763422]\n",
      "867 [D loss: 0.724691, acc.: 50.00%] [G loss: 0.713562]\n",
      "868 [D loss: 0.734736, acc.: 50.00%] [G loss: 0.764621]\n",
      "869 [D loss: 0.678301, acc.: 59.38%] [G loss: 0.768845]\n",
      "870 [D loss: 0.721208, acc.: 37.50%] [G loss: 0.801140]\n",
      "871 [D loss: 0.630608, acc.: 71.88%] [G loss: 0.825358]\n",
      "872 [D loss: 0.652481, acc.: 59.38%] [G loss: 0.821579]\n",
      "873 [D loss: 0.626310, acc.: 68.75%] [G loss: 0.823837]\n",
      "874 [D loss: 0.698660, acc.: 62.50%] [G loss: 0.764907]\n",
      "875 [D loss: 0.656968, acc.: 59.38%] [G loss: 0.799107]\n",
      "876 [D loss: 0.676609, acc.: 62.50%] [G loss: 0.773548]\n",
      "877 [D loss: 0.603201, acc.: 68.75%] [G loss: 0.799951]\n",
      "878 [D loss: 0.679487, acc.: 56.25%] [G loss: 0.832881]\n",
      "879 [D loss: 0.647723, acc.: 68.75%] [G loss: 0.816377]\n",
      "880 [D loss: 0.770767, acc.: 50.00%] [G loss: 0.772162]\n",
      "881 [D loss: 0.647169, acc.: 65.62%] [G loss: 0.807512]\n",
      "882 [D loss: 0.617074, acc.: 71.88%] [G loss: 0.770679]\n",
      "883 [D loss: 0.690665, acc.: 50.00%] [G loss: 0.791628]\n",
      "884 [D loss: 0.676015, acc.: 62.50%] [G loss: 0.792856]\n",
      "885 [D loss: 0.624709, acc.: 68.75%] [G loss: 0.727176]\n",
      "886 [D loss: 0.639253, acc.: 56.25%] [G loss: 0.695413]\n",
      "887 [D loss: 0.713537, acc.: 46.88%] [G loss: 0.686594]\n",
      "888 [D loss: 0.641422, acc.: 62.50%] [G loss: 0.755459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889 [D loss: 0.664948, acc.: 59.38%] [G loss: 0.750603]\n",
      "890 [D loss: 0.699206, acc.: 65.62%] [G loss: 0.745692]\n",
      "891 [D loss: 0.647292, acc.: 62.50%] [G loss: 0.779088]\n",
      "892 [D loss: 0.597092, acc.: 65.62%] [G loss: 0.745177]\n",
      "893 [D loss: 0.645314, acc.: 56.25%] [G loss: 0.831403]\n",
      "894 [D loss: 0.744260, acc.: 53.12%] [G loss: 0.839472]\n",
      "895 [D loss: 0.700513, acc.: 59.38%] [G loss: 0.784212]\n",
      "896 [D loss: 0.658480, acc.: 56.25%] [G loss: 0.817392]\n",
      "897 [D loss: 0.658681, acc.: 59.38%] [G loss: 0.819688]\n",
      "898 [D loss: 0.711230, acc.: 50.00%] [G loss: 0.783757]\n",
      "899 [D loss: 0.658026, acc.: 59.38%] [G loss: 0.725820]\n",
      "900 [D loss: 0.615886, acc.: 65.62%] [G loss: 0.774219]\n",
      "901 [D loss: 0.571441, acc.: 84.38%] [G loss: 0.697828]\n",
      "902 [D loss: 0.628001, acc.: 59.38%] [G loss: 0.667270]\n",
      "903 [D loss: 0.712440, acc.: 53.12%] [G loss: 0.635409]\n",
      "904 [D loss: 0.710423, acc.: 46.88%] [G loss: 0.696296]\n",
      "905 [D loss: 0.656142, acc.: 65.62%] [G loss: 0.705749]\n",
      "906 [D loss: 0.568334, acc.: 75.00%] [G loss: 0.709008]\n",
      "907 [D loss: 0.586429, acc.: 71.88%] [G loss: 0.655054]\n",
      "908 [D loss: 0.716127, acc.: 53.12%] [G loss: 0.689836]\n",
      "909 [D loss: 0.704715, acc.: 50.00%] [G loss: 0.771270]\n",
      "910 [D loss: 0.679559, acc.: 65.62%] [G loss: 0.804435]\n",
      "911 [D loss: 0.642999, acc.: 65.62%] [G loss: 0.830950]\n",
      "912 [D loss: 0.583444, acc.: 81.25%] [G loss: 0.906106]\n",
      "913 [D loss: 0.682028, acc.: 56.25%] [G loss: 0.891382]\n",
      "914 [D loss: 0.660295, acc.: 56.25%] [G loss: 0.848523]\n",
      "915 [D loss: 0.701308, acc.: 53.12%] [G loss: 0.766740]\n",
      "916 [D loss: 0.617322, acc.: 62.50%] [G loss: 0.784931]\n",
      "917 [D loss: 0.648457, acc.: 68.75%] [G loss: 0.783177]\n",
      "918 [D loss: 0.615701, acc.: 68.75%] [G loss: 0.718994]\n",
      "919 [D loss: 0.575625, acc.: 68.75%] [G loss: 0.714253]\n",
      "920 [D loss: 0.612506, acc.: 56.25%] [G loss: 0.670974]\n",
      "921 [D loss: 0.627759, acc.: 68.75%] [G loss: 0.662303]\n",
      "922 [D loss: 0.605162, acc.: 62.50%] [G loss: 0.687041]\n",
      "923 [D loss: 0.728185, acc.: 43.75%] [G loss: 0.670398]\n",
      "924 [D loss: 0.658555, acc.: 62.50%] [G loss: 0.744584]\n",
      "925 [D loss: 0.691164, acc.: 56.25%] [G loss: 0.787794]\n",
      "926 [D loss: 0.581403, acc.: 71.88%] [G loss: 0.793433]\n",
      "927 [D loss: 0.653026, acc.: 56.25%] [G loss: 0.882931]\n",
      "928 [D loss: 0.567102, acc.: 84.38%] [G loss: 0.864906]\n",
      "929 [D loss: 0.603204, acc.: 71.88%] [G loss: 0.885056]\n",
      "930 [D loss: 0.645869, acc.: 56.25%] [G loss: 0.863920]\n",
      "931 [D loss: 0.604652, acc.: 65.62%] [G loss: 0.828127]\n",
      "932 [D loss: 0.703184, acc.: 56.25%] [G loss: 0.848359]\n",
      "933 [D loss: 0.716919, acc.: 50.00%] [G loss: 0.856287]\n",
      "934 [D loss: 0.604886, acc.: 78.12%] [G loss: 0.821251]\n",
      "935 [D loss: 0.660956, acc.: 56.25%] [G loss: 0.846266]\n",
      "936 [D loss: 0.675522, acc.: 59.38%] [G loss: 0.833757]\n",
      "937 [D loss: 0.759047, acc.: 40.62%] [G loss: 0.710605]\n",
      "938 [D loss: 0.770017, acc.: 46.88%] [G loss: 0.745142]\n",
      "939 [D loss: 0.692392, acc.: 50.00%] [G loss: 0.748478]\n",
      "940 [D loss: 0.655234, acc.: 56.25%] [G loss: 0.834151]\n",
      "941 [D loss: 0.720452, acc.: 50.00%] [G loss: 0.853431]\n",
      "942 [D loss: 0.654325, acc.: 62.50%] [G loss: 0.851815]\n",
      "943 [D loss: 0.618230, acc.: 68.75%] [G loss: 0.888760]\n",
      "944 [D loss: 0.657441, acc.: 62.50%] [G loss: 0.917969]\n",
      "945 [D loss: 0.689747, acc.: 56.25%] [G loss: 0.888003]\n",
      "946 [D loss: 0.745985, acc.: 46.88%] [G loss: 0.717445]\n",
      "947 [D loss: 0.652068, acc.: 71.88%] [G loss: 0.781897]\n",
      "948 [D loss: 0.669899, acc.: 53.12%] [G loss: 0.840030]\n",
      "949 [D loss: 0.620023, acc.: 71.88%] [G loss: 0.856362]\n",
      "950 [D loss: 0.569978, acc.: 75.00%] [G loss: 0.792668]\n",
      "951 [D loss: 0.675614, acc.: 53.12%] [G loss: 0.798454]\n",
      "952 [D loss: 0.616653, acc.: 68.75%] [G loss: 0.888961]\n",
      "953 [D loss: 0.619112, acc.: 68.75%] [G loss: 0.854501]\n",
      "954 [D loss: 0.594003, acc.: 68.75%] [G loss: 0.851911]\n",
      "955 [D loss: 0.655364, acc.: 59.38%] [G loss: 0.865194]\n",
      "956 [D loss: 0.599105, acc.: 68.75%] [G loss: 0.942574]\n",
      "957 [D loss: 0.667219, acc.: 50.00%] [G loss: 0.843347]\n",
      "958 [D loss: 0.680537, acc.: 56.25%] [G loss: 0.865033]\n",
      "959 [D loss: 0.658616, acc.: 62.50%] [G loss: 0.872890]\n",
      "960 [D loss: 0.654617, acc.: 59.38%] [G loss: 0.823409]\n",
      "961 [D loss: 0.576890, acc.: 68.75%] [G loss: 0.835653]\n",
      "962 [D loss: 0.746419, acc.: 50.00%] [G loss: 0.774053]\n",
      "963 [D loss: 0.635163, acc.: 75.00%] [G loss: 0.858121]\n",
      "964 [D loss: 0.738335, acc.: 56.25%] [G loss: 0.893937]\n",
      "965 [D loss: 0.679006, acc.: 53.12%] [G loss: 0.868738]\n",
      "966 [D loss: 0.599434, acc.: 78.12%] [G loss: 0.842876]\n",
      "967 [D loss: 0.639123, acc.: 59.38%] [G loss: 0.912967]\n",
      "968 [D loss: 0.675525, acc.: 56.25%] [G loss: 0.876536]\n",
      "969 [D loss: 0.659514, acc.: 62.50%] [G loss: 0.815142]\n",
      "970 [D loss: 0.654824, acc.: 56.25%] [G loss: 0.853971]\n",
      "971 [D loss: 0.675433, acc.: 62.50%] [G loss: 0.784778]\n",
      "972 [D loss: 0.737196, acc.: 46.88%] [G loss: 0.811398]\n",
      "973 [D loss: 0.783432, acc.: 37.50%] [G loss: 0.786795]\n",
      "974 [D loss: 0.690655, acc.: 53.12%] [G loss: 0.789601]\n",
      "975 [D loss: 0.634293, acc.: 62.50%] [G loss: 0.779275]\n",
      "976 [D loss: 0.652854, acc.: 59.38%] [G loss: 0.753323]\n",
      "977 [D loss: 0.721070, acc.: 50.00%] [G loss: 0.803508]\n",
      "978 [D loss: 0.708516, acc.: 43.75%] [G loss: 0.747816]\n",
      "979 [D loss: 0.681543, acc.: 59.38%] [G loss: 0.759321]\n",
      "980 [D loss: 0.659725, acc.: 53.12%] [G loss: 0.817789]\n",
      "981 [D loss: 0.699765, acc.: 62.50%] [G loss: 0.809619]\n",
      "982 [D loss: 0.649560, acc.: 68.75%] [G loss: 0.767655]\n",
      "983 [D loss: 0.746415, acc.: 46.88%] [G loss: 0.763341]\n",
      "984 [D loss: 0.648560, acc.: 65.62%] [G loss: 0.776033]\n",
      "985 [D loss: 0.680022, acc.: 53.12%] [G loss: 0.726171]\n",
      "986 [D loss: 0.687420, acc.: 59.38%] [G loss: 0.745284]\n",
      "987 [D loss: 0.687821, acc.: 53.12%] [G loss: 0.715504]\n",
      "988 [D loss: 0.698789, acc.: 56.25%] [G loss: 0.753460]\n",
      "989 [D loss: 0.641989, acc.: 56.25%] [G loss: 0.720820]\n",
      "990 [D loss: 0.701730, acc.: 43.75%] [G loss: 0.680478]\n",
      "991 [D loss: 0.773049, acc.: 46.88%] [G loss: 0.693097]\n",
      "992 [D loss: 0.708377, acc.: 50.00%] [G loss: 0.680338]\n",
      "993 [D loss: 0.712098, acc.: 53.12%] [G loss: 0.775924]\n",
      "994 [D loss: 0.654302, acc.: 71.88%] [G loss: 0.739265]\n",
      "995 [D loss: 0.741534, acc.: 43.75%] [G loss: 0.817412]\n",
      "996 [D loss: 0.664814, acc.: 53.12%] [G loss: 0.796630]\n",
      "997 [D loss: 0.678990, acc.: 53.12%] [G loss: 0.779627]\n",
      "998 [D loss: 0.664198, acc.: 53.12%] [G loss: 0.800145]\n",
      "999 [D loss: 0.662604, acc.: 59.38%] [G loss: 0.779661]\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Convolution2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        #mnist\n",
    "        self.img_rows = 28 \n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        #  \n",
    "        self.z_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Generator\n",
    "        self.generator = self.build_generator()\n",
    "        # generator\n",
    "        #self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        self.combined = self.build_combined1()\n",
    "        #self.combined = self.build_combined2()\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_shape = (self.z_dim,)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1024, input_shape=noise_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(128*7*7))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Reshape((7,7,128), input_shape=(128*7*7,)))\n",
    "        model.add(UpSampling2D((2,2)))\n",
    "        model.add(Convolution2D(64,5,5,border_mode='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(UpSampling2D((2,2)))\n",
    "        model.add(Convolution2D(1,5,5,border_mode='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(64,5,5, subsample=(2,2),\\\n",
    "                  border_mode='same', input_shape=img_shape))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Convolution2D(128,5,5,subsample=(2,2)))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))   \n",
    "        return model\n",
    "    \n",
    "    def build_combined1(self):\n",
    "        self.discriminator.trainable = False\n",
    "        model = Sequential([self.generator, self.discriminator])\n",
    "        return model\n",
    "\n",
    "    def build_combined2(self):\n",
    "        z = Input(shape=(self.z_dim,))\n",
    "        img = self.generator(z)\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator(img)\n",
    "        model = Model(z, valid)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # mnist\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Generator\n",
    "            noise = np.random.normal(0, 1, (half_batch, self.z_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "\n",
    "            # \n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # discriminator\n",
    "            # \n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            # \n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "\n",
    "            # 1 \n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # \n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # \n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        # \n",
    "        r, c = 5, 5\n",
    "\n",
    "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # 0-1\n",
    "        gen_imgs = 0.5 * gen_imgs +  0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/gcgan/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=1000, batch_size=32, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
